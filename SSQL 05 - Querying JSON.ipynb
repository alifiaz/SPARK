{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Querying JSON & Hierarchical Data with SQL\n\nApache Spark&trade; and Databricks&reg; make it easy to work with hierarchical data, such as nested JSON records.\n\n## In this lesson you:\n* Use SQL to query a table backed by JSON data\n* Query nested structured data\n* Query data containing array columns \n\n## Audience\n* Primary Audience: Data Analysts\n* Additional Audiences: Data Engineers and Data Scientists\n\n## Prerequisites\n* Web browser: **Chrome**\n* A cluster configured with **8 cores** and **DBR 6.2**\n* Familiarity with <a href=\"https://www.w3schools.com/sql/\" target=\"_blank\">ANSI SQL</a> is required"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the<br/>\nstart of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/a3098jg2t0?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/a3098jg2t0?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## Examining the contents of a JSON file\n\nJSON is a common file format in big data applications and in data lakes (or large stores of diverse data).  Datatypes such as JSON arise out of a number of data needs.  For instance, what if...  \n<br>\n* Your schema, or the structure of your data, changes over time?\n* You need nested fields like an array with many values or an array of arrays?\n* You don't know how you're going use your data yet so you don't want to spend time creating relational tables?\n\nThe popularity of JSON is largely due to the fact that JSON allows for nested, flexible schemas.\n\nThis lesson uses the `DatabricksBlog` table, which is backed by JSON file `dbfs:/mnt/training/databricks-blog.json`. If you examine the raw file, you can see that it contains compact JSON data. There's a single JSON object on each line of the file; each object corresponds to a row in the table. Each row represents a blog post on the <a href=\"https://databricks.com/blog\" target=\"_blank\">Databricks blog</a>, and the table contains all blog posts through August 9, 2017."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/1i3n3rb0vy?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/1i3n3rb0vy?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%fs head dbfs:/mnt/training/databricks-blog.json"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Truncated to first 65536 bytes]\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;roy&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/04/10/mapr-integrates-spark-stack.html&quot;, &quot;authors&quot;: [&quot;Tomer Shiran (VP of Product Management at MapR)&quot;], &quot;id&quot;: 33, &quot;categories&quot;: [&quot;Company Blog&quot;, &quot;Partners&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-04-10&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-04-10&quot;}, &quot;title&quot;: &quot;MapR Integrates the Complete Apache Spark Stack&quot;, &quot;slug&quot;: &quot;mapr-integrates-spark-stack&quot;, &quot;content&quot;: &quot;&lt;div class=\\&quot;post-meta\\&quot;&gt;This post is guest authored by our friends at MapR, announcing our new partnership to provide enterprise support for Apache Spark as part of MapR's Distribution of Hadoop.&lt;/div&gt;\\n\\n&lt;hr /&gt;\\n\\nWith over 500 paying customers, my team and I have the opportunity to talk to many organizations that are leveraging Hadoop in production to extract value from big data. One of the most common topics raised by our customers in recent months is Apache Spark. Some customers just want to learn more about the advantages of this technology and the use cases that it addresses, while others are already running it in production with the MapR Distribution. These customers range from the world\\u2019s largest cable telcos and retailers to Silicon Valley startups such as Quantifind, which recently talked about its use of Spark on MapR in an &lt;a href=\\&quot;http://www.datameer.com/ceoblog/big-data-brews-with-erich-nachbar/\\&quot; target=\\&quot;_blank\\&quot;&gt;interview&lt;/a&gt; with Stefan Groschupf, CEO of Datameer.\\n\\nToday, I am happy to &lt;a href=\\&quot;http://www.businesswire.com/news/home/20140410005101/en/MapR-Adds-Complete-Apache-Spark-Stack-Distribution#.U0a0G61dXKI\\&quot; target=\\&quot;_blank\\&quot;&gt;announce&lt;/a&gt; and share with you the beginning of our journey with Databricks, and the addition of the complete Spark stack to the MapR Distribution for Apache Hadoop. We are now the only Hadoop distribution to support the complete Spark stack, including Spark, Spark Streaming (stream processing), Shark (Hive on Spark), MLLib (machine learning) and GraphX (graph processing). This is a testament to our commitment to open source and to providing our customers with maximum flexibility to pick and choose the right tool for the job.\\n&lt;h2 id=\\&quot;why-spark\\&quot;&gt;Why Spark?&lt;/h2&gt;\\nOne of the challenges organizations face when adopting Hadoop is a shortage of developers who have experience building Hadoop applications. Our professional services organization has helped dozens of companies with the development and deployment of Hadoop applications, and our training department has trained countless engineers. Organizations are hungry for solutions that make it easier to develop Hadoop applications while increasing developer productivity, and Spark fits this bill. Spark jobs can require as little as 1/5th of code. Spark provides a simple programming abstraction allowing developers to design applications as operations on data collections (known as RDDs, or Resilient Distributed Datasets). Developers can build these applications in multiple programming languages, including Java, Scala and Python, and the same code can be reused across batch, interactive and streaming applications.\\n\\nIn addition to making developers happier and more productive, Spark provides significant benefits with respect to end-to-end application performance. To this end, Spark provides a general-purpose execution framework with in-memory pipelining. For many applications, this results in a 5-100x performance improvement, because some or all steps can execute in memory without unnecessarily writing to and reading from disk. The performance advantage of the Spark engine, combined with the industry-leading performance of the MapR Distribution, provides customers with the highest-performance platform for big data applications.\\n&lt;h2 id=\\&quot;why-databricks\\&quot;&gt;Why Databricks?&lt;/h2&gt;\\nDatabricks was founded by the creators of Apache Spark, and is currently the driving force behind the project. When we decided to add the Spark stack to our distribution and double down on our involvement in the Spark community, a strategic partnership with Databricks was a no-brainer. This partnership will benefit MapR customers who are interested in 24x7 support for Spark or any of the other projects in the stack, including Spark Streaming, Shark, MLLib and GraphX (with several other projects coming soon). In addition, MapR will be working closely with Databricks to drive the Spark roadmap and accelerate the development of new features, benefiting both MapR customers and the broader community.\\n\\nWe are very excited about the upcoming Apache Spark 1.0 release, expected later this month. We are looking forward to a great journey with Databricks and the other members of the Spark community.\\n\\n&lt;a href=\\&quot;http://w.on24.com/r.htm?e=780379&amp;amp;s=1&amp;amp;k=A6FB573A31B1DD9D8AB6294A101383ED&amp;amp;partnerref=MapR\\&quot; target=\\&quot;_blank\\&quot;&gt;Register for an upcoming joint webinar&lt;/a&gt; to learn more about the benefits of the complete Spark stack on MapR.&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;tdas&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/04/09/spark-0_9_1-released.html&quot;, &quot;authors&quot;: [&quot;Tathagata Das&quot;], &quot;id&quot;: 35, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;, &quot;Machine Learning&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-04-10&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-04-10&quot;}, &quot;title&quot;: &quot;Apache Spark 0.9.1 Released&quot;, &quot;slug&quot;: &quot;spark-0_9_1-released&quot;, &quot;content&quot;: &quot;We are happy to announce the availability of &lt;a href=\\&quot;http://spark.apache.org/releases/spark-release-0-9-1.html\\&quot; target=\\&quot;_blank\\&quot;&gt;Apache Spark 0.9.1&lt;/a&gt;! This is a maintenance release with bug fixes, performance improvements, better stability with YARN and improved parity of the Scala and Python API. We recommend all 0.9.0 users to upgrade to this stable release.\\n\\nThis is the first release since Spark graduated as a top level Apache project. Contributions to this release came from 37 developers.\\n\\nVisit the &lt;a href=\\&quot;http://spark.apache.org/releases/spark-release-0-9-1.html\\&quot; target=\\&quot;_blank\\&quot;&gt;release notes&lt;/a&gt; for more information about all the improvements and bug fixes. &lt;a href=\\&quot;http://spark.apache.org/downloads.html\\&quot; target=\\&quot;_blank\\&quot;&gt;Download&lt;/a&gt; it and try it out!&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;roy&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/03/31/application-spotlight-alpine.html&quot;, &quot;authors&quot;: [&quot;Steven Hillion&quot;], &quot;id&quot;: 37, &quot;categories&quot;: [&quot;Company Blog&quot;, &quot;Partners&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-04-01&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-04-01&quot;}, &quot;title&quot;: &quot;Application Spotlight: Alpine Data Labs&quot;, &quot;slug&quot;: &quot;application-spotlight-alpine&quot;, &quot;content&quot;: &quot;&lt;div class=\\&quot;post-meta\\&quot;&gt;This post is guest authored by our friends at Alpine Data Labs, part of the 'Application Spotlight' series highlighting innovative applications that are part of the Databricks \\&quot;Certified on Apache Spark\\&quot; program.&lt;/div&gt;\\n\\n&lt;hr /&gt;\\n\\nEveryone knows how hard it is to recruit engineers and data scientists in Silicon Valley. At &lt;a href=\\&quot;http://www.alpinenow.com\\&quot; target=\\&quot;_blank\\&quot;&gt;Alpine Data Labs&lt;/a&gt;, we think what we\\u2019re up to is pretty fun and challenging, but we still have to compete with other start-ups as well as the big internet companies to attract the best talent. One thing that can help is to be able to say that you\\u2019re working with the most innovative and powerful technologies.\\n\\nLast year, I was interviewing a talented engineer with a strong background in machine learning. And he said that the one thing he wanted to do above all was to work with Apache Spark. \\u201cWill I get to do that at Alpine?\\u201d he asked.\\n\\nIf it had been even a year earlier, I would have said \\u201cSure\\u2026at some point.\\u201d But in the meantime I\\u2019d met several of the members of the &lt;a href=\\&quot;https://amplab.cs.berkeley.edu\\&quot; target=\\&quot;_blank\\&quot;&gt;AMPLab&lt;/a&gt; research team at Berkeley, and been impressed with their mature approach to building a platform and ecosystem. And I\\u2019d seen enough companies installing Spark on their dev clusters that it was clear this was a technology to watch. In a remarkably short time, it went from experimental to very real. And now prospects in the Alpine pipeline were asking me if it was on the roadmap. So yes, I told my candidate. \\u201cYou\\u2019ll be working on Spark from day one.\\u201d\\n\\nLast week, Alpine announced at &lt;a href=\\&quot;http://www.cmswire.com/cms/big-data/alpine-turns-big-data-to-a-priceless-asset-fast-gigaomlive-024541.php\\&quot; target=\\&quot;_blank\\&quot;&gt;GigaOM&lt;/a&gt; that it\\u2019s one of the first analytics companies to leverage Spark for building predictive models. We demonstrated the Alpine engine running on &lt;a href=\\&quot;http://www.gopivotal.com/big-data/analytics-workbench\\&quot; target=\\&quot;_blank\\&quot;&gt;Pivotal\\u2019s Analytics Workbench&lt;/a&gt;, where it ran an iterative classification algorithm (logistic regression) on 50 million rows in less than 50 seconds.\\n\\nFurthermore, we were officially certified on Spark by the team at Databricks. It\\u2019s been an honor to work with them and the research team at Berkeley. We think their technology will be a serious contender for the leading platform for data science.\\n\\nSpark is more to us than just speed. It\\u2019s really the entire ecosystem that represents such an exciting paradigm for working with data.\\n\\nStill, the core capability of caching data in memory was our primary consideration, and our iterative algorithms have been shown to speed up by one or even two orders of magnitude (thanks again to that Pivotal cluster).\\n\\nWe\\u2019ve always had this mantra at Alpine: \\u201cAvoid multiple passes through the data!\\u201d And we\\u2019ve designed many of our machine learning algorithms to avoid scanning the data too many times, packing on calculations into each MapReduce job like a waiter piling up plates to try and clear a table in one go. But it\\u2019s rare that we can avoid it entirely. With Spark, it\\u2019s incredibly satisfying to watch the progress bar zip along as the system re-uses data it\\u2019s already seen before.\\n\\nAnother thing that\\u2019s getting our engineers excited is Spark\\u2019s &lt;a href=\\&quot;http://spark.apache.org/mllib/\\&quot; target=\\&quot;_blank\\&quot;&gt;MLLib&lt;/a&gt;, the machine-learning library written on top of the Spark runtime. Alpine has long thought that machine learning algorithms should be open source. (I helped to kick off the &lt;a href=\\&quot;http://madlib.net/\\&quot; target=\\&quot;_blank\\&quot;&gt;MADlib&lt;/a&gt; library of analytics functions for databases, and Alpine now uses it extensively.) So we\\u2019re now beginning to contribute some of our code back into MLLib. And, moreover, we think MLLib and MLI have the potential to be a more general repository for open-source machine learning.\\n\\nSo I\\u2019ll congratulate the Alpine team for helping to bring the power of Spark to our users, and I\\u2019ll also congratulate the Spark team and Databricks for making it possible!&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;michael&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html&quot;, &quot;authors&quot;: [&quot;Michael Armbrust&quot;, &quot;Reynold Xin&quot;], &quot;id&quot;: 42, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-03-27&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-03-27&quot;}, &quot;title&quot;: &quot;Spark SQL: Manipulating Structured Data Using Apache Spark&quot;, &quot;slug&quot;: &quot;spark-sql-manipulating-structured-data-using-spark-2&quot;, &quot;content&quot;: &quot;Building a unified platform for big data analytics has long been the vision of Apache Spark, allowing a single program to perform ETL, MapReduce, and complex analytics. An important aspect of unification that our users have consistently requested is the ability to more easily import data stored in external sources, such as Apache Hive. Today, we are excited to announce &lt;a href=\\&quot;https://spark.apache.org/docs/latest/sql-programming-guide.html\\&quot;&gt;Spark SQL&lt;/a&gt;, a new component recently merged into the Spark repository.\\n\\nSpark SQL brings native support for SQL to Spark and streamlines the process of querying data stored both in RDDs (Spark\\u2019s distributed datasets) and in external sources. Spark SQL conveniently blurs the lines between RDDs and relational tables. Unifying these powerful abstractions makes it easy for developers to intermix SQL commands querying external data with complex analytics, all within in a single application. Concretely, Spark SQL will allow developers to:\\n&lt;ul&gt;\\n \\t&lt;li&gt;Import relational data from Parquet files and Hive tables&lt;/li&gt;\\n \\t&lt;li&gt;Run SQL queries over imported data and existing RDDs&lt;/li&gt;\\n \\t&lt;li&gt;Easily write RDDs out to Hive tables or Parquet files&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;h2 id=\\&quot;spark-sql-in-action\\&quot;&gt;Spark SQL In Action&lt;/h2&gt;\\nNow, let\\u2019s take a closer look at how Spark SQL gives developers the power to integrate SQL commands into applications that also take advantage of MLlib, Spark\\u2019s machine learning library. Consider an application that needs to predict which users are likely candidates for a service, based on their profile. Often, such an analysis requires joining data from multiple sources. For the purposes of illustration, imagine an application with two tables:\\n&lt;ul&gt;\\n \\t&lt;li&gt;Users(userId INT, name String, email STRING,\\nage INT, latitude: DOUBLE, longitude: DOUBLE,\\nsubscribed: BOOLEAN)&lt;/li&gt;\\n \\t&lt;li&gt;Events(userId INT, action INT)&lt;/li&gt;\\n&lt;/ul&gt;\\nGiven the data stored in in these tables, one might want to build a model that will predict which users are good targets for a new campaign, based on users that are similar.\\n\\n[scala]\\n// Data can easily be extracted from existing sources,\\n// such as Apache Hive.\\nval trainingDataTable = sql(&amp;quot;&amp;quot;&amp;quot;\\n  SELECT e.action\\n         u.age,\\n         u.latitude,\\n         u.logitude\\n  FROM Users u\\n  JOIN Events e\\n  ON u.userId = e.userId&amp;quot;&amp;quot;&amp;quot;)\\n\\n// Since `sql` returns an RDD, the results of the above\\n// query can be easily used in MLlib\\nval trainingData = trainingDataTable.map { row =&amp;gt;\\n  val features = Array[Double](row(1), row(2), row(3))\\n  LabeledPoint(row(0), features)\\n}\\n\\nval model =\\n  new LogisticRegressionWithSGD().run(trainingData)\\n[/scala]\\n\\nNow that we have used SQL to join existing data and train a model, we can use this model to predict which users are likely targets.\\n\\n[scala]\\nval allCandidates = sql(&amp;quot;&amp;quot;&amp;quot;\\n  SELECT userId,\\n         age,\\n         latitude,\\n         logitude\\n  FROM Users\\n  WHERE subscribed = FALSE&amp;quot;&amp;quot;&amp;quot;)\\n\\n// Results of ML algorithms can be used as tables\\n// in subsequent SQL statements.\\ncase class Score(userId: Int, score: Double)\\nval scores = allCandidates.map { row =&amp;gt;\\n  val features = Array[Double](row(1), row(2), row(3))\\n  Score(row(0), model.predict(features))\\n}\\nscores.registerAsTable(&amp;quot;Scores&amp;quot;)\\n\\nval topCandidates = sql(&amp;quot;&amp;quot;&amp;quot;\\n  SELECT u.name, u.email\\n  FROM Scores s\\n    JOIN Users u ON s.userId = u.userId\\n  ORDER BY score DESC\\n  LIMIT 100&amp;quot;&amp;quot;&amp;quot;)\\n\\n// Send emails to top candidates to promote the service.\\n[/scala]\\n\\nIn this example, Spark SQL made it easy to extract and join the various datasets preparing them for the machine learning algorithm. Since the results of Spark SQL are also stored in RDDs, interfacing with other Spark libraries is trivial. Furthermore, Spark SQL allows developers to close the loop, by making it easy to manipulate and join the output of these algorithms, producing the desired final result.\\n\\nTo summarize, the unified Spark platform gives developers the power to choose the right tool for the right job, without having to juggle multiple systems. If you would like to see more concrete examples of using Spark SQL please check out the &lt;a href=\\&quot;http://spark.apache.org/docs/1.0.0/sql-programming-guide.html\\&quot;&gt;programming guide&lt;/a&gt;.\\n&lt;h2 id=\\&quot;optimizing-with-catalyst\\&quot;&gt;Optimizing with Catalyst&lt;/h2&gt;\\nIn addition to providing new ways to interact with data, Spark SQL also brings a powerful new optimization framework called Catalyst. Using Catalyst, Spark can automatically transform SQL queries so that they execute more efficiently. The Catalyst framework allows the developers behind Spark SQL to rapidly add new optimizations, enabling us to build a faster system more quickly. In one recent example, we found an inefficiency in Hive group-bys that took an experienced developer an entire weekend and over 250 lines of code to fix; we were then able to make the same fix in Catalyst in only a few lines of code.\\n&lt;h2 id=\\&quot;future-of-shark\\&quot;&gt;Future of Shark&lt;/h2&gt;\\nThe natural question that arises is about the future of Shark. Shark was among the first systems that delivered up to 100X speedup over Hive. It builds on the Apache Hive codebase and achieves performance improvements by swapping out the physical execution engine part of Hive. While this approach enables Shark users to speed up their Hive queries without modification to their existing warehouses, Shark inherits the large, complicated code base from Hive that makes it hard to optimize and maintain. As Spark SQL matures, Shark will transition to using Spark SQL for query optimization and physical execution so that users can benefit from the ongoing optimization efforts within Spark SQL.\\n\\nIn short, we will continue to invest in Shark and make it an excellent drop-in replacement for Apache Hive. It will take advantage of the new Spark SQL component, and will provide features that complement it, such as Hive compatibility and the standalone SharkServer, which allows external tools to connect queries through JDBC/ODBC.\\n&lt;h2 id=\\&quot;whats-next\\&quot;&gt;What\\u2019s next&lt;/h2&gt;\\nSpark SQL will be included in Spark 1.0 as an alpha component. However, this is only the beginning of better support for relational data in Spark, and this post only scratches the surface of Catalyst. Look for future blog posts on the following topics:\\n&lt;ul&gt;\\n \\t&lt;li&gt;Generating custom bytecode to speed up expression evaluation&lt;/li&gt;\\n \\t&lt;li&gt;Reading and writing data using other formats and systems, include Avro and HBase&lt;/li&gt;\\n \\t&lt;li&gt;API support for using Spark SQL in Python and Java&lt;/li&gt;\\n&lt;/ul&gt;&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;patrick&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/02/03/release-0_9_0.html&quot;, &quot;authors&quot;: [&quot;Patrick Wendell&quot;], &quot;id&quot;: 58, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-02-04&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-02-04&quot;}, &quot;title&quot;: &quot;Apache Spark 0.9.0 Released&quot;, &quot;slug&quot;: &quot;release-0_9_0&quot;, &quot;content&quot;: &quot;Our goal with Apache Spark is very simple: provide the best platform for computation on big data. We do this through both a powerful core engine and rich libraries for useful analytics tasks. Today, we are excited to announce the release of Apache Spark 0.9.0. This major release extends Spark\\u2019s libraries and further improves its performance and usability. Apache Spark 0.9.0 is the largest release to date, with work from 83 contributors, who submitted over 300 patches.\\n\\nApache Spark 0.9 features significant extensions to the set of standard analytical libraries packaged with Spark. The release introduces GraphX, a library for graph computation that comes with implementations of several standard algorithms, such as PageRank. Spark\\u2019s machine learning library (MLlib) has been extended to support Python, using the NumPy numerical library. A Naive Bayes Classifier has also been added to MLlib. Finally, Spark Streaming, which supports near-real-time continuous computation, has added a simplified high-availability mode and several significant optimizations.\\n\\nIn addition to higher-level libraries, Spark 0.9 features improvements to the core computation engine. Spark now now automatically spills reduce output to disk, increasing the stability of workloads with very large aggregations. Support for Spark in YARN mode has been hardened and improved. The standalone mode has added automatic supervision of applications and better support for sharing clusters amongst several users. Finally, we\\u2019ve focused on stabilizing API\\u2019s ahead of Apache Spark\\u2019s 1.0 release to make things easy for developers writing Spark applications. This includes upgrading to Scala 2.10, allowing applications written in Scala to use newer libraries.\\n\\nApache Spark 0.9.0 can be &lt;a href=\\&quot;http://spark.incubator.apache.org/downloads.html\\&quot;&gt;downloaded directly&lt;/a&gt; from the Apache Spark website. It will also be available to CDH users via a Cloudera parcel, which can automatically install Spark on existing CDH clusters. For a more detailed explanation of the features in this release, head on over to the &lt;a href=\\&quot;http://spark.incubator.apache.org/releases/spark-release-0-9-0.html\\&quot;&gt;official release notes&lt;/a&gt;. Enjoy the newest release of Spark!&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;ali&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/01/01/simr.html&quot;, &quot;authors&quot;: [&quot;Ali Ghodsi&quot;, &quot;Ahir Reddy&quot;], &quot;id&quot;: 65, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Ecosystem&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-01-02&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-01-02&quot;}, &quot;title&quot;: &quot;Apache Spark In MapReduce (SIMR)&quot;, &quot;slug&quot;: &quot;simr&quot;, &quot;content&quot;: &quot;Apache Hadoop integration has always been a key goal of Apache Spark and &lt;a href=\\&quot;http://hortonworks.com/wp-content/uploads/2013/06/YARN.png\\&quot;&gt;YARN&lt;/a&gt; users have long been able to run &lt;a href=\\&quot;http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\\&quot;&gt;Spark on YARN&lt;/a&gt;. However, up to now, it has been relatively hard to run Spark on Hadoop MapReduce v1 clusters, i.e. clusters that do not have YARN installed. Typically, users would have to get permission to install Spark/Scala on some subset of the machines, a process that could be time consuming. Enter &lt;a href=\\&quot;http://databricks.github.io/simr/\\&quot;&gt;SIMR (Spark In MapReduce)&lt;/a&gt;, which has been released in conjunction with &lt;a href=\\&quot;https://databricks.com/blog/2013/12/19/release-0_8_1.html\\&quot;&gt;Apache Spark 0.8.1&lt;/a&gt;.\\n\\nSIMR allows anyone with access to a Hadoop MapReduce v1 cluster to run Spark out of the box. A user can run Spark directly on top of Hadoop MapReduce v1 without any administrative rights, and without having Spark or Scala installed on any of the nodes. The only requirements are HDFS access and MapReduce v1. SIMR is open sourced under the &lt;a href=\\&quot;http://apache.org/licenses/LICENSE-2.0.html\\&quot;&gt;Apache license&lt;/a&gt; and was jointly developed by Databricks and UC Berkeley &lt;a href=\\&quot;http://amplab.cs.berkeley.edu\\&quot;&gt;AMPLab&lt;/a&gt;.\\n\\nThe basic idea is that a user can download the package of SIMR (&lt;a href=\\&quot;http://databricks.github.io/simr/#download\\&quot;&gt;3 files&lt;/a&gt;) that matches their Hadoop cluster and immediately start using Spark. SIMR includes the interactive Spark shell, and allows users to use the shell backed by the computational power of the cluster. It is a simple as &lt;code&gt;./simr --shell&lt;/code&gt;:\\n\\n&amp;nbsp;\\n\\n&lt;img src=\\&quot;https://databricks.com/wp-content/uploads/2014/01/simrshell.png\\&quot; alt=\\&quot;simrshell\\&quot; width=\\&quot;90%\\&quot; /&gt;\\n\\nRunning a Spark program simply requires bundling it along with its dependencies into a jar and launching the job through SIMR. SIMR uses the following command line syntax for running jobs:\\n\\n&lt;pre&gt;./simr jar_file main_class parameters&lt;/pre&gt;\\n\\nSIMR simply launches a MapReduce job with the desired number of map slots, and ensures that Spark/Scala and your job gets shipped to all those nodes. It then designates one of the mappers as the master and runs the Spark driver inside it. On the rest of the mappers it launches Spark executors that will execute tasks on behalf of the driver. Voila, your Spark job is running inside MapReduce on the cluster.\\n\\nSIMR lets users interact with the driver program. For example, users can type into the Spark shell and see its output interactively. The way this works is that SIMR runs a relay server on the master mapper and a relay client on the machine that launched SIMR. Any input from the client and output from the driver are relayed back and forth between the client and the master mapper.\\n\\nTo make all this work, SIMR makes extensive use of HDFS. The master mapper is elected using leader election by writing to HDFS and picking the mapper that firsts gets to write to HDFS. Similarly, the executors launched inside the rest of the mappers find out the driver\\u2019s URL by reading it from a specific file from HDFS. Thus, SIMR uses MapReduce and HDFS in place of a cluster manager.\\n\\n&lt;img src=\\&quot;https://databricks.com/wp-content/uploads/2014/01/simr-arch.png\\&quot; alt=\\&quot;simr-arch\\&quot; width=\\&quot;90%\\&quot; /&gt;\\n\\nSIMR is not intended to be used in production mode, but rather to enable users to explore and use Spark before running it on a proper resource manager, such as YARN, Mesos, or Standalone Mode. MapReduce 2 (YARN) users can of course use the existing &lt;a href=\\&quot;http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\\&quot;&gt;Spark on YARN&lt;/a&gt; solution, or explore other &lt;a href=\\&quot;http://spark.incubator.apache.org/docs/latest/index.html#launching-on-a-cluster\\&quot;&gt;Spark deployment options&lt;/a&gt;.\\n\\nWe hope SIMR will enable users to try out Spark without any heavy operational burden. Towards this goal, we have pre-built several SIMR binaries for different versions of Hadoop. Please go ahead and try it and let us know if you have any feedback.\\n\\nSIMR resources:\\n&lt;ul&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://databricks.github.io/simr\\&quot;&gt;Homepage&lt;/a&gt;&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://databricks.github.io/simr#download\\&quot;&gt;Download&lt;/a&gt;&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;https://github.com/databricks/simr\\&quot;&gt;Source code&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;&quot;}\n\n*** WARNING: skipped 18619 bytes of output ***\n\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;roy&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/03/18/spark-certification.html&quot;, &quot;authors&quot;: [&quot;Databricks Press Office&quot;], &quot;id&quot;: 2411, &quot;categories&quot;: [&quot;Announcements&quot;, &quot;Company Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-03-19&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-03-19&quot;}, &quot;title&quot;: &quot;Databricks announces \\&quot;Certified on Apache Spark\\&quot; Program&quot;, &quot;slug&quot;: &quot;spark-certification&quot;, &quot;content&quot;: &quot;&lt;strong&gt;BERKELEY, Calif. \\u2013 March 18, 2014 \\u2013&lt;/strong&gt; Databricks, the company founded by the creators of Apache Spark that is revolutionizing what enterprises can do with Big Data, today announced the Databricks &lt;a href=\\&quot;/certification/\\&quot;&gt;\\u201cCertified on Spark\\u201d Program&lt;/a&gt; for applications built on top of the Apache Spark platform. This program ensures that certified applications will work with a multitude of commercially supported Spark distributions.\\n\\n\\u201cPioneering application developers that are leveraging the power of Spark have had to choose between two sub-optimal choices: they either have to package Spark platform support with their application or attempt to maintain integration/certification individually with a rapidly increasing set of commercially supported Spark distributions,\\u201d said Ion Stoica, Databricks CEO. \\u201cThe Databricks \\u2018Certified on Spark\\u2019 program enables developers to certify solely against the 100% open-source Apache Spark distribution, and ensures interoperability with Apache Spark-compatible distributions. Databricks will handle the task of certifying the compatibility of each commercial Spark distribution with the Apache version and will soon announce the initial set of distributions that meet this criteria.\\u201d\\n\\nThe pioneering partners of the Databricks certification program include Adatao, Alpine Data Labs, and Tresata - advanced analytics application vendors that have been at the forefront in leveraging the power of Spark to deliver faster and deeper insights for their enterprise customers.\\n\\n\\u201cCertified on Spark\\u201d also provides multiple benefits for enterprise users including:\\n&lt;ul&gt;\\n \\t&lt;li&gt;Decoupling Spark distribution (and commercial support) from application licensing&lt;/li&gt;\\n \\t&lt;li&gt;Full transparency into which applications are truly designed to work with and leverage the power of Spark&lt;/li&gt;\\n \\t&lt;li&gt;A rapidly increasing set of certified applications to incentivize distribution vendors to maintain compatibility with Apache Spark and avoid forking and fragmentation, ultimately resulting in greater compatibility and shared innovation for users&lt;/li&gt;\\n&lt;/ul&gt;\\n\\u201cAt Databricks we are fully committed to keeping Spark 100% open source and to bringing it to the widest possible set of users,\\u201d said Matei Zaharia, Databricks CTO and the creator of Spark. \\u201cThe \\u2018Certified on Spark\\u2019 program is key to maintaining a healthy and unified Apache Spark platform, and it is an expression of our community focused efforts, such as organizing meetups and the upcoming Spark Summit, and driving the future of open-source development of Spark.\\u201d\\n\\nApplication developers that are interested in applying for the \\u201cCertified on Spark\\u201d program should visit &lt;a href=\\&quot;http://www.databricks.com\\&quot;&gt;www.databricks.com&lt;/a&gt; and select \\u201cApply for Certification\\u201d. Enterprise users can also visit the Databricks site regularly to see the latest set of certified application vendors and read \\u201capplication spotlight\\u201d blog articles that deep-dive into specific examples of Spark-powered applications.&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;ion&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/03/02/spark-apache-top-level-project.html&quot;, &quot;authors&quot;: [&quot;Ion Stoica&quot;], &quot;id&quot;: 2412, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-03-03&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-03-03&quot;}, &quot;title&quot;: &quot;Apache Spark Now a Top-level Apache Project&quot;, &quot;slug&quot;: &quot;spark-apache-top-level-project&quot;, &quot;content&quot;: &quot;&lt;div class=\\&quot;blogContent\\&quot;&gt;\\n\\nWe are delighted with the recent &lt;a href=\\&quot;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50\\&quot;&gt;announcement&lt;/a&gt; of the Apache Software Foundation that &lt;a href=\\&quot;http://spark.apache.org\\&quot;&gt;Apache Spark&lt;/a&gt; has become a top-level Apache project. This is a recognition of the fantastic work done by the Spark open source community, which now counts over 140 developers from 30+ companies.\\n\\nIn short time, Spark has become an increasingly popular solution for numerous big data applications, including machine learning, interactive queries, and stream processing. Spark now is an integral part of the Hadoop ecosystem, with many organizations employing Spark to perform sophisticated processing on their Hadoop data.\\n\\nAt Databricks we are looking forward to continuing our work with the open source community to accelerate the development and adoption of Apache Spark. Currently employing the lead developers and creators of many of the components of the Spark ecosystem, including Matei Zaharia, the creator of Spark, and Patrick Wendell, the release manager of Spark, we are committed to the success of Apache Spark.\\n\\n&lt;/div&gt;&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;rxin&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/02/12/big-data-benchmark.html&quot;, &quot;authors&quot;: [&quot;Ahir Reddy&quot;, &quot;Reynold Xin&quot;], &quot;id&quot;: 2413, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-02-13&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-02-13&quot;}, &quot;title&quot;: &quot;AMPLab updates the Big Data Benchmark&quot;, &quot;slug&quot;: &quot;big-data-benchmark&quot;, &quot;content&quot;: &quot;The AMPLab at UC Berkeley, with help from Databricks, recently released an update to the &lt;a href=\\&quot;https://amplab.cs.berkeley.edu/benchmark/\\&quot;&gt;Big Data Benchmark&lt;/a&gt;. This benchmark uses Amazon EC2 to compare performance of five popular SQL query engines in the Big Data ecosystem on common types of queries, which can be reproduced through publicly available scripts and datasets.\\n\\nIn the past year, the community has invested heavily in performance optimizations of query engines. We are glad to see that all projects have evolved in this area. Although the queries used in the benchmark are simple, we are proud that Shark remains one of the fastest engines for these workloads, and has improved significantly since the last run.\\n\\nWhile this benchmark reaffirms Shark as a highly performant SQL query engine, we are working hard at Databricks to push the boundaries further. Stay tuned for some exciting news we will share soon with the community.\\n&lt;ul&gt;\\n\\t&lt;li&gt;&lt;a href=\\&quot;https://amplab.cs.berkeley.edu/benchmark/\\&quot;&gt;Big Data Benchmark&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&amp;nbsp;&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;pat.mcdonough&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/02/10/strata-santa-clara-2014.html&quot;, &quot;authors&quot;: [&quot;Pat McDonough&quot;], &quot;id&quot;: 2414, &quot;categories&quot;: [&quot;Company Blog&quot;, &quot;Events&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-02-11&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-02-11&quot;}, &quot;title&quot;: &quot;Databricks at the O'Reilly Strata Conference 2014&quot;, &quot;slug&quot;: &quot;strata-santa-clara-2014&quot;, &quot;content&quot;: &quot;The Databricks team is excited to take part in a number of activities throughout the 2014 O\\u2019Reilly Strata Conference in Santa Clara. From hands-on training, to office hours, to several talks (including a keynote), there are plenty of chances for attendees to learn how Apache Spark is bringing ease of use and outstanding performance to your big data. The schedule for the Databricks team includes:\\n&lt;ul&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://ampcamp.berkeley.edu/4/\\&quot;&gt;AMPCamp4&lt;/a&gt;, Hosted at Strata&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://strataconf.com/strata2014/public/content/office-hours\\&quot;&gt;Office Hours&lt;/a&gt; on Wednesday at 5:45pm&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://strataconf.com/strata2014/public/schedule/detail/33057\\&quot;&gt;How Companies are Using Spark, and Where the Edge in Big Data Will Be&lt;/a&gt;, a keynote talk presented by Matei Zaharia on Thursday at 9:15am&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://strataconf.com/strata2014/public/schedule/detail/32375\\&quot;&gt;Querying Petabytes of Data in Seconds with BlinkDB&lt;/a&gt;, co-presented by Reynold Xin on Thursday at 1:30pm&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;h2 id=\\&quot;about-ampcamp-4\\&quot;&gt;About AMPCamp 4&lt;/h2&gt;\\nWe\\u2019ll be kicking off the conference on Tuesday by helping to host the fourth iteration of AMPCamp, where some of the newer components of the Berkeley Data Analytics stack will get some time in the spotlight. There are talks on BlinkDB, MLbase, GraphX, and Tachyon in the morning, and hands-on training for BlinkDB, MLbase, Spark, Spark Streaming, GraphX, and Shark in the afternoon.\\n&lt;h2 id=\\&quot;conference-talks--office-hours\\&quot;&gt;Conference Talks &amp;amp; Office Hours&lt;/h2&gt;\\nOn Wednesday afternoon, attendees can come speak directly with a few of the members from our team in Strata\\u2019s Office Hours. Office hours provide a chance to meet face-to-face with the Databricks team and to ask any questions about the Spark project and ecosystem.\\n\\nTo learn the latest about Apache Spark, attendees should listen to talks featuring the Databricks team, including a keynote by our CTO Matei Zaharia. On Thursday morning, Matei will describe how organizations must now compete on the speed and sophistication with which they can draw value from data, and how Apache Spark makes a new class of applications possible.\\n&lt;h2 id=\\&quot;well-see-you-there\\&quot;&gt;We\\u2019ll See You There!&lt;/h2&gt;\\nWe\\u2019re looking forward to seeing you at the conference!&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;ion&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/01/21/spark-and-hadoop.html&quot;, &quot;authors&quot;: [&quot;Ion Stoica&quot;], &quot;id&quot;: 2415, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Ecosystem&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-01-22&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-01-22&quot;}, &quot;title&quot;: &quot;Apache Spark and Hadoop: Working Together&quot;, &quot;slug&quot;: &quot;spark-and-hadoop&quot;, &quot;content&quot;: &quot;We are often asked how does &lt;a href=\\&quot;http://spark.incubator.apache.org\\&quot;&gt;Apache Spark&lt;/a&gt; fits in the Hadoop ecosystem, and how one can run Spark in a existing Hadoop cluster. This blog aims to answer these questions.\\n\\nFirst, Spark is intended to &lt;em&gt;enhance&lt;/em&gt;, not replace, the Hadoop stack. From day one, Spark was designed to read and write data from and to HDFS, as well as other storage systems, such as HBase and Amazon\\u2019s S3. As such, Hadoop users can enrich their processing capabilities by combining Spark with Hadoop MapReduce, HBase, and other big data frameworks.\\n\\nSecond, we have constantly focused on making it as easy as possible for &lt;em&gt;every Hadoop user&lt;/em&gt; to take advantage of Spark\\u2019s capabilities. No matter whether you run Hadoop 1.x or Hadoop 2.0 (YARN), and no matter whether you have administrative privileges to configure the Hadoop cluster or not, there is a way for you to run Spark! In particular, there are three ways to deploy Spark in a Hadoop cluster: standalone, YARN, and SIMR.\\n\\n&lt;img class=\\&quot;aligncenter\\&quot; src=\\&quot;https://databricks.com/wp-content/uploads/2014/01/SparkHadoop.png\\&quot; alt=\\&quot;SparkHadoop1.png\\&quot; /&gt;\\n\\n&lt;strong&gt;Standalone deployment&lt;/strong&gt;: With the standalone deployment one can statically allocate resources on all or a subset of machines in a Hadoop cluster and run Spark side by side with Hadoop MR. The user can then run arbitrary Spark jobs on her HDFS data. Its simplicity makes this the deployment of choice for many Hadoop 1.x users.\\n\\n&lt;strong&gt;&lt;a href=\\&quot;https://hadoop.apache.org/docs/current2/hadoop-yarn/hadoop-yarn-site/YARN.html\\&quot;&gt;Hadoop Yarn&lt;/a&gt; deployment&lt;/strong&gt;: Hadoop users who have already deployed or are planning to deploy &lt;a href=\\&quot;https://hadoop.apache.org/docs/current2/hadoop-yarn/hadoop-yarn-site/YARN.html\\&quot;&gt;Hadoop Yarn&lt;/a&gt; can simply run Spark on YARN without any pre-installation or administrative access required. This allows users to easily integrate Spark in their Hadoop stack and take advantage of the full power of Spark, as well as of other components running on top of Spark.\\n\\n&lt;strong&gt;Spark In MapReduce (&lt;a href=\\&quot;https://databricks.com/blog/2014/01/01/simr.html\\&quot;&gt;SIMR&lt;/a&gt;)&lt;/strong&gt;: For the Hadoop users that are not running YARN yet, another option, in addition to the standalone deployment, is to use SIMR to launch Spark jobs inside MapReduce. With SIMR, users can start experimenting with Spark and use its shell within a couple of minutes after downloading it! This tremendously lowers the barrier of deployment, and lets virtually everyone play with Spark.\\n&lt;h2 id=\\&quot;interoperability-with-other-systems\\&quot;&gt;Interoperability with other Systems&lt;/h2&gt;\\nSpark interoperates not only with Hadoop, but with other popular big data technologies as well.\\n&lt;ul&gt;\\n \\t&lt;li&gt;&lt;strong&gt;&lt;a href=\\&quot;http://hive.apache.org/\\&quot;&gt;Apache Hive&lt;/a&gt;&lt;/strong&gt;: Through &lt;a href=\\&quot;https://github.com/amplab/shark/wiki\\&quot;&gt;Shark&lt;/a&gt;, Spark enables Apache Hive users to run their unmodified queries much faster. Hive is a popular data warehouse solution running on top of Hadoop, while Shark is a system that allows the Hive framework to run on top of Spark instead of Hadoop. As a result, Shark can accelerate Hive queries by as much as 100x when the input data fits into memory, and up 10x when the input data is stored on disk.&lt;/li&gt;\\n \\t&lt;li&gt;&lt;strong&gt;&lt;a href=\\&quot;http://aws.amazon.com/\\&quot;&gt;AWS EC2&lt;/a&gt;&lt;/strong&gt;: Users can easily run Spark (and Shark) on top of Amazon\\u2019s EC2 either using the &lt;a href=\\&quot;http://spark.incubator.apache.org/docs/latest/ec2-scripts.html\\&quot;&gt;scripts&lt;/a&gt; that come with Spark, or the hosted &lt;a href=\\&quot;http://aws.amazon.com/articles/4926593393724923\\&quot;&gt;versions of Spark and Shark&lt;/a&gt; on Amazon\\u2019s Elastic MapReduce.&lt;/li&gt;\\n \\t&lt;li&gt;&lt;strong&gt;&lt;a href=\\&quot;http://mesos.apache.org/\\&quot;&gt;Apache Mesos&lt;/a&gt;&lt;/strong&gt;: Spark runs on top of Mesos, a cluster manager system which provides efficient resource isolation across distributed applications, including &lt;a href=\\&quot;http://www.mcs.anl.gov/research/projects/mpi/\\&quot;&gt;MPI&lt;/a&gt; and Hadoop. Mesos enables &lt;em&gt;fine grained&lt;/em&gt; sharing which allows a Spark job to dynamically take advantage of the idle resources in the cluster during its execution. This leads to considerable performance improvements, especially for long running Spark jobs.&lt;/li&gt;\\n&lt;/ul&gt;&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;patrick&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2013/12/19/release-0_8_1.html&quot;, &quot;authors&quot;: [&quot;Patrick Wendell&quot;], &quot;id&quot;: 2416, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2013-12-20&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2013-12-20&quot;}, &quot;title&quot;: &quot;Apache Spark 0.8.1 Released&quot;, &quot;slug&quot;: &quot;release-0_8_1&quot;, &quot;content&quot;: &quot;We are happy to announce the release of Apache Spark 0.8.1. In addition to performance and stability improvements, this release adds three new features. First, Spark now supports for the newest versions of YARN (2.2+). Second, the standalone cluster manager supports a high-availability mode in which it can tolerate master failures. Third, shuffles have been optimized to create fewer files, improving shuffle performance drastically in some settings.\\n\\nIn conjunction with the Apache Spark 0.8.1 release we are separately releasing &lt;a href=\\&quot;https://databricks.com/blog/2014/01/01/simr.html\\&quot;&gt;Spark In MapReduce (SIMR)&lt;/a&gt;, which enables seamlessly running Spark on Hadoop MapReduce v1 clusters without requiring the installation of Scala or Spark.\\n\\nWhile Apache Spark 0.8.1 is a minor release, it includes these larger features for the benefit of Scala 2.9 users. The next major release of Apache Spark, 0.9.0, will be based on Scala 2.10.\\n\\nThis release was a community effort, featuring contributions from more than 40 developers. For much more information about Apache Spark 0.8.1, head over to the &lt;a href=\\&quot;http://spark.incubator.apache.org/releases/spark-release-0-8-1.html\\&quot;&gt;release notes&lt;/a&gt; or &lt;a href=\\&quot;http://spark.incubator.apache.org/downloads.html\\&quot;&gt;download Apache Spark 0.8.1&lt;/a&gt; and try it out for yourself.&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;andy&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2013/12/18/spark-summit-2013-follow-up.html&quot;, &quot;authors&quot;: [&quot;Andy Konwinski&quot;], &quot;id&quot;: 2417, &quot;categories&quot;: [&quot;Company Blog&quot;, &quot;Customers&quot;, &quot;Events&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2013-12-19&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2013-12-19&quot;}, &quot;title&quot;: &quot;Highlights From Spark Summit 2013&quot;, &quot;slug&quot;: &quot;spark-summit-2013-follow-up&quot;, &quot;content&quot;: &quot;Earlier this month we held the &lt;a href=\\&quot;http://spark-summit.org/2013\\&quot;&gt;first Spark Summit&lt;/a&gt;, a conference to bring the Apache Spark community together. We are excited to share some statistics and highlights from the event.\\n&lt;ul&gt;\\n \\t&lt;li&gt;450 participants from over 180 companies attended&lt;/li&gt;\\n \\t&lt;li&gt;Participants came from 13 countries&lt;/li&gt;\\n \\t&lt;li&gt;Spark training was sold out at 200 participants from 80 companies&lt;/li&gt;\\n \\t&lt;li&gt;20 organizations sponsored the event, including all major Hadoop platform vendors&lt;/li&gt;\\n \\t&lt;li&gt;20 different organizations gave talks&lt;/li&gt;\\n&lt;/ul&gt;\\nVideos and slides for all talks are now available on the &lt;a href=\\&quot;http://spark-summit.org/2013\\&quot;&gt;Summit 2013 page&lt;/a&gt;.\\n\\nThe Summit included Keynotes from Databricks, the UC Berkeley AMPLab, and Yahoo, as well as presentations from 18 other companies including Amazon, Red Hat, and Adobe. Talk topics covered a wide range including specialized applications such as mapping and manipulating the brain, product launches, and research projects about future directions for the project. We are very excited to see Spark and related projects come such a long way from research prototypes originally developed in AMPLab at Berkeley to being used in production by startups and large companies alike.\\n&lt;h2 id=\\&quot;the-state-of-spark-and-where-were-going-next\\&quot;&gt;The State of Spark, and Where We\\u2019re Going Next&lt;/h2&gt;\\n&lt;iframe style=\\&quot;float: left; padding: 10px;\\&quot; src=\\&quot;//www.youtube.com/embed/nU6vO2EJAb4\\&quot; width=\\&quot;300\\&quot; height=\\&quot;173\\&quot; frameborder=\\&quot;0\\&quot; allowfullscreen=\\&quot;allowfullscreen\\&quot;&gt;&lt;/iframe&gt;\\n\\nIn the first keynote of the day, Matei Zaharia, who started the Spark project and is now CTO at Databricks, gave a summary of recent growth in the projec&lt;span style=\\&quot;letter-spacing: 3px;\\&quot;&gt;t&lt;/span&gt;&lt;a style=\\&quot;vertical-align: super; font-size: 60%;\\&quot; href=\\&quot;#footnote-1\\&quot;&gt;1&lt;/a&gt;, highlighting key contributions from across the community. In particular, Spark recently reached 100 contributors, making it the second-largest open source development community in the Big Data space after Hadoop, and it\\u2019s also overtaken Hadoop in the past 6 months. Matei also previewed what is coming up in the Spark development roadmap, including features under development for the upcoming 0.8.1 and 0.9 releases including high availability for the Spark Master in standalone mode, external hashing and sorting, support for Scala 2.10, and more. Finally, Matei discussed features of Spark that differentiate it from other projects, such as the focus on unification of diverse programming models.\\n&lt;h2 id=\\&quot;spark-and-hadoop\\&quot;&gt;Spark and Hadoop&lt;/h2&gt;\\n&lt;iframe style=\\&quot;float: right; padding-left: 10px;\\&quot; src=\\&quot;//www.youtube.com/embed/qs40jiN2iwM\\&quot; width=\\&quot;300\\&quot; height=\\&quot;173\\&quot; frameborder=\\&quot;0\\&quot; allowfullscreen=\\&quot;allowfullscreen\\&quot;&gt;&lt;/iframe&gt;\\n\\nIt was exciting to hear from Eric Baldeschwieler, whose background includes leading the Yahoo team that took Hadoop from being a prototype project to what it is today, as well as serving as both CEO and CTO of Hortonworks. In his keynote, Eric presented his view of how Spark is on track to become the \\u201clingua franca\\u201d for Big Data. He also talked about how Spark updates Hadoop with important features such as effective utilization of RAM, low latency queries, and streaming ingest. Finally, he discussed how the Spark and Hadoop projects relate to each other now and going forward.\\n&lt;h2 id=\\&quot;spark-training-day\\&quot;&gt;Spark Training Day&lt;/h2&gt;\\nOn the sold-out second day, 200 attendees heard 4 talks on using, deploying, and administering Spark, and also participated in hands-on training led by the creators of Spark. Amazon, a sponsor of the Summit, donated EC2 resources so participants could each have their own 6 node Spark cluster to practice using Spark, Spark Streaming and Shark on real Wikipedia and Twitter data. We have made the hands-on exercises &lt;a href=\\&quot;http://spark-summit.org/exercises\\&quot;&gt;available on the summit website&lt;/a&gt; for free. Using these, anybody can use their own Amazon AWS account to launch a 6 node Spark cluster and get experience analyzing real data.\\n\\n&lt;iframe style=\\&quot;float: left; padding-right: 10px;\\&quot; src=\\&quot;//www.youtube.com/embed/zGW8glN-Mo8\\&quot; width=\\&quot;300\\&quot; height=\\&quot;173\\&quot; frameborder=\\&quot;0\\&quot; allowfullscreen=\\&quot;allowfullscreen\\&quot;&gt;&lt;/iframe&gt;\\n\\nThe final talk of the training day was given by core Spark developer and Databricks cofounder Patrick Wendell. Patrick\\u2019s talk was about Administering Spark and was prepared in response to requests for more advanced technical material as part of the training. In it he dove into the core software components of the project, the different resource managers that Spark runs on, what type of hardware to use when running Spark, how to link against Spark when writing applications, monitoring Spark clusters running in production, and more.\\n&lt;h2 id=\\&quot;other-interesting-talks\\&quot;&gt;Other Interesting Talks&lt;/h2&gt;\\nIn addition to those above, there were many more great talks at the Summit, here are a few more that highligh the diversity of topics covered:\\n&lt;ul class=\\&quot;talk-list\\&quot;&gt;\\n \\t&lt;li&gt;&lt;span class=\\&quot;talk-title\\&quot;&gt;Mapping and manipulating the brain at scale&lt;/span&gt; (&lt;a href=\\&quot;http://spark-summit.org/talk/freeman-mapping-and-manipulating-the-brain-at-scale/\\&quot;&gt;abstract&lt;/a&gt;, &lt;a h\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["To expose the JSON file as a table, use the standard SQL create table using syntax introduced in the previous lesson:"],"metadata":{}},{"cell_type":"code","source":["%sql\nCREATE TABLE IF NOT EXISTS DatabricksBlog\n  USING json\n  OPTIONS (\n    path \"dbfs:/mnt/training/databricks-blog.json\",\n    inferSchema \"true\"\n  )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Take a look at the schema with the `DESCRIBE` function."],"metadata":{}},{"cell_type":"code","source":["%sql\nDESCRIBE DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>authors</td><td>array<string></td><td>null</td></tr><tr><td>categories</td><td>array<string></td><td>null</td></tr><tr><td>content</td><td>string</td><td>null</td></tr><tr><td>creator</td><td>string</td><td>null</td></tr><tr><td>dates</td><td>struct<createdOn:string,publishedOn:string,tz:string></td><td>null</td></tr><tr><td>description</td><td>string</td><td>null</td></tr><tr><td>id</td><td>bigint</td><td>null</td></tr><tr><td>link</td><td>string</td><td>null</td></tr><tr><td>slug</td><td>string</td><td>null</td></tr><tr><td>status</td><td>string</td><td>null</td></tr><tr><td>title</td><td>string</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Run a query to view the contents of the table.\n\nNotice:\n* The `authors` column is an array containing multiple author names.\n* The `categories` column is an array of multiple blog post category names.\n* The `dates` column contains nested fields `createdOn`, `publishedOn` and `tz`."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT authors, categories, dates, content \nFROM DatabricksBlog\nlimit 5"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>authors</th><th>categories</th><th>dates</th><th>content</th></tr></thead><tbody><tr><td>List(Tomer Shiran (VP of Product Management at MapR))</td><td>List(Company Blog, Partners)</td><td>List(2014-04-10, 2014-04-10, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at MapR, announcing our new partnership to provide enterprise support for Apache Spark as part of MapR's Distribution of Hadoop.</div>\n\n<hr />\n\nWith over 500 paying customers, my team and I have the opportunity to talk to many organizations that are leveraging Hadoop in production to extract value from big data. One of the most common topics raised by our customers in recent months is Apache Spark. Some customers just want to learn more about the advantages of this technology and the use cases that it addresses, while others are already running it in production with the MapR Distribution. These customers range from the world’s largest cable telcos and retailers to Silicon Valley startups such as Quantifind, which recently talked about its use of Spark on MapR in an <a href=\"http://www.datameer.com/ceoblog/big-data-brews-with-erich-nachbar/\" target=\"_blank\">interview</a> with Stefan Groschupf, CEO of Datameer.\n\nToday, I am happy to <a href=\"http://www.businesswire.com/news/home/20140410005101/en/MapR-Adds-Complete-Apache-Spark-Stack-Distribution#.U0a0G61dXKI\" target=\"_blank\">announce</a> and share with you the beginning of our journey with Databricks, and the addition of the complete Spark stack to the MapR Distribution for Apache Hadoop. We are now the only Hadoop distribution to support the complete Spark stack, including Spark, Spark Streaming (stream processing), Shark (Hive on Spark), MLLib (machine learning) and GraphX (graph processing). This is a testament to our commitment to open source and to providing our customers with maximum flexibility to pick and choose the right tool for the job.\n<h2 id=\"why-spark\">Why Spark?</h2>\nOne of the challenges organizations face when adopting Hadoop is a shortage of developers who have experience building Hadoop applications. Our professional services organization has helped dozens of companies with the development and deployment of Hadoop applications, and our training department has trained countless engineers. Organizations are hungry for solutions that make it easier to develop Hadoop applications while increasing developer productivity, and Spark fits this bill. Spark jobs can require as little as 1/5th of code. Spark provides a simple programming abstraction allowing developers to design applications as operations on data collections (known as RDDs, or Resilient Distributed Datasets). Developers can build these applications in multiple programming languages, including Java, Scala and Python, and the same code can be reused across batch, interactive and streaming applications.\n\nIn addition to making developers happier and more productive, Spark provides significant benefits with respect to end-to-end application performance. To this end, Spark provides a general-purpose execution framework with in-memory pipelining. For many applications, this results in a 5-100x performance improvement, because some or all steps can execute in memory without unnecessarily writing to and reading from disk. The performance advantage of the Spark engine, combined with the industry-leading performance of the MapR Distribution, provides customers with the highest-performance platform for big data applications.\n<h2 id=\"why-databricks\">Why Databricks?</h2>\nDatabricks was founded by the creators of Apache Spark, and is currently the driving force behind the project. When we decided to add the Spark stack to our distribution and double down on our involvement in the Spark community, a strategic partnership with Databricks was a no-brainer. This partnership will benefit MapR customers who are interested in 24x7 support for Spark or any of the other projects in the stack, including Spark Streaming, Shark, MLLib and GraphX (with several other projects coming soon). In addition, MapR will be working closely with Databricks to drive the Spark roadmap and accelerate the development of new features, benefiting both MapR customers and the broader community.\n\nWe are very excited about the upcoming Apache Spark 1.0 release, expected later this month. We are looking forward to a great journey with Databricks and the other members of the Spark community.\n\n<a href=\"http://w.on24.com/r.htm?e=780379&amp;s=1&amp;k=A6FB573A31B1DD9D8AB6294A101383ED&amp;partnerref=MapR\" target=\"_blank\">Register for an upcoming joint webinar</a> to learn more about the benefits of the complete Spark stack on MapR.</td></tr><tr><td>List(Tathagata Das)</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2014-04-10, 2014-04-10, UTC)</td><td>We are happy to announce the availability of <a href=\"http://spark.apache.org/releases/spark-release-0-9-1.html\" target=\"_blank\">Apache Spark 0.9.1</a>! This is a maintenance release with bug fixes, performance improvements, better stability with YARN and improved parity of the Scala and Python API. We recommend all 0.9.0 users to upgrade to this stable release.\n\nThis is the first release since Spark graduated as a top level Apache project. Contributions to this release came from 37 developers.\n\nVisit the <a href=\"http://spark.apache.org/releases/spark-release-0-9-1.html\" target=\"_blank\">release notes</a> for more information about all the improvements and bug fixes. <a href=\"http://spark.apache.org/downloads.html\" target=\"_blank\">Download</a> it and try it out!</td></tr><tr><td>List(Steven Hillion)</td><td>List(Company Blog, Partners)</td><td>List(2014-04-01, 2014-04-01, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at Alpine Data Labs, part of the 'Application Spotlight' series highlighting innovative applications that are part of the Databricks \"Certified on Apache Spark\" program.</div>\n\n<hr />\n\nEveryone knows how hard it is to recruit engineers and data scientists in Silicon Valley. At <a href=\"http://www.alpinenow.com\" target=\"_blank\">Alpine Data Labs</a>, we think what we’re up to is pretty fun and challenging, but we still have to compete with other start-ups as well as the big internet companies to attract the best talent. One thing that can help is to be able to say that you’re working with the most innovative and powerful technologies.\n\nLast year, I was interviewing a talented engineer with a strong background in machine learning. And he said that the one thing he wanted to do above all was to work with Apache Spark. “Will I get to do that at Alpine?” he asked.\n\nIf it had been even a year earlier, I would have said “Sure…at some point.” But in the meantime I’d met several of the members of the <a href=\"https://amplab.cs.berkeley.edu\" target=\"_blank\">AMPLab</a> research team at Berkeley, and been impressed with their mature approach to building a platform and ecosystem. And I’d seen enough companies installing Spark on their dev clusters that it was clear this was a technology to watch. In a remarkably short time, it went from experimental to very real. And now prospects in the Alpine pipeline were asking me if it was on the roadmap. So yes, I told my candidate. “You’ll be working on Spark from day one.”\n\nLast week, Alpine announced at <a href=\"http://www.cmswire.com/cms/big-data/alpine-turns-big-data-to-a-priceless-asset-fast-gigaomlive-024541.php\" target=\"_blank\">GigaOM</a> that it’s one of the first analytics companies to leverage Spark for building predictive models. We demonstrated the Alpine engine running on <a href=\"http://www.gopivotal.com/big-data/analytics-workbench\" target=\"_blank\">Pivotal’s Analytics Workbench</a>, where it ran an iterative classification algorithm (logistic regression) on 50 million rows in less than 50 seconds.\n\nFurthermore, we were officially certified on Spark by the team at Databricks. It’s been an honor to work with them and the research team at Berkeley. We think their technology will be a serious contender for the leading platform for data science.\n\nSpark is more to us than just speed. It’s really the entire ecosystem that represents such an exciting paradigm for working with data.\n\nStill, the core capability of caching data in memory was our primary consideration, and our iterative algorithms have been shown to speed up by one or even two orders of magnitude (thanks again to that Pivotal cluster).\n\nWe’ve always had this mantra at Alpine: “Avoid multiple passes through the data!” And we’ve designed many of our machine learning algorithms to avoid scanning the data too many times, packing on calculations into each MapReduce job like a waiter piling up plates to try and clear a table in one go. But it’s rare that we can avoid it entirely. With Spark, it’s incredibly satisfying to watch the progress bar zip along as the system re-uses data it’s already seen before.\n\nAnother thing that’s getting our engineers excited is Spark’s <a href=\"http://spark.apache.org/mllib/\" target=\"_blank\">MLLib</a>, the machine-learning library written on top of the Spark runtime. Alpine has long thought that machine learning algorithms should be open source. (I helped to kick off the <a href=\"http://madlib.net/\" target=\"_blank\">MADlib</a> library of analytics functions for databases, and Alpine now uses it extensively.) So we’re now beginning to contribute some of our code back into MLLib. And, moreover, we think MLLib and MLI have the potential to be a more general repository for open-source machine learning.\n\nSo I’ll congratulate the Alpine team for helping to bring the power of Spark to our users, and I’ll also congratulate the Spark team and Databricks for making it possible!</td></tr><tr><td>List(Michael Armbrust, Reynold Xin)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-03-27, 2014-03-27, UTC)</td><td>Building a unified platform for big data analytics has long been the vision of Apache Spark, allowing a single program to perform ETL, MapReduce, and complex analytics. An important aspect of unification that our users have consistently requested is the ability to more easily import data stored in external sources, such as Apache Hive. Today, we are excited to announce <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\">Spark SQL</a>, a new component recently merged into the Spark repository.\n\nSpark SQL brings native support for SQL to Spark and streamlines the process of querying data stored both in RDDs (Spark’s distributed datasets) and in external sources. Spark SQL conveniently blurs the lines between RDDs and relational tables. Unifying these powerful abstractions makes it easy for developers to intermix SQL commands querying external data with complex analytics, all within in a single application. Concretely, Spark SQL will allow developers to:\n<ul>\n \t<li>Import relational data from Parquet files and Hive tables</li>\n \t<li>Run SQL queries over imported data and existing RDDs</li>\n \t<li>Easily write RDDs out to Hive tables or Parquet files</li>\n</ul>\n<h2 id=\"spark-sql-in-action\">Spark SQL In Action</h2>\nNow, let’s take a closer look at how Spark SQL gives developers the power to integrate SQL commands into applications that also take advantage of MLlib, Spark’s machine learning library. Consider an application that needs to predict which users are likely candidates for a service, based on their profile. Often, such an analysis requires joining data from multiple sources. For the purposes of illustration, imagine an application with two tables:\n<ul>\n \t<li>Users(userId INT, name String, email STRING,\nage INT, latitude: DOUBLE, longitude: DOUBLE,\nsubscribed: BOOLEAN)</li>\n \t<li>Events(userId INT, action INT)</li>\n</ul>\nGiven the data stored in in these tables, one might want to build a model that will predict which users are good targets for a new campaign, based on users that are similar.\n\n[scala]\n// Data can easily be extracted from existing sources,\n// such as Apache Hive.\nval trainingDataTable = sql(&quot;&quot;&quot;\n  SELECT e.action\n         u.age,\n         u.latitude,\n         u.logitude\n  FROM Users u\n  JOIN Events e\n  ON u.userId = e.userId&quot;&quot;&quot;)\n\n// Since `sql` returns an RDD, the results of the above\n// query can be easily used in MLlib\nval trainingData = trainingDataTable.map { row =&gt;\n  val features = Array[Double](row(1), row(2), row(3))\n  LabeledPoint(row(0), features)\n}\n\nval model =\n  new LogisticRegressionWithSGD().run(trainingData)\n[/scala]\n\nNow that we have used SQL to join existing data and train a model, we can use this model to predict which users are likely targets.\n\n[scala]\nval allCandidates = sql(&quot;&quot;&quot;\n  SELECT userId,\n         age,\n         latitude,\n         logitude\n  FROM Users\n  WHERE subscribed = FALSE&quot;&quot;&quot;)\n\n// Results of ML algorithms can be used as tables\n// in subsequent SQL statements.\ncase class Score(userId: Int, score: Double)\nval scores = allCandidates.map { row =&gt;\n  val features = Array[Double](row(1), row(2), row(3))\n  Score(row(0), model.predict(features))\n}\nscores.registerAsTable(&quot;Scores&quot;)\n\nval topCandidates = sql(&quot;&quot;&quot;\n  SELECT u.name, u.email\n  FROM Scores s\n    JOIN Users u ON s.userId = u.userId\n  ORDER BY score DESC\n  LIMIT 100&quot;&quot;&quot;)\n\n// Send emails to top candidates to promote the service.\n[/scala]\n\nIn this example, Spark SQL made it easy to extract and join the various datasets preparing them for the machine learning algorithm. Since the results of Spark SQL are also stored in RDDs, interfacing with other Spark libraries is trivial. Furthermore, Spark SQL allows developers to close the loop, by making it easy to manipulate and join the output of these algorithms, producing the desired final result.\n\nTo summarize, the unified Spark platform gives developers the power to choose the right tool for the right job, without having to juggle multiple systems. If you would like to see more concrete examples of using Spark SQL please check out the <a href=\"http://spark.apache.org/docs/1.0.0/sql-programming-guide.html\">programming guide</a>.\n<h2 id=\"optimizing-with-catalyst\">Optimizing with Catalyst</h2>\nIn addition to providing new ways to interact with data, Spark SQL also brings a powerful new optimization framework called Catalyst. Using Catalyst, Spark can automatically transform SQL queries so that they execute more efficiently. The Catalyst framework allows the developers behind Spark SQL to rapidly add new optimizations, enabling us to build a faster system more quickly. In one recent example, we found an inefficiency in Hive group-bys that took an experienced developer an entire weekend and over 250 lines of code to fix; we were then able to make the same fix in Catalyst in only a few lines of code.\n<h2 id=\"future-of-shark\">Future of Shark</h2>\nThe natural question that arises is about the future of Shark. Shark was among the first systems that delivered up to 100X speedup over Hive. It builds on the Apache Hive codebase and achieves performance improvements by swapping out the physical execution engine part of Hive. While this approach enables Shark users to speed up their Hive queries without modification to their existing warehouses, Shark inherits the large, complicated code base from Hive that makes it hard to optimize and maintain. As Spark SQL matures, Shark will transition to using Spark SQL for query optimization and physical execution so that users can benefit from the ongoing optimization efforts within Spark SQL.\n\nIn short, we will continue to invest in Shark and make it an excellent drop-in replacement for Apache Hive. It will take advantage of the new Spark SQL component, and will provide features that complement it, such as Hive compatibility and the standalone SharkServer, which allows external tools to connect queries through JDBC/ODBC.\n<h2 id=\"whats-next\">What’s next</h2>\nSpark SQL will be included in Spark 1.0 as an alpha component. However, this is only the beginning of better support for relational data in Spark, and this post only scratches the surface of Catalyst. Look for future blog posts on the following topics:\n<ul>\n \t<li>Generating custom bytecode to speed up expression evaluation</li>\n \t<li>Reading and writing data using other formats and systems, include Avro and HBase</li>\n \t<li>API support for using Spark SQL in Python and Java</li>\n</ul></td></tr><tr><td>List(Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-02-04, 2014-02-04, UTC)</td><td>Our goal with Apache Spark is very simple: provide the best platform for computation on big data. We do this through both a powerful core engine and rich libraries for useful analytics tasks. Today, we are excited to announce the release of Apache Spark 0.9.0. This major release extends Spark’s libraries and further improves its performance and usability. Apache Spark 0.9.0 is the largest release to date, with work from 83 contributors, who submitted over 300 patches.\n\nApache Spark 0.9 features significant extensions to the set of standard analytical libraries packaged with Spark. The release introduces GraphX, a library for graph computation that comes with implementations of several standard algorithms, such as PageRank. Spark’s machine learning library (MLlib) has been extended to support Python, using the NumPy numerical library. A Naive Bayes Classifier has also been added to MLlib. Finally, Spark Streaming, which supports near-real-time continuous computation, has added a simplified high-availability mode and several significant optimizations.\n\nIn addition to higher-level libraries, Spark 0.9 features improvements to the core computation engine. Spark now now automatically spills reduce output to disk, increasing the stability of workloads with very large aggregations. Support for Spark in YARN mode has been hardened and improved. The standalone mode has added automatic supervision of applications and better support for sharing clusters amongst several users. Finally, we’ve focused on stabilizing API’s ahead of Apache Spark’s 1.0 release to make things easy for developers writing Spark applications. This includes upgrading to Scala 2.10, allowing applications written in Scala to use newer libraries.\n\nApache Spark 0.9.0 can be <a href=\"http://spark.incubator.apache.org/downloads.html\">downloaded directly</a> from the Apache Spark website. It will also be available to CDH users via a Cloudera parcel, which can automatically install Spark on existing CDH clusters. For a more detailed explanation of the features in this release, head on over to the <a href=\"http://spark.incubator.apache.org/releases/spark-release-0-9-0.html\">official release notes</a>. Enjoy the newest release of Spark!</td></tr></tbody></table></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["## Nested Data\n\nThink of nested data as columns within columns. \n\nFor instance, look at the `dates` column."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/kqmfblujy9?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/kqmfblujy9?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT dates FROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dates</th></tr></thead><tbody><tr><td>List(2014-04-10, 2014-04-10, UTC)</td></tr><tr><td>List(2014-04-10, 2014-04-10, UTC)</td></tr><tr><td>List(2014-04-01, 2014-04-01, UTC)</td></tr><tr><td>List(2014-03-27, 2014-03-27, UTC)</td></tr><tr><td>List(2014-02-04, 2014-02-04, UTC)</td></tr><tr><td>List(2014-01-02, 2014-01-02, UTC)</td></tr><tr><td>List(2014-03-26, 2014-03-26, UTC)</td></tr><tr><td>List(2014-03-21, 2014-03-21, UTC)</td></tr><tr><td>List(2014-03-19, 2014-03-19, UTC)</td></tr><tr><td>List(2014-03-03, 2014-03-03, UTC)</td></tr></tbody></table></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["Pull out a specific subfield with \"dot\" notation."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT dates.createdOn, dates.publishedOn, dates.tz\nFROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>createdOn</th><th>publishedOn</th><th>tz</th></tr></thead><tbody><tr><td>2014-04-10</td><td>2014-04-10</td><td>UTC</td></tr><tr><td>2014-04-10</td><td>2014-04-10</td><td>UTC</td></tr><tr><td>2014-04-01</td><td>2014-04-01</td><td>UTC</td></tr><tr><td>2014-03-27</td><td>2014-03-27</td><td>UTC</td></tr><tr><td>2014-02-04</td><td>2014-02-04</td><td>UTC</td></tr><tr><td>2014-01-02</td><td>2014-01-02</td><td>UTC</td></tr><tr><td>2014-03-26</td><td>2014-03-26</td><td>UTC</td></tr><tr><td>2014-03-21</td><td>2014-03-21</td><td>UTC</td></tr><tr><td>2014-03-19</td><td>2014-03-19</td><td>UTC</td></tr><tr><td>2014-03-03</td><td>2014-03-03</td><td>UTC</td></tr></tbody></table></div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["Both `createdOn` and `publishedOn` are stored as strings.\n\nCast those values to SQL timestamps:\n\nIn this case, use a single `SELECT` statement to:\n0. Cast `dates.publishedOn` to a `timestamp` data type.\n0. \"Flatten\" the `dates.publishedOn` column to just `publishedOn`."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT title, \n       cast(dates.publishedOn AS timestamp) AS publishedOn \nFROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>publishedOn</th></tr></thead><tbody><tr><td>MapR Integrates the Complete Apache Spark Stack</td><td>2014-04-10T00:00:00.000+0000</td></tr><tr><td>Apache Spark 0.9.1 Released</td><td>2014-04-10T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Alpine Data Labs</td><td>2014-04-01T00:00:00.000+0000</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>2014-03-27T00:00:00.000+0000</td></tr><tr><td>Apache Spark 0.9.0 Released</td><td>2014-02-04T00:00:00.000+0000</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>2014-01-02T00:00:00.000+0000</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>2014-03-26T00:00:00.000+0000</td></tr><tr><td>Apache Spark: A Delight for Developers</td><td>2014-03-21T00:00:00.000+0000</td></tr><tr><td>Databricks announces \"Certified on Apache Spark\" Program</td><td>2014-03-19T00:00:00.000+0000</td></tr><tr><td>Apache Spark Now a Top-level Apache Project</td><td>2014-03-03T00:00:00.000+0000</td></tr></tbody></table></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["Create the temporary view `DatabricksBlog2` to capture the conversion and flattening of the `publishedOn` column."],"metadata":{}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW DatabricksBlog2 AS\n  SELECT *, \n         cast(dates.publishedOn AS timestamp) AS publishedOn \n  FROM DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["Now that we have this temporary view, we can use `DESCRIBE` to check its schema and confirm the timestamp conversion."],"metadata":{}},{"cell_type":"code","source":["%sql\nDESCRIBE DatabricksBlog2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>authors</td><td>array<string></td><td>null</td></tr><tr><td>categories</td><td>array<string></td><td>null</td></tr><tr><td>content</td><td>string</td><td>null</td></tr><tr><td>creator</td><td>string</td><td>null</td></tr><tr><td>dates</td><td>struct<createdOn:string,publishedOn:string,tz:string></td><td>null</td></tr><tr><td>description</td><td>string</td><td>null</td></tr><tr><td>id</td><td>bigint</td><td>null</td></tr><tr><td>link</td><td>string</td><td>null</td></tr><tr><td>slug</td><td>string</td><td>null</td></tr><tr><td>status</td><td>string</td><td>null</td></tr><tr><td>title</td><td>string</td><td>null</td></tr><tr><td>publishedOn</td><td>timestamp</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["-sandbox\nNow the dates are represented by a `timestamp` data type, query for articles within certain date ranges (such as getting a list of all articles published in 2013), and format the date for presentation purposes.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See the Spark documentation, <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\" target=\"_blank\">built-in functions</a>, for a long list of date-specific functions."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT authors, categories,\nTRANSFORM (categories, category -> LOWER(category)) AS lwr_categories\nFROM DatabricksBlog\nlimit 20"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>authors</th><th>categories</th><th>lwr_categories</th></tr></thead><tbody><tr><td>List(Tomer Shiran (VP of Product Management at MapR))</td><td>List(Company Blog, Partners)</td><td>List(company blog, partners)</td></tr><tr><td>List(Tathagata Das)</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(apache spark, engineering blog, machine learning)</td></tr><tr><td>List(Steven Hillion)</td><td>List(Company Blog, Partners)</td><td>List(company blog, partners)</td></tr><tr><td>List(Michael Armbrust, Reynold Xin)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Ali Ghodsi, Ahir Reddy)</td><td>List(Apache Spark, Ecosystem, Engineering Blog)</td><td>List(apache spark, ecosystem, engineering blog)</td></tr><tr><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td><td>List(Company Blog, Customers)</td><td>List(company blog, customers)</td></tr><tr><td>List(Jai Ranganathan, Matei Zaharia)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Databricks Press Office)</td><td>List(Announcements, Company Blog)</td><td>List(announcements, company blog)</td></tr><tr><td>List(Ion Stoica)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Ahir Reddy, Reynold Xin)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Pat McDonough)</td><td>List(Company Blog, Events)</td><td>List(company blog, events)</td></tr><tr><td>List(Ion Stoica)</td><td>List(Apache Spark, Ecosystem, Engineering Blog)</td><td>List(apache spark, ecosystem, engineering blog)</td></tr><tr><td>List(Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Andy Konwinski)</td><td>List(Company Blog, Customers, Events)</td><td>List(company blog, customers, events)</td></tr><tr><td>List(Pat McDonough)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Ion Stoica)</td><td>List(Company Blog, Partners)</td><td>List(company blog, partners)</td></tr><tr><td>List(Matei Zaharia)</td><td>List(Announcements, Company Blog)</td><td>List(announcements, company blog)</td></tr><tr><td>List(Ion Stoica, Matei Zaharia)</td><td>List(Announcements, Company Blog)</td><td>List(announcements, company blog)</td></tr><tr><td>List(Arsalan Tavakoli-Shiraji)</td><td>List(Company Blog, Partners)</td><td>List(company blog, partners)</td></tr></tbody></table></div>"]}}],"execution_count":27},{"cell_type":"code","source":["%sql\nSELECT title,\n  FILTER (categories, category -> category = \"Apache Spark\") filtered\nFROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>filtered</th></tr></thead><tbody><tr><td>MapR Integrates the Complete Apache Spark Stack</td><td>List()</td></tr><tr><td>Apache Spark 0.9.1 Released</td><td>List(Apache Spark)</td></tr><tr><td>Application Spotlight: Alpine Data Labs</td><td>List()</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>List(Apache Spark)</td></tr><tr><td>Apache Spark 0.9.0 Released</td><td>List(Apache Spark)</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>List(Apache Spark)</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>List()</td></tr><tr><td>Apache Spark: A Delight for Developers</td><td>List(Apache Spark)</td></tr><tr><td>Databricks announces \"Certified on Apache Spark\" Program</td><td>List()</td></tr><tr><td>Apache Spark Now a Top-level Apache Project</td><td>List(Apache Spark)</td></tr></tbody></table></div>"]}}],"execution_count":28},{"cell_type":"code","source":["%sql\nSELECT title,\n  EXISTS (authors, author -> author = \"Reynold Xin\" \n    OR author = \"Ion Stoica\") selected\nFROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>selected</th></tr></thead><tbody><tr><td>MapR Integrates the Complete Apache Spark Stack</td><td>false</td></tr><tr><td>Apache Spark 0.9.1 Released</td><td>false</td></tr><tr><td>Application Spotlight: Alpine Data Labs</td><td>false</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>true</td></tr><tr><td>Apache Spark 0.9.0 Released</td><td>false</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>false</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>false</td></tr><tr><td>Apache Spark: A Delight for Developers</td><td>false</td></tr><tr><td>Databricks announces \"Certified on Apache Spark\" Program</td><td>false</td></tr><tr><td>Apache Spark Now a Top-level Apache Project</td><td>true</td></tr></tbody></table></div>"]}}],"execution_count":29},{"cell_type":"code","source":["%sql\nSELECT title, \n       date_format(publishedOn, \"MMM dd, yyyy\") AS date, \n       link,\n       year(publishedOn) as year\nFROM DatabricksBlog2\nWHERE year(publishedOn) = 2013\nORDER BY publishedOn\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>date</th><th>link</th><th>year</th></tr></thead><tbody><tr><td>Databricks and the Apache Spark Platform</td><td>Oct 27, 2013</td><td>https://databricks.com/blog/2013/10/27/databricks-and-the-apache-spark-platform.html</td><td>2013</td></tr><tr><td>The Growing Apache Spark Community</td><td>Oct 28, 2013</td><td>https://databricks.com/blog/2013/10/27/the-growing-spark-community.html</td><td>2013</td></tr><tr><td>Databricks and Cloudera Partner to Support Apache Spark</td><td>Oct 29, 2013</td><td>https://databricks.com/blog/2013/10/28/databricks-and-cloudera-partner-to-support-spark.html</td><td>2013</td></tr><tr><td>Putting Apache Spark to Use: Fast In-Memory Computing for Your Big Data Applications</td><td>Nov 22, 2013</td><td>https://databricks.com/blog/2013/11/21/putting-spark-to-use.html</td><td>2013</td></tr><tr><td>Highlights From Spark Summit 2013</td><td>Dec 19, 2013</td><td>https://databricks.com/blog/2013/12/18/spark-summit-2013-follow-up.html</td><td>2013</td></tr><tr><td>Apache Spark 0.8.1 Released</td><td>Dec 20, 2013</td><td>https://databricks.com/blog/2013/12/19/release-0_8_1.html</td><td>2013</td></tr></tbody></table></div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["## Array Data\n\nThe table also contains array columns. \n\nEasily determine the size of each array using the built-in `size(..)` function with array columns."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/w9vj8mjpf7?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/w9vj8mjpf7?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT size(authors), \n       authors \nFROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>size(authors)</th><th>authors</th></tr></thead><tbody><tr><td>1</td><td>List(Tomer Shiran (VP of Product Management at MapR))</td></tr><tr><td>1</td><td>List(Tathagata Das)</td></tr><tr><td>1</td><td>List(Steven Hillion)</td></tr><tr><td>2</td><td>List(Michael Armbrust, Reynold Xin)</td></tr><tr><td>1</td><td>List(Patrick Wendell)</td></tr><tr><td>2</td><td>List(Ali Ghodsi, Ahir Reddy)</td></tr><tr><td>2</td><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td></tr><tr><td>2</td><td>List(Jai Ranganathan, Matei Zaharia)</td></tr><tr><td>1</td><td>List(Databricks Press Office)</td></tr><tr><td>1</td><td>List(Ion Stoica)</td></tr></tbody></table></div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["Pull the first element from the array `authors` using an array subscript operator."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT authors[0] AS primaryAuthor \nFROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>primaryAuthor</th></tr></thead><tbody><tr><td>Tomer Shiran (VP of Product Management at MapR)</td></tr><tr><td>Tathagata Das</td></tr><tr><td>Steven Hillion</td></tr><tr><td>Michael Armbrust</td></tr><tr><td>Patrick Wendell</td></tr><tr><td>Ali Ghodsi</td></tr><tr><td>Russell Cardullo (Data Infrastructure Engineer at Sharethrough)</td></tr><tr><td>Jai Ranganathan</td></tr><tr><td>Databricks Press Office</td></tr><tr><td>Ion Stoica</td></tr></tbody></table></div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["### Explode\n\nThe `explode` function allows you to split an array column into multiple rows, copying all the other columns into each new row. \n\nFor example, you can split the column `authors` into the column `author`, with one author per row."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/h8tv263d04?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/h8tv263d04?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT title, \n       authors, \n       explode(authors) AS author, \n       link \nFROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>authors</th><th>author</th><th>link</th></tr></thead><tbody><tr><td>MapR Integrates the Complete Apache Spark Stack</td><td>List(Tomer Shiran (VP of Product Management at MapR))</td><td>Tomer Shiran (VP of Product Management at MapR)</td><td>https://databricks.com/blog/2014/04/10/mapr-integrates-spark-stack.html</td></tr><tr><td>Apache Spark 0.9.1 Released</td><td>List(Tathagata Das)</td><td>Tathagata Das</td><td>https://databricks.com/blog/2014/04/09/spark-0_9_1-released.html</td></tr><tr><td>Application Spotlight: Alpine Data Labs</td><td>List(Steven Hillion)</td><td>Steven Hillion</td><td>https://databricks.com/blog/2014/03/31/application-spotlight-alpine.html</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>List(Michael Armbrust, Reynold Xin)</td><td>Michael Armbrust</td><td>https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>List(Michael Armbrust, Reynold Xin)</td><td>Reynold Xin</td><td>https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html</td></tr><tr><td>Apache Spark 0.9.0 Released</td><td>List(Patrick Wendell)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2014/02/03/release-0_9_0.html</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>List(Ali Ghodsi, Ahir Reddy)</td><td>Ali Ghodsi</td><td>https://databricks.com/blog/2014/01/01/simr.html</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>List(Ali Ghodsi, Ahir Reddy)</td><td>Ahir Reddy</td><td>https://databricks.com/blog/2014/01/01/simr.html</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td><td>Russell Cardullo (Data Infrastructure Engineer at Sharethrough)</td><td>https://databricks.com/blog/2014/03/25/sharethrough-and-spark-streaming.html</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td><td>Michael Ruggiero (Data Infrastructure Engineer at Sharethrough)</td><td>https://databricks.com/blog/2014/03/25/sharethrough-and-spark-streaming.html</td></tr></tbody></table></div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["It's more obvious to restrict the output to articles that have multiple authors, and sort by the title."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT title, \n       authors, \n       explode(authors) AS author, \n       link \nFROM DatabricksBlog \nWHERE size(authors) > 1 \nORDER BY title\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>authors</th><th>author</th><th>link</th></tr></thead><tbody><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Matei Zaharia</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Holden Karau</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Andy Konwinski</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>AMPLab updates the Big Data Benchmark</td><td>List(Ahir Reddy, Reynold Xin)</td><td>Ahir Reddy</td><td>https://databricks.com/blog/2014/02/12/big-data-benchmark.html</td></tr><tr><td>AMPLab updates the Big Data Benchmark</td><td>List(Ahir Reddy, Reynold Xin)</td><td>Reynold Xin</td><td>https://databricks.com/blog/2014/02/12/big-data-benchmark.html</td></tr><tr><td>Announcing Apache Spark Packages</td><td>List(Xiangrui Meng, Patrick Wendell)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2014/12/22/announcing-spark-packages.html</td></tr><tr><td>Announcing Apache Spark Packages</td><td>List(Xiangrui Meng, Patrick Wendell)</td><td>Xiangrui Meng</td><td>https://databricks.com/blog/2014/12/22/announcing-spark-packages.html</td></tr><tr><td>Apache Spark 1.1: Bringing Hadoop Input/Output Formats to PySpark</td><td>List(Nick Pentreath (Graphflow), Kan Zhang (IBM))</td><td>Kan Zhang (IBM)</td><td>https://databricks.com/blog/2014/09/17/spark-1-1-bringing-hadoop-inputoutput-formats-to-pyspark.html</td></tr><tr><td>Apache Spark 1.1: Bringing Hadoop Input/Output Formats to PySpark</td><td>List(Nick Pentreath (Graphflow), Kan Zhang (IBM))</td><td>Nick Pentreath (Graphflow)</td><td>https://databricks.com/blog/2014/09/17/spark-1-1-bringing-hadoop-inputoutput-formats-to-pyspark.html</td></tr></tbody></table></div>"]}}],"execution_count":40},{"cell_type":"markdown","source":["### Lateral View\nThe data has multiple columns with nested objects.  In this case, the data has multiple dates, authors, and categories.\n\nTake a look at the blog entry **Apache Spark 1.1: The State of Spark Streaming**:"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT dates.publishedOn, title, authors, categories\nFROM DatabricksBlog\nWHERE title = \"Apache Spark 1.1: The State of Spark Streaming\"\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>publishedOn</th><th>title</th><th>authors</th><th>categories</th></tr></thead><tbody><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>List(Arsalan Tavakoli-Shiraji, Tathagata Das, Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog, Streaming)</td></tr></tbody></table></div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["Next, use `LATERAL VIEW` to explode multiple columns at once, in this case, the columns `authors` and `categories`."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT dates.publishedOn, title, author, category\nFROM DatabricksBlog\nLATERAL VIEW explode(authors) exploded_authors_view AS author\nLATERAL VIEW explode(categories) exploded_categories AS category\nWHERE title = \"Apache Spark 1.1: The State of Spark Streaming\"\nORDER BY author, category\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>publishedOn</th><th>title</th><th>author</th><th>category</th></tr></thead><tbody><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Arsalan Tavakoli-Shiraji</td><td>Apache Spark</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Arsalan Tavakoli-Shiraji</td><td>Engineering Blog</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Arsalan Tavakoli-Shiraji</td><td>Streaming</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Patrick Wendell</td><td>Apache Spark</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Patrick Wendell</td><td>Engineering Blog</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Patrick Wendell</td><td>Streaming</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Tathagata Das</td><td>Apache Spark</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Tathagata Das</td><td>Engineering Blog</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Tathagata Das</td><td>Streaming</td></tr></tbody></table></div>"]}}],"execution_count":44},{"cell_type":"markdown","source":["## Exercise 1\n\nIdentify all the articles written or co-written by Michael Armbrust."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 1\n\nStarting with the table `DatabricksBlog`, create a temporary view called `ArticlesByMichael` where:\n0. Michael Armbrust is the author\n0. The data set contains the column `title` (it may contain others)\n0. It contains only one record per article\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** See the Spark documentation, <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\" target=\"_blank\">built-in functions</a>.  \n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** Include the column `authors` in your view, to help you debug your solution."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nresultsDF = spark.sql(\"select title from ArticlesByMichael order by title\")\ndbTest(\"SQL-L5-articlesByMichael-count\", 3, resultsDF.count())\n\nresults = [r[0] for r in resultsDF.collect()]\ndbTest(\"SQL-L5-articlesByMichael-0\", \"Exciting Performance Improvements on the Horizon for Spark SQL\", results[0])\ndbTest(\"SQL-L5-articlesByMichael-1\", \"Spark SQL Data Sources API: Unified Data Access for the Apache Spark Platform\", results[1])\ndbTest(\"SQL-L5-articlesByMichael-2\", \"Spark SQL: Manipulating Structured Data Using Apache Spark\", results[2])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["### Step 2\nShow the list of Michael Armbrust's articles."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["## Exercise 2\n\nIdentify the complete set of categories used in the Databricks blog articles."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1\n\nStarting with the table `DatabricksBlog`, create another view called `UniqueCategories` where:\n0. The data set contains the one column `category` (and no others)\n0. This list of categories should be unique"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nresultsCount = spark.sql(\"SELECT category FROM UniqueCategories order by category\")\n\ndbTest(\"SQL-L5-uniqueCategories-count\", 12, resultsCount.count())\n\nresults = [r[0] for r in resultsCount.collect()]\ndbTest(\"SQL-L5-uniqueCategories-0\", \"Announcements\", results[0])\ndbTest(\"SQL-L5-uniqueCategories-1\", \"Apache Spark\", results[1])\ndbTest(\"SQL-L5-uniqueCategories-2\", \"Company Blog\", results[2])\n\ndbTest(\"SQL-L5-uniqueCategories-9\", \"Platform\", results[9])\ndbTest(\"SQL-L5-uniqueCategories-10\", \"Product\", results[10])\ndbTest(\"SQL-L5-uniqueCategories-11\", \"Streaming\", results[11])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["### Step 2\nShow the complete list of categories."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["## Exercise 3\n\nCount how many times each category is referenced in the Databricks blog."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 1\n\nStarting with the table `DatabricksBlog`, create a temporary view called `TotalArticlesByCategory` where:\n0. The new table contains two columns, `category` and `total`\n0. The `category` column is a single, distinct category (similar to the last exercise)\n0. The `total` column is the total number of articles in that category\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** You need either multiple views or a `LATERAL VIEW` to solve this.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Because articles can be tagged with multiple categories, the sum of the totals adds up to more than the total number of articles."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nresultsDF = spark.sql(\"SELECT category, total FROM TotalArticlesByCategory ORDER BY category\")\ndbTest(\"SQL-L5-articlesByCategory-count\", 12, resultsDF.count())\n\nresults = [ (r[0]+\" w/\"+str(r[1])) for r in resultsDF.collect()]\n\ndbTest(\"SQL-L5-articlesByCategory-0\", \"Announcements w/72\", results[0])\ndbTest(\"SQL-L5-articlesByCategory-1\", \"Apache Spark w/132\", results[1])\ndbTest(\"SQL-L5-articlesByCategory-2\", \"Company Blog w/224\", results[2])\n\ndbTest(\"SQL-L5-articlesByCategory-9\", \"Platform w/4\", results[9])\ndbTest(\"SQL-L5-articlesByCategory-10\", \"Product w/83\", results[10])\ndbTest(\"SQL-L5-articlesByCategory-11\", \"Streaming w/21\", results[11])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["### Step 2\nDisplay the totals of each category, order by `category`."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["## Summary\n\n* Spark SQL allows you to query and manipulate structured and semi-structured data\n* Spark SQL's built-in functions provide powerful primitives for querying complex schemas"],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n**Q:** What is the syntax for accessing nested columns?  \n**A:** Use the dot notation: ```SELECT dates.publishedOn```\n\n**Q:** What is the syntax for accessing the first element in an array?  \n**A:** Use the [subscript] notation:  ```SELECT authors[0]```\n\n**Q:** What is the syntax for expanding an array into multiple rows?  \n**A:** Use the explode keyword, either:  \n```SELECT explode(authors) as Author``` or  \n```LATERAL VIEW explode(authors) exploded_authors_view AS author```"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Querying Data Lakes with SQL]($./SSQL 06 - Data Lakes)."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"https://docs.databricks.com/spark/latest/spark-sql/index.html\" target=\"_blank\">Spark SQL Reference</a>\n* <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\" target=\"_blank\">Spark SQL, DataFrames and Datasets Guide</a>\n* <a href=\"https://stackoverflow.com/questions/36876959/sparksql-can-i-explode-two-different-variables-in-the-same-query\" target=\"_blank\">SparkSQL: Can I explode two different variables in the same query? (StackOverflow)</a>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"SSQL 05 - Querying JSON","notebookId":3953685866790579},"nbformat":4,"nbformat_minor":0}
