{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Querying JSON & Hierarchical Data with SQL\n\nApache Spark&trade; and Databricks&reg; make it easy to work with hierarchical data, such as nested JSON records.\n\n## In this lesson you:\n* Use SQL to query a table backed by JSON data\n* Query nested structured data\n* Query data containing array columns \n\n## Audience\n* Primary Audience: Data Analysts\n* Additional Audiences: Data Engineers and Data Scientists\n\n## Prerequisites\n* Web browser: **Chrome**\n* A cluster configured with **8 cores** and **DBR 6.2**\n* Familiarity with <a href=\"https://www.w3schools.com/sql/\" target=\"_blank\">ANSI SQL</a> is required"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the<br/>\nstart of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/a3098jg2t0?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/a3098jg2t0?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## Examining the contents of a JSON file\n\nJSON is a common file format in big data applications and in data lakes (or large stores of diverse data).  Datatypes such as JSON arise out of a number of data needs.  For instance, what if...  \n<br>\n* Your schema, or the structure of your data, changes over time?\n* You need nested fields like an array with many values or an array of arrays?\n* You don't know how you're going use your data yet so you don't want to spend time creating relational tables?\n\nThe popularity of JSON is largely due to the fact that JSON allows for nested, flexible schemas.\n\nThis lesson uses the `DatabricksBlog` table, which is backed by JSON file `dbfs:/mnt/training/databricks-blog.json`. If you examine the raw file, you can see that it contains compact JSON data. There's a single JSON object on each line of the file; each object corresponds to a row in the table. Each row represents a blog post on the <a href=\"https://databricks.com/blog\" target=\"_blank\">Databricks blog</a>, and the table contains all blog posts through August 9, 2017."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/1i3n3rb0vy?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/1i3n3rb0vy?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%fs head dbfs:/mnt/training/databricks-blog.json"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Truncated to first 65536 bytes]\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;roy&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/04/10/mapr-integrates-spark-stack.html&quot;, &quot;authors&quot;: [&quot;Tomer Shiran (VP of Product Management at MapR)&quot;], &quot;id&quot;: 33, &quot;categories&quot;: [&quot;Company Blog&quot;, &quot;Partners&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-04-10&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-04-10&quot;}, &quot;title&quot;: &quot;MapR Integrates the Complete Apache Spark Stack&quot;, &quot;slug&quot;: &quot;mapr-integrates-spark-stack&quot;, &quot;content&quot;: &quot;&lt;div class=\\&quot;post-meta\\&quot;&gt;This post is guest authored by our friends at MapR, announcing our new partnership to provide enterprise support for Apache Spark as part of MapR's Distribution of Hadoop.&lt;/div&gt;\\n\\n&lt;hr /&gt;\\n\\nWith over 500 paying customers, my team and I have the opportunity to talk to many organizations that are leveraging Hadoop in production to extract value from big data. One of the most common topics raised by our customers in recent months is Apache Spark. Some customers just want to learn more about the advantages of this technology and the use cases that it addresses, while others are already running it in production with the MapR Distribution. These customers range from the world\\u2019s largest cable telcos and retailers to Silicon Valley startups such as Quantifind, which recently talked about its use of Spark on MapR in an &lt;a href=\\&quot;http://www.datameer.com/ceoblog/big-data-brews-with-erich-nachbar/\\&quot; target=\\&quot;_blank\\&quot;&gt;interview&lt;/a&gt; with Stefan Groschupf, CEO of Datameer.\\n\\nToday, I am happy to &lt;a href=\\&quot;http://www.businesswire.com/news/home/20140410005101/en/MapR-Adds-Complete-Apache-Spark-Stack-Distribution#.U0a0G61dXKI\\&quot; target=\\&quot;_blank\\&quot;&gt;announce&lt;/a&gt; and share with you the beginning of our journey with Databricks, and the addition of the complete Spark stack to the MapR Distribution for Apache Hadoop. We are now the only Hadoop distribution to support the complete Spark stack, including Spark, Spark Streaming (stream processing), Shark (Hive on Spark), MLLib (machine learning) and GraphX (graph processing). This is a testament to our commitment to open source and to providing our customers with maximum flexibility to pick and choose the right tool for the job.\\n&lt;h2 id=\\&quot;why-spark\\&quot;&gt;Why Spark?&lt;/h2&gt;\\nOne of the challenges organizations face when adopting Hadoop is a shortage of developers who have experience building Hadoop applications. Our professional services organization has helped dozens of companies with the development and deployment of Hadoop applications, and our training department has trained countless engineers. Organizations are hungry for solutions that make it easier to develop Hadoop applications while increasing developer productivity, and Spark fits this bill. Spark jobs can require as little as 1/5th of code. Spark provides a simple programming abstraction allowing developers to design applications as operations on data collections (known as RDDs, or Resilient Distributed Datasets). Developers can build these applications in multiple programming languages, including Java, Scala and Python, and the same code can be reused across batch, interactive and streaming applications.\\n\\nIn addition to making developers happier and more productive, Spark provides significant benefits with respect to end-to-end application performance. To this end, Spark provides a general-purpose execution framework with in-memory pipelining. For many applications, this results in a 5-100x performance improvement, because some or all steps can execute in memory without unnecessarily writing to and reading from disk. The performance advantage of the Spark engine, combined with the industry-leading performance of the MapR Distribution, provides customers with the highest-performance platform for big data applications.\\n&lt;h2 id=\\&quot;why-databricks\\&quot;&gt;Why Databricks?&lt;/h2&gt;\\nDatabricks was founded by the creators of Apache Spark, and is currently the driving force behind the project. When we decided to add the Spark stack to our distribution and double down on our involvement in the Spark community, a strategic partnership with Databricks was a no-brainer. This partnership will benefit MapR customers who are interested in 24x7 support for Spark or any of the other projects in the stack, including Spark Streaming, Shark, MLLib and GraphX (with several other projects coming soon). In addition, MapR will be working closely with Databricks to drive the Spark roadmap and accelerate the development of new features, benefiting both MapR customers and the broader community.\\n\\nWe are very excited about the upcoming Apache Spark 1.0 release, expected later this month. We are looking forward to a great journey with Databricks and the other members of the Spark community.\\n\\n&lt;a href=\\&quot;http://w.on24.com/r.htm?e=780379&amp;amp;s=1&amp;amp;k=A6FB573A31B1DD9D8AB6294A101383ED&amp;amp;partnerref=MapR\\&quot; target=\\&quot;_blank\\&quot;&gt;Register for an upcoming joint webinar&lt;/a&gt; to learn more about the benefits of the complete Spark stack on MapR.&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;tdas&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/04/09/spark-0_9_1-released.html&quot;, &quot;authors&quot;: [&quot;Tathagata Das&quot;], &quot;id&quot;: 35, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;, &quot;Machine Learning&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-04-10&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-04-10&quot;}, &quot;title&quot;: &quot;Apache Spark 0.9.1 Released&quot;, &quot;slug&quot;: &quot;spark-0_9_1-released&quot;, &quot;content&quot;: &quot;We are happy to announce the availability of &lt;a href=\\&quot;http://spark.apache.org/releases/spark-release-0-9-1.html\\&quot; target=\\&quot;_blank\\&quot;&gt;Apache Spark 0.9.1&lt;/a&gt;! This is a maintenance release with bug fixes, performance improvements, better stability with YARN and improved parity of the Scala and Python API. We recommend all 0.9.0 users to upgrade to this stable release.\\n\\nThis is the first release since Spark graduated as a top level Apache project. Contributions to this release came from 37 developers.\\n\\nVisit the &lt;a href=\\&quot;http://spark.apache.org/releases/spark-release-0-9-1.html\\&quot; target=\\&quot;_blank\\&quot;&gt;release notes&lt;/a&gt; for more information about all the improvements and bug fixes. &lt;a href=\\&quot;http://spark.apache.org/downloads.html\\&quot; target=\\&quot;_blank\\&quot;&gt;Download&lt;/a&gt; it and try it out!&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;roy&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/03/31/application-spotlight-alpine.html&quot;, &quot;authors&quot;: [&quot;Steven Hillion&quot;], &quot;id&quot;: 37, &quot;categories&quot;: [&quot;Company Blog&quot;, &quot;Partners&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-04-01&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-04-01&quot;}, &quot;title&quot;: &quot;Application Spotlight: Alpine Data Labs&quot;, &quot;slug&quot;: &quot;application-spotlight-alpine&quot;, &quot;content&quot;: &quot;&lt;div class=\\&quot;post-meta\\&quot;&gt;This post is guest authored by our friends at Alpine Data Labs, part of the 'Application Spotlight' series highlighting innovative applications that are part of the Databricks \\&quot;Certified on Apache Spark\\&quot; program.&lt;/div&gt;\\n\\n&lt;hr /&gt;\\n\\nEveryone knows how hard it is to recruit engineers and data scientists in Silicon Valley. At &lt;a href=\\&quot;http://www.alpinenow.com\\&quot; target=\\&quot;_blank\\&quot;&gt;Alpine Data Labs&lt;/a&gt;, we think what we\\u2019re up to is pretty fun and challenging, but we still have to compete with other start-ups as well as the big internet companies to attract the best talent. One thing that can help is to be able to say that you\\u2019re working with the most innovative and powerful technologies.\\n\\nLast year, I was interviewing a talented engineer with a strong background in machine learning. And he said that the one thing he wanted to do above all was to work with Apache Spark. \\u201cWill I get to do that at Alpine?\\u201d he asked.\\n\\nIf it had been even a year earlier, I would have said \\u201cSure\\u2026at some point.\\u201d But in the meantime I\\u2019d met several of the members of the &lt;a href=\\&quot;https://amplab.cs.berkeley.edu\\&quot; target=\\&quot;_blank\\&quot;&gt;AMPLab&lt;/a&gt; research team at Berkeley, and been impressed with their mature approach to building a platform and ecosystem. And I\\u2019d seen enough companies installing Spark on their dev clusters that it was clear this was a technology to watch. In a remarkably short time, it went from experimental to very real. And now prospects in the Alpine pipeline were asking me if it was on the roadmap. So yes, I told my candidate. \\u201cYou\\u2019ll be working on Spark from day one.\\u201d\\n\\nLast week, Alpine announced at &lt;a href=\\&quot;http://www.cmswire.com/cms/big-data/alpine-turns-big-data-to-a-priceless-asset-fast-gigaomlive-024541.php\\&quot; target=\\&quot;_blank\\&quot;&gt;GigaOM&lt;/a&gt; that it\\u2019s one of the first analytics companies to leverage Spark for building predictive models. We demonstrated the Alpine engine running on &lt;a href=\\&quot;http://www.gopivotal.com/big-data/analytics-workbench\\&quot; target=\\&quot;_blank\\&quot;&gt;Pivotal\\u2019s Analytics Workbench&lt;/a&gt;, where it ran an iterative classification algorithm (logistic regression) on 50 million rows in less than 50 seconds.\\n\\nFurthermore, we were officially certified on Spark by the team at Databricks. It\\u2019s been an honor to work with them and the research team at Berkeley. We think their technology will be a serious contender for the leading platform for data science.\\n\\nSpark is more to us than just speed. It\\u2019s really the entire ecosystem that represents such an exciting paradigm for working with data.\\n\\nStill, the core capability of caching data in memory was our primary consideration, and our iterative algorithms have been shown to speed up by one or even two orders of magnitude (thanks again to that Pivotal cluster).\\n\\nWe\\u2019ve always had this mantra at Alpine: \\u201cAvoid multiple passes through the data!\\u201d And we\\u2019ve designed many of our machine learning algorithms to avoid scanning the data too many times, packing on calculations into each MapReduce job like a waiter piling up plates to try and clear a table in one go. But it\\u2019s rare that we can avoid it entirely. With Spark, it\\u2019s incredibly satisfying to watch the progress bar zip along as the system re-uses data it\\u2019s already seen before.\\n\\nAnother thing that\\u2019s getting our engineers excited is Spark\\u2019s &lt;a href=\\&quot;http://spark.apache.org/mllib/\\&quot; target=\\&quot;_blank\\&quot;&gt;MLLib&lt;/a&gt;, the machine-learning library written on top of the Spark runtime. Alpine has long thought that machine learning algorithms should be open source. (I helped to kick off the &lt;a href=\\&quot;http://madlib.net/\\&quot; target=\\&quot;_blank\\&quot;&gt;MADlib&lt;/a&gt; library of analytics functions for databases, and Alpine now uses it extensively.) So we\\u2019re now beginning to contribute some of our code back into MLLib. And, moreover, we think MLLib and MLI have the potential to be a more general repository for open-source machine learning.\\n\\nSo I\\u2019ll congratulate the Alpine team for helping to bring the power of Spark to our users, and I\\u2019ll also congratulate the Spark team and Databricks for making it possible!&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;michael&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html&quot;, &quot;authors&quot;: [&quot;Michael Armbrust&quot;, &quot;Reynold Xin&quot;], &quot;id&quot;: 42, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-03-27&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-03-27&quot;}, &quot;title&quot;: &quot;Spark SQL: Manipulating Structured Data Using Apache Spark&quot;, &quot;slug&quot;: &quot;spark-sql-manipulating-structured-data-using-spark-2&quot;, &quot;content&quot;: &quot;Building a unified platform for big data analytics has long been the vision of Apache Spark, allowing a single program to perform ETL, MapReduce, and complex analytics. An important aspect of unification that our users have consistently requested is the ability to more easily import data stored in external sources, such as Apache Hive. Today, we are excited to announce &lt;a href=\\&quot;https://spark.apache.org/docs/latest/sql-programming-guide.html\\&quot;&gt;Spark SQL&lt;/a&gt;, a new component recently merged into the Spark repository.\\n\\nSpark SQL brings native support for SQL to Spark and streamlines the process of querying data stored both in RDDs (Spark\\u2019s distributed datasets) and in external sources. Spark SQL conveniently blurs the lines between RDDs and relational tables. Unifying these powerful abstractions makes it easy for developers to intermix SQL commands querying external data with complex analytics, all within in a single application. Concretely, Spark SQL will allow developers to:\\n&lt;ul&gt;\\n \\t&lt;li&gt;Import relational data from Parquet files and Hive tables&lt;/li&gt;\\n \\t&lt;li&gt;Run SQL queries over imported data and existing RDDs&lt;/li&gt;\\n \\t&lt;li&gt;Easily write RDDs out to Hive tables or Parquet files&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;h2 id=\\&quot;spark-sql-in-action\\&quot;&gt;Spark SQL In Action&lt;/h2&gt;\\nNow, let\\u2019s take a closer look at how Spark SQL gives developers the power to integrate SQL commands into applications that also take advantage of MLlib, Spark\\u2019s machine learning library. Consider an application that needs to predict which users are likely candidates for a service, based on their profile. Often, such an analysis requires joining data from multiple sources. For the purposes of illustration, imagine an application with two tables:\\n&lt;ul&gt;\\n \\t&lt;li&gt;Users(userId INT, name String, email STRING,\\nage INT, latitude: DOUBLE, longitude: DOUBLE,\\nsubscribed: BOOLEAN)&lt;/li&gt;\\n \\t&lt;li&gt;Events(userId INT, action INT)&lt;/li&gt;\\n&lt;/ul&gt;\\nGiven the data stored in in these tables, one might want to build a model that will predict which users are good targets for a new campaign, based on users that are similar.\\n\\n[scala]\\n// Data can easily be extracted from existing sources,\\n// such as Apache Hive.\\nval trainingDataTable = sql(&amp;quot;&amp;quot;&amp;quot;\\n  SELECT e.action\\n         u.age,\\n         u.latitude,\\n         u.logitude\\n  FROM Users u\\n  JOIN Events e\\n  ON u.userId = e.userId&amp;quot;&amp;quot;&amp;quot;)\\n\\n// Since `sql` returns an RDD, the results of the above\\n// query can be easily used in MLlib\\nval trainingData = trainingDataTable.map { row =&amp;gt;\\n  val features = Array[Double](row(1), row(2), row(3))\\n  LabeledPoint(row(0), features)\\n}\\n\\nval model =\\n  new LogisticRegressionWithSGD().run(trainingData)\\n[/scala]\\n\\nNow that we have used SQL to join existing data and train a model, we can use this model to predict which users are likely targets.\\n\\n[scala]\\nval allCandidates = sql(&amp;quot;&amp;quot;&amp;quot;\\n  SELECT userId,\\n         age,\\n         latitude,\\n         logitude\\n  FROM Users\\n  WHERE subscribed = FALSE&amp;quot;&amp;quot;&amp;quot;)\\n\\n// Results of ML algorithms can be used as tables\\n// in subsequent SQL statements.\\ncase class Score(userId: Int, score: Double)\\nval scores = allCandidates.map { row =&amp;gt;\\n  val features = Array[Double](row(1), row(2), row(3))\\n  Score(row(0), model.predict(features))\\n}\\nscores.registerAsTable(&amp;quot;Scores&amp;quot;)\\n\\nval topCandidates = sql(&amp;quot;&amp;quot;&amp;quot;\\n  SELECT u.name, u.email\\n  FROM Scores s\\n    JOIN Users u ON s.userId = u.userId\\n  ORDER BY score DESC\\n  LIMIT 100&amp;quot;&amp;quot;&amp;quot;)\\n\\n// Send emails to top candidates to promote the service.\\n[/scala]\\n\\nIn this example, Spark SQL made it easy to extract and join the various datasets preparing them for the machine learning algorithm. Since the results of Spark SQL are also stored in RDDs, interfacing with other Spark libraries is trivial. Furthermore, Spark SQL allows developers to close the loop, by making it easy to manipulate and join the output of these algorithms, producing the desired final result.\\n\\nTo summarize, the unified Spark platform gives developers the power to choose the right tool for the right job, without having to juggle multiple systems. If you would like to see more concrete examples of using Spark SQL please check out the &lt;a href=\\&quot;http://spark.apache.org/docs/1.0.0/sql-programming-guide.html\\&quot;&gt;programming guide&lt;/a&gt;.\\n&lt;h2 id=\\&quot;optimizing-with-catalyst\\&quot;&gt;Optimizing with Catalyst&lt;/h2&gt;\\nIn addition to providing new ways to interact with data, Spark SQL also brings a powerful new optimization framework called Catalyst. Using Catalyst, Spark can automatically transform SQL queries so that they execute more efficiently. The Catalyst framework allows the developers behind Spark SQL to rapidly add new optimizations, enabling us to build a faster system more quickly. In one recent example, we found an inefficiency in Hive group-bys that took an experienced developer an entire weekend and over 250 lines of code to fix; we were then able to make the same fix in Catalyst in only a few lines of code.\\n&lt;h2 id=\\&quot;future-of-shark\\&quot;&gt;Future of Shark&lt;/h2&gt;\\nThe natural question that arises is about the future of Shark. Shark was among the first systems that delivered up to 100X speedup over Hive. It builds on the Apache Hive codebase and achieves performance improvements by swapping out the physical execution engine part of Hive. While this approach enables Shark users to speed up their Hive queries without modification to their existing warehouses, Shark inherits the large, complicated code base from Hive that makes it hard to optimize and maintain. As Spark SQL matures, Shark will transition to using Spark SQL for query optimization and physical execution so that users can benefit from the ongoing optimization efforts within Spark SQL.\\n\\nIn short, we will continue to invest in Shark and make it an excellent drop-in replacement for Apache Hive. It will take advantage of the new Spark SQL component, and will provide features that complement it, such as Hive compatibility and the standalone SharkServer, which allows external tools to connect queries through JDBC/ODBC.\\n&lt;h2 id=\\&quot;whats-next\\&quot;&gt;What\\u2019s next&lt;/h2&gt;\\nSpark SQL will be included in Spark 1.0 as an alpha component. However, this is only the beginning of better support for relational data in Spark, and this post only scratches the surface of Catalyst. Look for future blog posts on the following topics:\\n&lt;ul&gt;\\n \\t&lt;li&gt;Generating custom bytecode to speed up expression evaluation&lt;/li&gt;\\n \\t&lt;li&gt;Reading and writing data using other formats and systems, include Avro and HBase&lt;/li&gt;\\n \\t&lt;li&gt;API support for using Spark SQL in Python and Java&lt;/li&gt;\\n&lt;/ul&gt;&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;patrick&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/02/03/release-0_9_0.html&quot;, &quot;authors&quot;: [&quot;Patrick Wendell&quot;], &quot;id&quot;: 58, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-02-04&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-02-04&quot;}, &quot;title&quot;: &quot;Apache Spark 0.9.0 Released&quot;, &quot;slug&quot;: &quot;release-0_9_0&quot;, &quot;content&quot;: &quot;Our goal with Apache Spark is very simple: provide the best platform for computation on big data. We do this through both a powerful core engine and rich libraries for useful analytics tasks. Today, we are excited to announce the release of Apache Spark 0.9.0. This major release extends Spark\\u2019s libraries and further improves its performance and usability. Apache Spark 0.9.0 is the largest release to date, with work from 83 contributors, who submitted over 300 patches.\\n\\nApache Spark 0.9 features significant extensions to the set of standard analytical libraries packaged with Spark. The release introduces GraphX, a library for graph computation that comes with implementations of several standard algorithms, such as PageRank. Spark\\u2019s machine learning library (MLlib) has been extended to support Python, using the NumPy numerical library. A Naive Bayes Classifier has also been added to MLlib. Finally, Spark Streaming, which supports near-real-time continuous computation, has added a simplified high-availability mode and several significant optimizations.\\n\\nIn addition to higher-level libraries, Spark 0.9 features improvements to the core computation engine. Spark now now automatically spills reduce output to disk, increasing the stability of workloads with very large aggregations. Support for Spark in YARN mode has been hardened and improved. The standalone mode has added automatic supervision of applications and better support for sharing clusters amongst several users. Finally, we\\u2019ve focused on stabilizing API\\u2019s ahead of Apache Spark\\u2019s 1.0 release to make things easy for developers writing Spark applications. This includes upgrading to Scala 2.10, allowing applications written in Scala to use newer libraries.\\n\\nApache Spark 0.9.0 can be &lt;a href=\\&quot;http://spark.incubator.apache.org/downloads.html\\&quot;&gt;downloaded directly&lt;/a&gt; from the Apache Spark website. It will also be available to CDH users via a Cloudera parcel, which can automatically install Spark on existing CDH clusters. For a more detailed explanation of the features in this release, head on over to the &lt;a href=\\&quot;http://spark.incubator.apache.org/releases/spark-release-0-9-0.html\\&quot;&gt;official release notes&lt;/a&gt;. Enjoy the newest release of Spark!&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;ali&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/01/01/simr.html&quot;, &quot;authors&quot;: [&quot;Ali Ghodsi&quot;, &quot;Ahir Reddy&quot;], &quot;id&quot;: 65, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Ecosystem&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-01-02&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-01-02&quot;}, &quot;title&quot;: &quot;Apache Spark In MapReduce (SIMR)&quot;, &quot;slug&quot;: &quot;simr&quot;, &quot;content&quot;: &quot;Apache Hadoop integration has always been a key goal of Apache Spark and &lt;a href=\\&quot;http://hortonworks.com/wp-content/uploads/2013/06/YARN.png\\&quot;&gt;YARN&lt;/a&gt; users have long been able to run &lt;a href=\\&quot;http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\\&quot;&gt;Spark on YARN&lt;/a&gt;. However, up to now, it has been relatively hard to run Spark on Hadoop MapReduce v1 clusters, i.e. clusters that do not have YARN installed. Typically, users would have to get permission to install Spark/Scala on some subset of the machines, a process that could be time consuming. Enter &lt;a href=\\&quot;http://databricks.github.io/simr/\\&quot;&gt;SIMR (Spark In MapReduce)&lt;/a&gt;, which has been released in conjunction with &lt;a href=\\&quot;https://databricks.com/blog/2013/12/19/release-0_8_1.html\\&quot;&gt;Apache Spark 0.8.1&lt;/a&gt;.\\n\\nSIMR allows anyone with access to a Hadoop MapReduce v1 cluster to run Spark out of the box. A user can run Spark directly on top of Hadoop MapReduce v1 without any administrative rights, and without having Spark or Scala installed on any of the nodes. The only requirements are HDFS access and MapReduce v1. SIMR is open sourced under the &lt;a href=\\&quot;http://apache.org/licenses/LICENSE-2.0.html\\&quot;&gt;Apache license&lt;/a&gt; and was jointly developed by Databricks and UC Berkeley &lt;a href=\\&quot;http://amplab.cs.berkeley.edu\\&quot;&gt;AMPLab&lt;/a&gt;.\\n\\nThe basic idea is that a user can download the package of SIMR (&lt;a href=\\&quot;http://databricks.github.io/simr/#download\\&quot;&gt;3 files&lt;/a&gt;) that matches their Hadoop cluster and immediately start using Spark. SIMR includes the interactive Spark shell, and allows users to use the shell backed by the computational power of the cluster. It is a simple as &lt;code&gt;./simr --shell&lt;/code&gt;:\\n\\n&amp;nbsp;\\n\\n&lt;img src=\\&quot;https://databricks.com/wp-content/uploads/2014/01/simrshell.png\\&quot; alt=\\&quot;simrshell\\&quot; width=\\&quot;90%\\&quot; /&gt;\\n\\nRunning a Spark program simply requires bundling it along with its dependencies into a jar and launching the job through SIMR. SIMR uses the following command line syntax for running jobs:\\n\\n&lt;pre&gt;./simr jar_file main_class parameters&lt;/pre&gt;\\n\\nSIMR simply launches a MapReduce job with the desired number of map slots, and ensures that Spark/Scala and your job gets shipped to all those nodes. It then designates one of the mappers as the master and runs the Spark driver inside it. On the rest of the mappers it launches Spark executors that will execute tasks on behalf of the driver. Voila, your Spark job is running inside MapReduce on the cluster.\\n\\nSIMR lets users interact with the driver program. For example, users can type into the Spark shell and see its output interactively. The way this works is that SIMR runs a relay server on the master mapper and a relay client on the machine that launched SIMR. Any input from the client and output from the driver are relayed back and forth between the client and the master mapper.\\n\\nTo make all this work, SIMR makes extensive use of HDFS. The master mapper is elected using leader election by writing to HDFS and picking the mapper that firsts gets to write to HDFS. Similarly, the executors launched inside the rest of the mappers find out the driver\\u2019s URL by reading it from a specific file from HDFS. Thus, SIMR uses MapReduce and HDFS in place of a cluster manager.\\n\\n&lt;img src=\\&quot;https://databricks.com/wp-content/uploads/2014/01/simr-arch.png\\&quot; alt=\\&quot;simr-arch\\&quot; width=\\&quot;90%\\&quot; /&gt;\\n\\nSIMR is not intended to be used in production mode, but rather to enable users to explore and use Spark before running it on a proper resource manager, such as YARN, Mesos, or Standalone Mode. MapReduce 2 (YARN) users can of course use the existing &lt;a href=\\&quot;http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\\&quot;&gt;Spark on YARN&lt;/a&gt; solution, or explore other &lt;a href=\\&quot;http://spark.incubator.apache.org/docs/latest/index.html#launching-on-a-cluster\\&quot;&gt;Spark deployment options&lt;/a&gt;.\\n\\nWe hope SIMR will enable users to try out Spark without any heavy operational burden. Towards this goal, we have pre-built several SIMR binaries for different versions of Hadoop. Please go ahead and try it and let us know if you have any feedback.\\n\\nSIMR resources:\\n&lt;ul&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://databricks.github.io/simr\\&quot;&gt;Homepage&lt;/a&gt;&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://databricks.github.io/simr#download\\&quot;&gt;Download&lt;/a&gt;&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;https://github.com/databricks/simr\\&quot;&gt;Source code&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;&quot;}\n\n*** WARNING: skipped 18619 bytes of output ***\n\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;roy&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/03/18/spark-certification.html&quot;, &quot;authors&quot;: [&quot;Databricks Press Office&quot;], &quot;id&quot;: 2411, &quot;categories&quot;: [&quot;Announcements&quot;, &quot;Company Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-03-19&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-03-19&quot;}, &quot;title&quot;: &quot;Databricks announces \\&quot;Certified on Apache Spark\\&quot; Program&quot;, &quot;slug&quot;: &quot;spark-certification&quot;, &quot;content&quot;: &quot;&lt;strong&gt;BERKELEY, Calif. \\u2013 March 18, 2014 \\u2013&lt;/strong&gt; Databricks, the company founded by the creators of Apache Spark that is revolutionizing what enterprises can do with Big Data, today announced the Databricks &lt;a href=\\&quot;/certification/\\&quot;&gt;\\u201cCertified on Spark\\u201d Program&lt;/a&gt; for applications built on top of the Apache Spark platform. This program ensures that certified applications will work with a multitude of commercially supported Spark distributions.\\n\\n\\u201cPioneering application developers that are leveraging the power of Spark have had to choose between two sub-optimal choices: they either have to package Spark platform support with their application or attempt to maintain integration/certification individually with a rapidly increasing set of commercially supported Spark distributions,\\u201d said Ion Stoica, Databricks CEO. \\u201cThe Databricks \\u2018Certified on Spark\\u2019 program enables developers to certify solely against the 100% open-source Apache Spark distribution, and ensures interoperability with Apache Spark-compatible distributions. Databricks will handle the task of certifying the compatibility of each commercial Spark distribution with the Apache version and will soon announce the initial set of distributions that meet this criteria.\\u201d\\n\\nThe pioneering partners of the Databricks certification program include Adatao, Alpine Data Labs, and Tresata - advanced analytics application vendors that have been at the forefront in leveraging the power of Spark to deliver faster and deeper insights for their enterprise customers.\\n\\n\\u201cCertified on Spark\\u201d also provides multiple benefits for enterprise users including:\\n&lt;ul&gt;\\n \\t&lt;li&gt;Decoupling Spark distribution (and commercial support) from application licensing&lt;/li&gt;\\n \\t&lt;li&gt;Full transparency into which applications are truly designed to work with and leverage the power of Spark&lt;/li&gt;\\n \\t&lt;li&gt;A rapidly increasing set of certified applications to incentivize distribution vendors to maintain compatibility with Apache Spark and avoid forking and fragmentation, ultimately resulting in greater compatibility and shared innovation for users&lt;/li&gt;\\n&lt;/ul&gt;\\n\\u201cAt Databricks we are fully committed to keeping Spark 100% open source and to bringing it to the widest possible set of users,\\u201d said Matei Zaharia, Databricks CTO and the creator of Spark. \\u201cThe \\u2018Certified on Spark\\u2019 program is key to maintaining a healthy and unified Apache Spark platform, and it is an expression of our community focused efforts, such as organizing meetups and the upcoming Spark Summit, and driving the future of open-source development of Spark.\\u201d\\n\\nApplication developers that are interested in applying for the \\u201cCertified on Spark\\u201d program should visit &lt;a href=\\&quot;http://www.databricks.com\\&quot;&gt;www.databricks.com&lt;/a&gt; and select \\u201cApply for Certification\\u201d. Enterprise users can also visit the Databricks site regularly to see the latest set of certified application vendors and read \\u201capplication spotlight\\u201d blog articles that deep-dive into specific examples of Spark-powered applications.&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;ion&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/03/02/spark-apache-top-level-project.html&quot;, &quot;authors&quot;: [&quot;Ion Stoica&quot;], &quot;id&quot;: 2412, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-03-03&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-03-03&quot;}, &quot;title&quot;: &quot;Apache Spark Now a Top-level Apache Project&quot;, &quot;slug&quot;: &quot;spark-apache-top-level-project&quot;, &quot;content&quot;: &quot;&lt;div class=\\&quot;blogContent\\&quot;&gt;\\n\\nWe are delighted with the recent &lt;a href=\\&quot;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50\\&quot;&gt;announcement&lt;/a&gt; of the Apache Software Foundation that &lt;a href=\\&quot;http://spark.apache.org\\&quot;&gt;Apache Spark&lt;/a&gt; has become a top-level Apache project. This is a recognition of the fantastic work done by the Spark open source community, which now counts over 140 developers from 30+ companies.\\n\\nIn short time, Spark has become an increasingly popular solution for numerous big data applications, including machine learning, interactive queries, and stream processing. Spark now is an integral part of the Hadoop ecosystem, with many organizations employing Spark to perform sophisticated processing on their Hadoop data.\\n\\nAt Databricks we are looking forward to continuing our work with the open source community to accelerate the development and adoption of Apache Spark. Currently employing the lead developers and creators of many of the components of the Spark ecosystem, including Matei Zaharia, the creator of Spark, and Patrick Wendell, the release manager of Spark, we are committed to the success of Apache Spark.\\n\\n&lt;/div&gt;&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;rxin&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/02/12/big-data-benchmark.html&quot;, &quot;authors&quot;: [&quot;Ahir Reddy&quot;, &quot;Reynold Xin&quot;], &quot;id&quot;: 2413, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-02-13&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-02-13&quot;}, &quot;title&quot;: &quot;AMPLab updates the Big Data Benchmark&quot;, &quot;slug&quot;: &quot;big-data-benchmark&quot;, &quot;content&quot;: &quot;The AMPLab at UC Berkeley, with help from Databricks, recently released an update to the &lt;a href=\\&quot;https://amplab.cs.berkeley.edu/benchmark/\\&quot;&gt;Big Data Benchmark&lt;/a&gt;. This benchmark uses Amazon EC2 to compare performance of five popular SQL query engines in the Big Data ecosystem on common types of queries, which can be reproduced through publicly available scripts and datasets.\\n\\nIn the past year, the community has invested heavily in performance optimizations of query engines. We are glad to see that all projects have evolved in this area. Although the queries used in the benchmark are simple, we are proud that Shark remains one of the fastest engines for these workloads, and has improved significantly since the last run.\\n\\nWhile this benchmark reaffirms Shark as a highly performant SQL query engine, we are working hard at Databricks to push the boundaries further. Stay tuned for some exciting news we will share soon with the community.\\n&lt;ul&gt;\\n\\t&lt;li&gt;&lt;a href=\\&quot;https://amplab.cs.berkeley.edu/benchmark/\\&quot;&gt;Big Data Benchmark&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&amp;nbsp;&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;pat.mcdonough&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/02/10/strata-santa-clara-2014.html&quot;, &quot;authors&quot;: [&quot;Pat McDonough&quot;], &quot;id&quot;: 2414, &quot;categories&quot;: [&quot;Company Blog&quot;, &quot;Events&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-02-11&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-02-11&quot;}, &quot;title&quot;: &quot;Databricks at the O'Reilly Strata Conference 2014&quot;, &quot;slug&quot;: &quot;strata-santa-clara-2014&quot;, &quot;content&quot;: &quot;The Databricks team is excited to take part in a number of activities throughout the 2014 O\\u2019Reilly Strata Conference in Santa Clara. From hands-on training, to office hours, to several talks (including a keynote), there are plenty of chances for attendees to learn how Apache Spark is bringing ease of use and outstanding performance to your big data. The schedule for the Databricks team includes:\\n&lt;ul&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://ampcamp.berkeley.edu/4/\\&quot;&gt;AMPCamp4&lt;/a&gt;, Hosted at Strata&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://strataconf.com/strata2014/public/content/office-hours\\&quot;&gt;Office Hours&lt;/a&gt; on Wednesday at 5:45pm&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://strataconf.com/strata2014/public/schedule/detail/33057\\&quot;&gt;How Companies are Using Spark, and Where the Edge in Big Data Will Be&lt;/a&gt;, a keynote talk presented by Matei Zaharia on Thursday at 9:15am&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://strataconf.com/strata2014/public/schedule/detail/32375\\&quot;&gt;Querying Petabytes of Data in Seconds with BlinkDB&lt;/a&gt;, co-presented by Reynold Xin on Thursday at 1:30pm&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;h2 id=\\&quot;about-ampcamp-4\\&quot;&gt;About AMPCamp 4&lt;/h2&gt;\\nWe\\u2019ll be kicking off the conference on Tuesday by helping to host the fourth iteration of AMPCamp, where some of the newer components of the Berkeley Data Analytics stack will get some time in the spotlight. There are talks on BlinkDB, MLbase, GraphX, and Tachyon in the morning, and hands-on training for BlinkDB, MLbase, Spark, Spark Streaming, GraphX, and Shark in the afternoon.\\n&lt;h2 id=\\&quot;conference-talks--office-hours\\&quot;&gt;Conference Talks &amp;amp; Office Hours&lt;/h2&gt;\\nOn Wednesday afternoon, attendees can come speak directly with a few of the members from our team in Strata\\u2019s Office Hours. Office hours provide a chance to meet face-to-face with the Databricks team and to ask any questions about the Spark project and ecosystem.\\n\\nTo learn the latest about Apache Spark, attendees should listen to talks featuring the Databricks team, including a keynote by our CTO Matei Zaharia. On Thursday morning, Matei will describe how organizations must now compete on the speed and sophistication with which they can draw value from data, and how Apache Spark makes a new class of applications possible.\\n&lt;h2 id=\\&quot;well-see-you-there\\&quot;&gt;We\\u2019ll See You There!&lt;/h2&gt;\\nWe\\u2019re looking forward to seeing you at the conference!&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;ion&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/01/21/spark-and-hadoop.html&quot;, &quot;authors&quot;: [&quot;Ion Stoica&quot;], &quot;id&quot;: 2415, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Ecosystem&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-01-22&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-01-22&quot;}, &quot;title&quot;: &quot;Apache Spark and Hadoop: Working Together&quot;, &quot;slug&quot;: &quot;spark-and-hadoop&quot;, &quot;content&quot;: &quot;We are often asked how does &lt;a href=\\&quot;http://spark.incubator.apache.org\\&quot;&gt;Apache Spark&lt;/a&gt; fits in the Hadoop ecosystem, and how one can run Spark in a existing Hadoop cluster. This blog aims to answer these questions.\\n\\nFirst, Spark is intended to &lt;em&gt;enhance&lt;/em&gt;, not replace, the Hadoop stack. From day one, Spark was designed to read and write data from and to HDFS, as well as other storage systems, such as HBase and Amazon\\u2019s S3. As such, Hadoop users can enrich their processing capabilities by combining Spark with Hadoop MapReduce, HBase, and other big data frameworks.\\n\\nSecond, we have constantly focused on making it as easy as possible for &lt;em&gt;every Hadoop user&lt;/em&gt; to take advantage of Spark\\u2019s capabilities. No matter whether you run Hadoop 1.x or Hadoop 2.0 (YARN), and no matter whether you have administrative privileges to configure the Hadoop cluster or not, there is a way for you to run Spark! In particular, there are three ways to deploy Spark in a Hadoop cluster: standalone, YARN, and SIMR.\\n\\n&lt;img class=\\&quot;aligncenter\\&quot; src=\\&quot;https://databricks.com/wp-content/uploads/2014/01/SparkHadoop.png\\&quot; alt=\\&quot;SparkHadoop1.png\\&quot; /&gt;\\n\\n&lt;strong&gt;Standalone deployment&lt;/strong&gt;: With the standalone deployment one can statically allocate resources on all or a subset of machines in a Hadoop cluster and run Spark side by side with Hadoop MR. The user can then run arbitrary Spark jobs on her HDFS data. Its simplicity makes this the deployment of choice for many Hadoop 1.x users.\\n\\n&lt;strong&gt;&lt;a href=\\&quot;https://hadoop.apache.org/docs/current2/hadoop-yarn/hadoop-yarn-site/YARN.html\\&quot;&gt;Hadoop Yarn&lt;/a&gt; deployment&lt;/strong&gt;: Hadoop users who have already deployed or are planning to deploy &lt;a href=\\&quot;https://hadoop.apache.org/docs/current2/hadoop-yarn/hadoop-yarn-site/YARN.html\\&quot;&gt;Hadoop Yarn&lt;/a&gt; can simply run Spark on YARN without any pre-installation or administrative access required. This allows users to easily integrate Spark in their Hadoop stack and take advantage of the full power of Spark, as well as of other components running on top of Spark.\\n\\n&lt;strong&gt;Spark In MapReduce (&lt;a href=\\&quot;https://databricks.com/blog/2014/01/01/simr.html\\&quot;&gt;SIMR&lt;/a&gt;)&lt;/strong&gt;: For the Hadoop users that are not running YARN yet, another option, in addition to the standalone deployment, is to use SIMR to launch Spark jobs inside MapReduce. With SIMR, users can start experimenting with Spark and use its shell within a couple of minutes after downloading it! This tremendously lowers the barrier of deployment, and lets virtually everyone play with Spark.\\n&lt;h2 id=\\&quot;interoperability-with-other-systems\\&quot;&gt;Interoperability with other Systems&lt;/h2&gt;\\nSpark interoperates not only with Hadoop, but with other popular big data technologies as well.\\n&lt;ul&gt;\\n \\t&lt;li&gt;&lt;strong&gt;&lt;a href=\\&quot;http://hive.apache.org/\\&quot;&gt;Apache Hive&lt;/a&gt;&lt;/strong&gt;: Through &lt;a href=\\&quot;https://github.com/amplab/shark/wiki\\&quot;&gt;Shark&lt;/a&gt;, Spark enables Apache Hive users to run their unmodified queries much faster. Hive is a popular data warehouse solution running on top of Hadoop, while Shark is a system that allows the Hive framework to run on top of Spark instead of Hadoop. As a result, Shark can accelerate Hive queries by as much as 100x when the input data fits into memory, and up 10x when the input data is stored on disk.&lt;/li&gt;\\n \\t&lt;li&gt;&lt;strong&gt;&lt;a href=\\&quot;http://aws.amazon.com/\\&quot;&gt;AWS EC2&lt;/a&gt;&lt;/strong&gt;: Users can easily run Spark (and Shark) on top of Amazon\\u2019s EC2 either using the &lt;a href=\\&quot;http://spark.incubator.apache.org/docs/latest/ec2-scripts.html\\&quot;&gt;scripts&lt;/a&gt; that come with Spark, or the hosted &lt;a href=\\&quot;http://aws.amazon.com/articles/4926593393724923\\&quot;&gt;versions of Spark and Shark&lt;/a&gt; on Amazon\\u2019s Elastic MapReduce.&lt;/li&gt;\\n \\t&lt;li&gt;&lt;strong&gt;&lt;a href=\\&quot;http://mesos.apache.org/\\&quot;&gt;Apache Mesos&lt;/a&gt;&lt;/strong&gt;: Spark runs on top of Mesos, a cluster manager system which provides efficient resource isolation across distributed applications, including &lt;a href=\\&quot;http://www.mcs.anl.gov/research/projects/mpi/\\&quot;&gt;MPI&lt;/a&gt; and Hadoop. Mesos enables &lt;em&gt;fine grained&lt;/em&gt; sharing which allows a Spark job to dynamically take advantage of the idle resources in the cluster during its execution. This leads to considerable performance improvements, especially for long running Spark jobs.&lt;/li&gt;\\n&lt;/ul&gt;&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;patrick&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2013/12/19/release-0_8_1.html&quot;, &quot;authors&quot;: [&quot;Patrick Wendell&quot;], &quot;id&quot;: 2416, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2013-12-20&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2013-12-20&quot;}, &quot;title&quot;: &quot;Apache Spark 0.8.1 Released&quot;, &quot;slug&quot;: &quot;release-0_8_1&quot;, &quot;content&quot;: &quot;We are happy to announce the release of Apache Spark 0.8.1. In addition to performance and stability improvements, this release adds three new features. First, Spark now supports for the newest versions of YARN (2.2+). Second, the standalone cluster manager supports a high-availability mode in which it can tolerate master failures. Third, shuffles have been optimized to create fewer files, improving shuffle performance drastically in some settings.\\n\\nIn conjunction with the Apache Spark 0.8.1 release we are separately releasing &lt;a href=\\&quot;https://databricks.com/blog/2014/01/01/simr.html\\&quot;&gt;Spark In MapReduce (SIMR)&lt;/a&gt;, which enables seamlessly running Spark on Hadoop MapReduce v1 clusters without requiring the installation of Scala or Spark.\\n\\nWhile Apache Spark 0.8.1 is a minor release, it includes these larger features for the benefit of Scala 2.9 users. The next major release of Apache Spark, 0.9.0, will be based on Scala 2.10.\\n\\nThis release was a community effort, featuring contributions from more than 40 developers. For much more information about Apache Spark 0.8.1, head over to the &lt;a href=\\&quot;http://spark.incubator.apache.org/releases/spark-release-0-8-1.html\\&quot;&gt;release notes&lt;/a&gt; or &lt;a href=\\&quot;http://spark.incubator.apache.org/downloads.html\\&quot;&gt;download Apache Spark 0.8.1&lt;/a&gt; and try it out for yourself.&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;andy&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2013/12/18/spark-summit-2013-follow-up.html&quot;, &quot;authors&quot;: [&quot;Andy Konwinski&quot;], &quot;id&quot;: 2417, &quot;categories&quot;: [&quot;Company Blog&quot;, &quot;Customers&quot;, &quot;Events&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2013-12-19&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2013-12-19&quot;}, &quot;title&quot;: &quot;Highlights From Spark Summit 2013&quot;, &quot;slug&quot;: &quot;spark-summit-2013-follow-up&quot;, &quot;content&quot;: &quot;Earlier this month we held the &lt;a href=\\&quot;http://spark-summit.org/2013\\&quot;&gt;first Spark Summit&lt;/a&gt;, a conference to bring the Apache Spark community together. We are excited to share some statistics and highlights from the event.\\n&lt;ul&gt;\\n \\t&lt;li&gt;450 participants from over 180 companies attended&lt;/li&gt;\\n \\t&lt;li&gt;Participants came from 13 countries&lt;/li&gt;\\n \\t&lt;li&gt;Spark training was sold out at 200 participants from 80 companies&lt;/li&gt;\\n \\t&lt;li&gt;20 organizations sponsored the event, including all major Hadoop platform vendors&lt;/li&gt;\\n \\t&lt;li&gt;20 different organizations gave talks&lt;/li&gt;\\n&lt;/ul&gt;\\nVideos and slides for all talks are now available on the &lt;a href=\\&quot;http://spark-summit.org/2013\\&quot;&gt;Summit 2013 page&lt;/a&gt;.\\n\\nThe Summit included Keynotes from Databricks, the UC Berkeley AMPLab, and Yahoo, as well as presentations from 18 other companies including Amazon, Red Hat, and Adobe. Talk topics covered a wide range including specialized applications such as mapping and manipulating the brain, product launches, and research projects about future directions for the project. We are very excited to see Spark and related projects come such a long way from research prototypes originally developed in AMPLab at Berkeley to being used in production by startups and large companies alike.\\n&lt;h2 id=\\&quot;the-state-of-spark-and-where-were-going-next\\&quot;&gt;The State of Spark, and Where We\\u2019re Going Next&lt;/h2&gt;\\n&lt;iframe style=\\&quot;float: left; padding: 10px;\\&quot; src=\\&quot;//www.youtube.com/embed/nU6vO2EJAb4\\&quot; width=\\&quot;300\\&quot; height=\\&quot;173\\&quot; frameborder=\\&quot;0\\&quot; allowfullscreen=\\&quot;allowfullscreen\\&quot;&gt;&lt;/iframe&gt;\\n\\nIn the first keynote of the day, Matei Zaharia, who started the Spark project and is now CTO at Databricks, gave a summary of recent growth in the projec&lt;span style=\\&quot;letter-spacing: 3px;\\&quot;&gt;t&lt;/span&gt;&lt;a style=\\&quot;vertical-align: super; font-size: 60%;\\&quot; href=\\&quot;#footnote-1\\&quot;&gt;1&lt;/a&gt;, highlighting key contributions from across the community. In particular, Spark recently reached 100 contributors, making it the second-largest open source development community in the Big Data space after Hadoop, and it\\u2019s also overtaken Hadoop in the past 6 months. Matei also previewed what is coming up in the Spark development roadmap, including features under development for the upcoming 0.8.1 and 0.9 releases including high availability for the Spark Master in standalone mode, external hashing and sorting, support for Scala 2.10, and more. Finally, Matei discussed features of Spark that differentiate it from other projects, such as the focus on unification of diverse programming models.\\n&lt;h2 id=\\&quot;spark-and-hadoop\\&quot;&gt;Spark and Hadoop&lt;/h2&gt;\\n&lt;iframe style=\\&quot;float: right; padding-left: 10px;\\&quot; src=\\&quot;//www.youtube.com/embed/qs40jiN2iwM\\&quot; width=\\&quot;300\\&quot; height=\\&quot;173\\&quot; frameborder=\\&quot;0\\&quot; allowfullscreen=\\&quot;allowfullscreen\\&quot;&gt;&lt;/iframe&gt;\\n\\nIt was exciting to hear from Eric Baldeschwieler, whose background includes leading the Yahoo team that took Hadoop from being a prototype project to what it is today, as well as serving as both CEO and CTO of Hortonworks. In his keynote, Eric presented his view of how Spark is on track to become the \\u201clingua franca\\u201d for Big Data. He also talked about how Spark updates Hadoop with important features such as effective utilization of RAM, low latency queries, and streaming ingest. Finally, he discussed how the Spark and Hadoop projects relate to each other now and going forward.\\n&lt;h2 id=\\&quot;spark-training-day\\&quot;&gt;Spark Training Day&lt;/h2&gt;\\nOn the sold-out second day, 200 attendees heard 4 talks on using, deploying, and administering Spark, and also participated in hands-on training led by the creators of Spark. Amazon, a sponsor of the Summit, donated EC2 resources so participants could each have their own 6 node Spark cluster to practice using Spark, Spark Streaming and Shark on real Wikipedia and Twitter data. We have made the hands-on exercises &lt;a href=\\&quot;http://spark-summit.org/exercises\\&quot;&gt;available on the summit website&lt;/a&gt; for free. Using these, anybody can use their own Amazon AWS account to launch a 6 node Spark cluster and get experience analyzing real data.\\n\\n&lt;iframe style=\\&quot;float: left; padding-right: 10px;\\&quot; src=\\&quot;//www.youtube.com/embed/zGW8glN-Mo8\\&quot; width=\\&quot;300\\&quot; height=\\&quot;173\\&quot; frameborder=\\&quot;0\\&quot; allowfullscreen=\\&quot;allowfullscreen\\&quot;&gt;&lt;/iframe&gt;\\n\\nThe final talk of the training day was given by core Spark developer and Databricks cofounder Patrick Wendell. Patrick\\u2019s talk was about Administering Spark and was prepared in response to requests for more advanced technical material as part of the training. In it he dove into the core software components of the project, the different resource managers that Spark runs on, what type of hardware to use when running Spark, how to link against Spark when writing applications, monitoring Spark clusters running in production, and more.\\n&lt;h2 id=\\&quot;other-interesting-talks\\&quot;&gt;Other Interesting Talks&lt;/h2&gt;\\nIn addition to those above, there were many more great talks at the Summit, here are a few more that highligh the diversity of topics covered:\\n&lt;ul class=\\&quot;talk-list\\&quot;&gt;\\n \\t&lt;li&gt;&lt;span class=\\&quot;talk-title\\&quot;&gt;Mapping and manipulating the brain at scale&lt;/span&gt; (&lt;a href=\\&quot;http://spark-summit.org/talk/freeman-mapping-and-manipulating-the-brain-at-scale/\\&quot;&gt;abstract&lt;/a&gt;, &lt;a h\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["To expose the JSON file as a table, use the standard SQL create table using syntax introduced in the previous lesson:"],"metadata":{}},{"cell_type":"code","source":["%sql\nCREATE TABLE IF NOT EXISTS DatabricksBlog\n  USING json\n  OPTIONS (\n    path \"dbfs:/mnt/training/databricks-blog.json\",\n    inferSchema \"true\"\n  )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Take a look at the schema with the `DESCRIBE` function."],"metadata":{}},{"cell_type":"code","source":["%sql\nDESCRIBE DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>authors</td><td>array<string></td><td>null</td></tr><tr><td>categories</td><td>array<string></td><td>null</td></tr><tr><td>content</td><td>string</td><td>null</td></tr><tr><td>creator</td><td>string</td><td>null</td></tr><tr><td>dates</td><td>struct<createdOn:string,publishedOn:string,tz:string></td><td>null</td></tr><tr><td>description</td><td>string</td><td>null</td></tr><tr><td>id</td><td>bigint</td><td>null</td></tr><tr><td>link</td><td>string</td><td>null</td></tr><tr><td>slug</td><td>string</td><td>null</td></tr><tr><td>status</td><td>string</td><td>null</td></tr><tr><td>title</td><td>string</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Run a query to view the contents of the table.\n\nNotice:\n* The `authors` column is an array containing multiple author names.\n* The `categories` column is an array of multiple blog post category names.\n* The `dates` column contains nested fields `createdOn`, `publishedOn` and `tz`."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT authors, categories, dates, content \nFROM DatabricksBlog\nlimit 20"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>authors</th><th>categories</th><th>dates</th><th>content</th></tr></thead><tbody><tr><td>List(Tomer Shiran (VP of Product Management at MapR))</td><td>List(Company Blog, Partners)</td><td>List(2014-04-10, 2014-04-10, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at MapR, announcing our new partnership to provide enterprise support for Apache Spark as part of MapR's Distribution of Hadoop.</div>\n\n<hr />\n\nWith over 500 paying customers, my team and I have the opportunity to talk to many organizations that are leveraging Hadoop in production to extract value from big data. One of the most common topics raised by our customers in recent months is Apache Spark. Some customers just want to learn more about the advantages of this technology and the use cases that it addresses, while others are already running it in production with the MapR Distribution. These customers range from the world’s largest cable telcos and retailers to Silicon Valley startups such as Quantifind, which recently talked about its use of Spark on MapR in an <a href=\"http://www.datameer.com/ceoblog/big-data-brews-with-erich-nachbar/\" target=\"_blank\">interview</a> with Stefan Groschupf, CEO of Datameer.\n\nToday, I am happy to <a href=\"http://www.businesswire.com/news/home/20140410005101/en/MapR-Adds-Complete-Apache-Spark-Stack-Distribution#.U0a0G61dXKI\" target=\"_blank\">announce</a> and share with you the beginning of our journey with Databricks, and the addition of the complete Spark stack to the MapR Distribution for Apache Hadoop. We are now the only Hadoop distribution to support the complete Spark stack, including Spark, Spark Streaming (stream processing), Shark (Hive on Spark), MLLib (machine learning) and GraphX (graph processing). This is a testament to our commitment to open source and to providing our customers with maximum flexibility to pick and choose the right tool for the job.\n<h2 id=\"why-spark\">Why Spark?</h2>\nOne of the challenges organizations face when adopting Hadoop is a shortage of developers who have experience building Hadoop applications. Our professional services organization has helped dozens of companies with the development and deployment of Hadoop applications, and our training department has trained countless engineers. Organizations are hungry for solutions that make it easier to develop Hadoop applications while increasing developer productivity, and Spark fits this bill. Spark jobs can require as little as 1/5th of code. Spark provides a simple programming abstraction allowing developers to design applications as operations on data collections (known as RDDs, or Resilient Distributed Datasets). Developers can build these applications in multiple programming languages, including Java, Scala and Python, and the same code can be reused across batch, interactive and streaming applications.\n\nIn addition to making developers happier and more productive, Spark provides significant benefits with respect to end-to-end application performance. To this end, Spark provides a general-purpose execution framework with in-memory pipelining. For many applications, this results in a 5-100x performance improvement, because some or all steps can execute in memory without unnecessarily writing to and reading from disk. The performance advantage of the Spark engine, combined with the industry-leading performance of the MapR Distribution, provides customers with the highest-performance platform for big data applications.\n<h2 id=\"why-databricks\">Why Databricks?</h2>\nDatabricks was founded by the creators of Apache Spark, and is currently the driving force behind the project. When we decided to add the Spark stack to our distribution and double down on our involvement in the Spark community, a strategic partnership with Databricks was a no-brainer. This partnership will benefit MapR customers who are interested in 24x7 support for Spark or any of the other projects in the stack, including Spark Streaming, Shark, MLLib and GraphX (with several other projects coming soon). In addition, MapR will be working closely with Databricks to drive the Spark roadmap and accelerate the development of new features, benefiting both MapR customers and the broader community.\n\nWe are very excited about the upcoming Apache Spark 1.0 release, expected later this month. We are looking forward to a great journey with Databricks and the other members of the Spark community.\n\n<a href=\"http://w.on24.com/r.htm?e=780379&amp;s=1&amp;k=A6FB573A31B1DD9D8AB6294A101383ED&amp;partnerref=MapR\" target=\"_blank\">Register for an upcoming joint webinar</a> to learn more about the benefits of the complete Spark stack on MapR.</td></tr><tr><td>List(Tathagata Das)</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2014-04-10, 2014-04-10, UTC)</td><td>We are happy to announce the availability of <a href=\"http://spark.apache.org/releases/spark-release-0-9-1.html\" target=\"_blank\">Apache Spark 0.9.1</a>! This is a maintenance release with bug fixes, performance improvements, better stability with YARN and improved parity of the Scala and Python API. We recommend all 0.9.0 users to upgrade to this stable release.\n\nThis is the first release since Spark graduated as a top level Apache project. Contributions to this release came from 37 developers.\n\nVisit the <a href=\"http://spark.apache.org/releases/spark-release-0-9-1.html\" target=\"_blank\">release notes</a> for more information about all the improvements and bug fixes. <a href=\"http://spark.apache.org/downloads.html\" target=\"_blank\">Download</a> it and try it out!</td></tr><tr><td>List(Steven Hillion)</td><td>List(Company Blog, Partners)</td><td>List(2014-04-01, 2014-04-01, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at Alpine Data Labs, part of the 'Application Spotlight' series highlighting innovative applications that are part of the Databricks \"Certified on Apache Spark\" program.</div>\n\n<hr />\n\nEveryone knows how hard it is to recruit engineers and data scientists in Silicon Valley. At <a href=\"http://www.alpinenow.com\" target=\"_blank\">Alpine Data Labs</a>, we think what we’re up to is pretty fun and challenging, but we still have to compete with other start-ups as well as the big internet companies to attract the best talent. One thing that can help is to be able to say that you’re working with the most innovative and powerful technologies.\n\nLast year, I was interviewing a talented engineer with a strong background in machine learning. And he said that the one thing he wanted to do above all was to work with Apache Spark. “Will I get to do that at Alpine?” he asked.\n\nIf it had been even a year earlier, I would have said “Sure…at some point.” But in the meantime I’d met several of the members of the <a href=\"https://amplab.cs.berkeley.edu\" target=\"_blank\">AMPLab</a> research team at Berkeley, and been impressed with their mature approach to building a platform and ecosystem. And I’d seen enough companies installing Spark on their dev clusters that it was clear this was a technology to watch. In a remarkably short time, it went from experimental to very real. And now prospects in the Alpine pipeline were asking me if it was on the roadmap. So yes, I told my candidate. “You’ll be working on Spark from day one.”\n\nLast week, Alpine announced at <a href=\"http://www.cmswire.com/cms/big-data/alpine-turns-big-data-to-a-priceless-asset-fast-gigaomlive-024541.php\" target=\"_blank\">GigaOM</a> that it’s one of the first analytics companies to leverage Spark for building predictive models. We demonstrated the Alpine engine running on <a href=\"http://www.gopivotal.com/big-data/analytics-workbench\" target=\"_blank\">Pivotal’s Analytics Workbench</a>, where it ran an iterative classification algorithm (logistic regression) on 50 million rows in less than 50 seconds.\n\nFurthermore, we were officially certified on Spark by the team at Databricks. It’s been an honor to work with them and the research team at Berkeley. We think their technology will be a serious contender for the leading platform for data science.\n\nSpark is more to us than just speed. It’s really the entire ecosystem that represents such an exciting paradigm for working with data.\n\nStill, the core capability of caching data in memory was our primary consideration, and our iterative algorithms have been shown to speed up by one or even two orders of magnitude (thanks again to that Pivotal cluster).\n\nWe’ve always had this mantra at Alpine: “Avoid multiple passes through the data!” And we’ve designed many of our machine learning algorithms to avoid scanning the data too many times, packing on calculations into each MapReduce job like a waiter piling up plates to try and clear a table in one go. But it’s rare that we can avoid it entirely. With Spark, it’s incredibly satisfying to watch the progress bar zip along as the system re-uses data it’s already seen before.\n\nAnother thing that’s getting our engineers excited is Spark’s <a href=\"http://spark.apache.org/mllib/\" target=\"_blank\">MLLib</a>, the machine-learning library written on top of the Spark runtime. Alpine has long thought that machine learning algorithms should be open source. (I helped to kick off the <a href=\"http://madlib.net/\" target=\"_blank\">MADlib</a> library of analytics functions for databases, and Alpine now uses it extensively.) So we’re now beginning to contribute some of our code back into MLLib. And, moreover, we think MLLib and MLI have the potential to be a more general repository for open-source machine learning.\n\nSo I’ll congratulate the Alpine team for helping to bring the power of Spark to our users, and I’ll also congratulate the Spark team and Databricks for making it possible!</td></tr><tr><td>List(Michael Armbrust, Reynold Xin)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-03-27, 2014-03-27, UTC)</td><td>Building a unified platform for big data analytics has long been the vision of Apache Spark, allowing a single program to perform ETL, MapReduce, and complex analytics. An important aspect of unification that our users have consistently requested is the ability to more easily import data stored in external sources, such as Apache Hive. Today, we are excited to announce <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\">Spark SQL</a>, a new component recently merged into the Spark repository.\n\nSpark SQL brings native support for SQL to Spark and streamlines the process of querying data stored both in RDDs (Spark’s distributed datasets) and in external sources. Spark SQL conveniently blurs the lines between RDDs and relational tables. Unifying these powerful abstractions makes it easy for developers to intermix SQL commands querying external data with complex analytics, all within in a single application. Concretely, Spark SQL will allow developers to:\n<ul>\n \t<li>Import relational data from Parquet files and Hive tables</li>\n \t<li>Run SQL queries over imported data and existing RDDs</li>\n \t<li>Easily write RDDs out to Hive tables or Parquet files</li>\n</ul>\n<h2 id=\"spark-sql-in-action\">Spark SQL In Action</h2>\nNow, let’s take a closer look at how Spark SQL gives developers the power to integrate SQL commands into applications that also take advantage of MLlib, Spark’s machine learning library. Consider an application that needs to predict which users are likely candidates for a service, based on their profile. Often, such an analysis requires joining data from multiple sources. For the purposes of illustration, imagine an application with two tables:\n<ul>\n \t<li>Users(userId INT, name String, email STRING,\nage INT, latitude: DOUBLE, longitude: DOUBLE,\nsubscribed: BOOLEAN)</li>\n \t<li>Events(userId INT, action INT)</li>\n</ul>\nGiven the data stored in in these tables, one might want to build a model that will predict which users are good targets for a new campaign, based on users that are similar.\n\n[scala]\n// Data can easily be extracted from existing sources,\n// such as Apache Hive.\nval trainingDataTable = sql(&quot;&quot;&quot;\n  SELECT e.action\n         u.age,\n         u.latitude,\n         u.logitude\n  FROM Users u\n  JOIN Events e\n  ON u.userId = e.userId&quot;&quot;&quot;)\n\n// Since `sql` returns an RDD, the results of the above\n// query can be easily used in MLlib\nval trainingData = trainingDataTable.map { row =&gt;\n  val features = Array[Double](row(1), row(2), row(3))\n  LabeledPoint(row(0), features)\n}\n\nval model =\n  new LogisticRegressionWithSGD().run(trainingData)\n[/scala]\n\nNow that we have used SQL to join existing data and train a model, we can use this model to predict which users are likely targets.\n\n[scala]\nval allCandidates = sql(&quot;&quot;&quot;\n  SELECT userId,\n         age,\n         latitude,\n         logitude\n  FROM Users\n  WHERE subscribed = FALSE&quot;&quot;&quot;)\n\n// Results of ML algorithms can be used as tables\n// in subsequent SQL statements.\ncase class Score(userId: Int, score: Double)\nval scores = allCandidates.map { row =&gt;\n  val features = Array[Double](row(1), row(2), row(3))\n  Score(row(0), model.predict(features))\n}\nscores.registerAsTable(&quot;Scores&quot;)\n\nval topCandidates = sql(&quot;&quot;&quot;\n  SELECT u.name, u.email\n  FROM Scores s\n    JOIN Users u ON s.userId = u.userId\n  ORDER BY score DESC\n  LIMIT 100&quot;&quot;&quot;)\n\n// Send emails to top candidates to promote the service.\n[/scala]\n\nIn this example, Spark SQL made it easy to extract and join the various datasets preparing them for the machine learning algorithm. Since the results of Spark SQL are also stored in RDDs, interfacing with other Spark libraries is trivial. Furthermore, Spark SQL allows developers to close the loop, by making it easy to manipulate and join the output of these algorithms, producing the desired final result.\n\nTo summarize, the unified Spark platform gives developers the power to choose the right tool for the right job, without having to juggle multiple systems. If you would like to see more concrete examples of using Spark SQL please check out the <a href=\"http://spark.apache.org/docs/1.0.0/sql-programming-guide.html\">programming guide</a>.\n<h2 id=\"optimizing-with-catalyst\">Optimizing with Catalyst</h2>\nIn addition to providing new ways to interact with data, Spark SQL also brings a powerful new optimization framework called Catalyst. Using Catalyst, Spark can automatically transform SQL queries so that they execute more efficiently. The Catalyst framework allows the developers behind Spark SQL to rapidly add new optimizations, enabling us to build a faster system more quickly. In one recent example, we found an inefficiency in Hive group-bys that took an experienced developer an entire weekend and over 250 lines of code to fix; we were then able to make the same fix in Catalyst in only a few lines of code.\n<h2 id=\"future-of-shark\">Future of Shark</h2>\nThe natural question that arises is about the future of Shark. Shark was among the first systems that delivered up to 100X speedup over Hive. It builds on the Apache Hive codebase and achieves performance improvements by swapping out the physical execution engine part of Hive. While this approach enables Shark users to speed up their Hive queries without modification to their existing warehouses, Shark inherits the large, complicated code base from Hive that makes it hard to optimize and maintain. As Spark SQL matures, Shark will transition to using Spark SQL for query optimization and physical execution so that users can benefit from the ongoing optimization efforts within Spark SQL.\n\nIn short, we will continue to invest in Shark and make it an excellent drop-in replacement for Apache Hive. It will take advantage of the new Spark SQL component, and will provide features that complement it, such as Hive compatibility and the standalone SharkServer, which allows external tools to connect queries through JDBC/ODBC.\n<h2 id=\"whats-next\">What’s next</h2>\nSpark SQL will be included in Spark 1.0 as an alpha component. However, this is only the beginning of better support for relational data in Spark, and this post only scratches the surface of Catalyst. Look for future blog posts on the following topics:\n<ul>\n \t<li>Generating custom bytecode to speed up expression evaluation</li>\n \t<li>Reading and writing data using other formats and systems, include Avro and HBase</li>\n \t<li>API support for using Spark SQL in Python and Java</li>\n</ul></td></tr><tr><td>List(Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-02-04, 2014-02-04, UTC)</td><td>Our goal with Apache Spark is very simple: provide the best platform for computation on big data. We do this through both a powerful core engine and rich libraries for useful analytics tasks. Today, we are excited to announce the release of Apache Spark 0.9.0. This major release extends Spark’s libraries and further improves its performance and usability. Apache Spark 0.9.0 is the largest release to date, with work from 83 contributors, who submitted over 300 patches.\n\nApache Spark 0.9 features significant extensions to the set of standard analytical libraries packaged with Spark. The release introduces GraphX, a library for graph computation that comes with implementations of several standard algorithms, such as PageRank. Spark’s machine learning library (MLlib) has been extended to support Python, using the NumPy numerical library. A Naive Bayes Classifier has also been added to MLlib. Finally, Spark Streaming, which supports near-real-time continuous computation, has added a simplified high-availability mode and several significant optimizations.\n\nIn addition to higher-level libraries, Spark 0.9 features improvements to the core computation engine. Spark now now automatically spills reduce output to disk, increasing the stability of workloads with very large aggregations. Support for Spark in YARN mode has been hardened and improved. The standalone mode has added automatic supervision of applications and better support for sharing clusters amongst several users. Finally, we’ve focused on stabilizing API’s ahead of Apache Spark’s 1.0 release to make things easy for developers writing Spark applications. This includes upgrading to Scala 2.10, allowing applications written in Scala to use newer libraries.\n\nApache Spark 0.9.0 can be <a href=\"http://spark.incubator.apache.org/downloads.html\">downloaded directly</a> from the Apache Spark website. It will also be available to CDH users via a Cloudera parcel, which can automatically install Spark on existing CDH clusters. For a more detailed explanation of the features in this release, head on over to the <a href=\"http://spark.incubator.apache.org/releases/spark-release-0-9-0.html\">official release notes</a>. Enjoy the newest release of Spark!</td></tr><tr><td>List(Ali Ghodsi, Ahir Reddy)</td><td>List(Apache Spark, Ecosystem, Engineering Blog)</td><td>List(2014-01-02, 2014-01-02, UTC)</td><td>Apache Hadoop integration has always been a key goal of Apache Spark and <a href=\"http://hortonworks.com/wp-content/uploads/2013/06/YARN.png\">YARN</a> users have long been able to run <a href=\"http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\">Spark on YARN</a>. However, up to now, it has been relatively hard to run Spark on Hadoop MapReduce v1 clusters, i.e. clusters that do not have YARN installed. Typically, users would have to get permission to install Spark/Scala on some subset of the machines, a process that could be time consuming. Enter <a href=\"http://databricks.github.io/simr/\">SIMR (Spark In MapReduce)</a>, which has been released in conjunction with <a href=\"https://databricks.com/blog/2013/12/19/release-0_8_1.html\">Apache Spark 0.8.1</a>.\n\nSIMR allows anyone with access to a Hadoop MapReduce v1 cluster to run Spark out of the box. A user can run Spark directly on top of Hadoop MapReduce v1 without any administrative rights, and without having Spark or Scala installed on any of the nodes. The only requirements are HDFS access and MapReduce v1. SIMR is open sourced under the <a href=\"http://apache.org/licenses/LICENSE-2.0.html\">Apache license</a> and was jointly developed by Databricks and UC Berkeley <a href=\"http://amplab.cs.berkeley.edu\">AMPLab</a>.\n\nThe basic idea is that a user can download the package of SIMR (<a href=\"http://databricks.github.io/simr/#download\">3 files</a>) that matches their Hadoop cluster and immediately start using Spark. SIMR includes the interactive Spark shell, and allows users to use the shell backed by the computational power of the cluster. It is a simple as <code>./simr --shell</code>:\n\n&nbsp;\n\n<img src=\"https://databricks.com/wp-content/uploads/2014/01/simrshell.png\" alt=\"simrshell\" width=\"90%\" />\n\nRunning a Spark program simply requires bundling it along with its dependencies into a jar and launching the job through SIMR. SIMR uses the following command line syntax for running jobs:\n\n<pre>./simr jar_file main_class parameters</pre>\n\nSIMR simply launches a MapReduce job with the desired number of map slots, and ensures that Spark/Scala and your job gets shipped to all those nodes. It then designates one of the mappers as the master and runs the Spark driver inside it. On the rest of the mappers it launches Spark executors that will execute tasks on behalf of the driver. Voila, your Spark job is running inside MapReduce on the cluster.\n\nSIMR lets users interact with the driver program. For example, users can type into the Spark shell and see its output interactively. The way this works is that SIMR runs a relay server on the master mapper and a relay client on the machine that launched SIMR. Any input from the client and output from the driver are relayed back and forth between the client and the master mapper.\n\nTo make all this work, SIMR makes extensive use of HDFS. The master mapper is elected using leader election by writing to HDFS and picking the mapper that firsts gets to write to HDFS. Similarly, the executors launched inside the rest of the mappers find out the driver’s URL by reading it from a specific file from HDFS. Thus, SIMR uses MapReduce and HDFS in place of a cluster manager.\n\n<img src=\"https://databricks.com/wp-content/uploads/2014/01/simr-arch.png\" alt=\"simr-arch\" width=\"90%\" />\n\nSIMR is not intended to be used in production mode, but rather to enable users to explore and use Spark before running it on a proper resource manager, such as YARN, Mesos, or Standalone Mode. MapReduce 2 (YARN) users can of course use the existing <a href=\"http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\">Spark on YARN</a> solution, or explore other <a href=\"http://spark.incubator.apache.org/docs/latest/index.html#launching-on-a-cluster\">Spark deployment options</a>.\n\nWe hope SIMR will enable users to try out Spark without any heavy operational burden. Towards this goal, we have pre-built several SIMR binaries for different versions of Hadoop. Please go ahead and try it and let us know if you have any feedback.\n\nSIMR resources:\n<ul>\n \t<li><a href=\"http://databricks.github.io/simr\">Homepage</a></li>\n \t<li><a href=\"http://databricks.github.io/simr#download\">Download</a></li>\n \t<li><a href=\"https://github.com/databricks/simr\">Source code</a></li>\n</ul></td></tr><tr><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td><td>List(Company Blog, Customers)</td><td>List(2014-03-26, 2014-03-26, UTC)</td><td><div class=\"post-meta\">We're very happy to see our friends at Cloudera continue to get the word out about Apache Spark, and their latest blog post is a great example of how users are putting Spark Streaming to use to solve complex problems in real time. Thanks to Russell Cardullo and Michael Ruggiero, Data Infrastructure Engineers at <a href=\"http://engineering.sharethrough.com/\">Sharethrough</a>, for this <a href=\"http://blog.cloudera.com/blog/2014/03/letting-it-flow-with-spark-streaming/\">guest post on Cloudera's blog</a>, which we've cross-posted below</div>\n\n<hr />\n\nAt Sharethrough, which offers an advertising exchange for delivering in-feed ads, we’ve been running on CDH for the past three years (after migrating from Amazon EMR), primarily for ETL. With the launch of our exchange platform in early 2013 and our desire to optimize content distribution in real time, our needs changed, yet CDH remains an important part of our infrastructure.\n\nIn mid-2013, we began to examine stream-based approaches to accessing click-stream data from our pipeline. We asked ourselves: Rather than “warm up our cold path” by running those larger batches more frequently, can we give our developers a programmatic model and framework optimized for incremental, small batch processing, yet continue to rely on the Cloudera platform? Ideally, our engineering team focuses on the data itself, rather than worrying about details like consistency of state across the pipeline or fault recovery.\n<h2>Spark (and Spark Streaming)</h2>\n<a href=\"http://spark.incubator.apache.org/\">Apache Spark</a> is a fast and general framework for large-scale data processing, with a programming model that supports building applications that would be more complex or less feasible using conventional MapReduce. (Spark ships inside Cloudera Enterprise 5, and is already supported for use with CDH 4.4 and later.) With an in-memory persistent storage abstraction, Spark supports complete MapReduce functionality without the long execution times required by things like data replication, disk I/O, and serialization.\n\nBecause <a href=\"http://spark.incubator.apache.org/streaming/\">Spark Streaming</a> shares the same API as Spark’s batch and interactive modes, we now use Spark Streaming to aggregate business-critical data in real time. A consistent API means that we can develop and test locally in the less complex batch mode and have that job work seamlessly in production streaming. For example, we can now optimize bidding in real time, using the entire dataset for that campaign without waiting for our less frequently run ETL flows to complete. We are also able to perform real-time experiments and measure results as they come in.\n<h2>Before and After</h2>\nOur batch-processing system looks like this:\n<ol>\n \t<li>Apache Flume writes out files based on optimal HDFS block size (64MB) to hourly buckets.</li>\n \t<li>MapReduce (Scalding) jobs are scheduled N times per day.</li>\n \t<li>Apache Sqoop moves results into the data warehouse.</li>\n \t<li>Latency is ~1 hour behind, plus Hadoop processing time.</li>\n</ol>\n<img class=\"alignleft size-full wp-image-152\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-streaming11.png\" alt=\"spark-streaming11\" width=\"450px\" />\n\n<strong>Sharethrough’s former batch-processing dataflow</strong>\n\nFor our particular use case, this batch-processing workflow wouldn’t provide access to performance data while the results of those calculations would still be valuable. For example, knowing that a client’s optimized content performance is 4.2 percent an hour after their daily budget is spent, means our advertisers aren’t getting their money’s worth, and our publishers aren’t seeing the fill they need. Even when the batch jobs take minutes, a spike in traffic could slow down a given batch job, causing it to “bump into” newly launched jobs.\n\nFor these use cases, a streaming dataflow is the viable solution:\n<ol>\n \t<li>Flume writes out clickstream data to HDFS.</li>\n \t<li>Spark reads from HDFS at batch sizes of five seconds.</li>\n \t<li>Output to a key-value store, updating our predictive modeling.</li>\n</ol>\n<img class=\"alignleft size-full wp-image-152\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-streaming21.png\" alt=\"spark-streaming11\" width=\"450px\" />\n\n<strong>Sharethrough’s new Spark Streaming-based dataflow</strong>\n\nIn this new model, our latency is only Spark processing time and the time it takes Flume to transmit files to HDFS; in practice, this works out to be about five seconds.\n<h2>On the Journey</h2>\nWhen we began using Spark Streaming, we shipped quickly with minimal fuss. To get the most out of our new streaming jobs, we quickly adjusted to the Spark programming model.\n\nHere are some things we discovered along the way:\n<ul>\n \t<li>The profile of a 24 x 7 streaming app is different than an hourly batch job — you may need finer-grained alerting and more patience with repeated errors. And with a streaming application, good exception handling is your friend. (Be prepared to answer questions like: “What if the Spark receiver is unavailable? Should the application retry? Should it forget data that was lost? Should it alert you?”)</li>\n \t<li>Take time to validate output against the input. A stateful job that, for example, keeps a count of clicks, may return results you didn’t expect in testing.</li>\n \t<li>Confirm that supporting objects are being serialized. The Scala DSL makes it easy to close over a non-serializable variable or reference. In our case, a GeoCoder object was not getting serialized and our app became very slow; it had to return to the driver program for the original, non-distributed object.</li>\n \t<li>The output of your Spark Streaming job is only as reliable as the queue that feeds Spark. If the producing queue drops, say, 1 percent of messages, you may need a periodic reconciliation strategy (such as merging your lossy “hot path” with “cold path” persistent data). For these kinds of merges, the monoid abstraction can be helpful when you need certainty that associative calculations (counts, for example) are accurate and reliable. For more on this, see merge-able stores like Twitter’s <a href=\"https://github.com/twitter/storehaus\">Storehaus</a> or Oscar Boykin’s “<a href=\"https://speakerdeck.com/johnynek/algebra-for-analytics\">Algebra for Analytics</a>“.</li>\n</ul>\n<h2>Conclusion</h2>\nSharethrough Engineering intends to do a lot more with Spark Streaming. Our engineers can interactively craft an application, test it in batch, move it into streaming and it just works. We’d encourage others interested in unlocking real-time processing to look at Spark Streaming. Because of the concise Spark API, engineers comfortable with MapReduce can build streaming applications today without having to learn a completely new programming model.\n\nSpark Streaming equips your organization with the kind of insights only available from up-to-the-minute data, either in the form of machine-learning algorithms or real-time dashboards: It’s up to you!</td></tr><tr><td>List(Jai Ranganathan, Matei Zaharia)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-03-21, 2014-03-21, UTC)</td><td><div class=\"post-meta\">\n\nThis article was cross-posted in the <a href=\"http://blog.cloudera.com/blog/2014/03/apache-spark-a-delight-for-developers/\">Cloudera developer blog</a>.\n\n</div>\n<a href=\"http://spark.apache.org/\">Apache Spark</a> is well known today for its <a href=\"http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/\">performance benefits</a> over MapReduce, as well as its <a href=\"http://blog.cloudera.com/blog/2014/03/why-apache-spark-is-a-crossover-hit-for-data-scientists/\">versatility</a>. However, another important benefit — the elegance of the development experience — gets less mainstream attention.\n\nIn this post, you’ll learn just a few of the features in Spark that make development purely a pleasure.\n<h2>Language Flexibility</h2>\nSpark natively provides support for a variety of popular development languages. Out of the box, it supports Scala, Java, and Python, with some promising work ongoing <a href=\"http://amplab-extras.github.io/SparkR-pkg/\">to support R</a>.\n\nOne common element among these languages (with the temporary exception of Java, which is due for a major update imminently in the form of Java 8) is that they all provide concise ways to express operations using “closures” and lambda functions. <a href=\"http://en.wikipedia.org/wiki/Closure_(computer_programming)\">Closures</a> allow users to define functions in-line with the core logic of the application, thereby preserving application flow and making for tight and easy-to-read code:\n\n<strong>Closures in Python with Spark:</strong>\n\n<pre>lines = sc.textFile(...)\nlines.filter(lambda s:\"ERROR\"in s).count()</pre>\n\n<strong>Closures in Scala with Spark:</strong>\n\n<pre>val lines = sc.textFile(...)\nlines.filter(s => s.contains(\"ERROR\")).count()</pre>\n\n<strong>Closures in Java with Spark:</strong>\n\n<pre>JavaRDD<String> lines = sc.textFile(...);\nlines.filter(newFunction<String,Boolean>()  {\n  Boolean call(String s){\n    return s.contains(\"error\");\n  }\n}).count();</pre>\n\nOn the performance front, a lot of work has been done to optimize all three of these languages to run efficiently on the Spark engine. Spark is written in Scala, which runs on the JVM, so Java can run efficiently in the same JVM container. Via the smart use of <a href=\"http://py4j.sourceforge.net/\">Py4J</a>, the overhead of Python accessing memory that is managed in Scala is also minimal.\n<h2>APIs That Match User Goals</h2>\nWhen developing in MapReduce, you are often forced to stitch together basic operations as custom Mapper/Reducer jobs because there are no built-in features to simplify this process. For that reason, many developers turn to the higher-level APIs offered by frameworks like Apache Crunch or Cascading to write their MapReduce jobs.\n\nIn contrast, Spark natively provides a rich and ever-growing library of operators. Spark APIs include functions for:\n\n<ul>\n<li><code>cartesian</code></li>\n<li><code>cogroup</code></li>\n<li><code>collect</code></li>\n<li><code>count</code></li>\n<li><code>countByValue</code></li>\n<li><code>distinct</code></li>\n<li><code>filter</code></li>\n<li><code>flatMap</code></li>\n<li><code>fold</code></li>\n<li><code>groupByKey</code></li>\n<li><code>join</code></li>\n<li><code>map</code></li>\n<li><code>mapPartitions</code></li>\n<li><code>reduce</code></li>\n<li><code>reduceByKey</code></li>\n<li><code>sample</code></li>\n<li><code>sortByKey</code></li>\n<li><code>subtract</code></li>\n<li><code>take</code></li>\n<li><code>union</code></li>\n</ul>\n\nand <a href=\"https://spark.apache.org/docs/0.9.0/api/core/#org.apache.spark.rdd.RDD\">many more.</a> In fact, there are more than 80 operators available out of the box in Spark!\n\nWhile many of these operations often boil down to Map/Reduce equivalent operations, the high-level API matches user intentions closely, allowing you to write much more concise code.\n\nAn important note here is that while scripting frameworks like Apache Pig provide many high-level operators as well, Spark allows you to access these operators in the context of a full programming language — thus, you can use control statements, functions, and classes as you would in a typical programming environment.\n<h2>Automatic Parallelization of Complex Flows</h2>\nWhen constructing a complex pipeline of MapReduce jobs, the task of correctly parallelizing the sequence of jobs is left to you. Thus, a scheduler tool such as Apache Oozie is often required to carefully construct this sequence.\n\nWith Spark, a whole series of individual tasks is expressed as a single program flow that is lazily evaluated so that the system has a complete picture of the execution graph. This approach allows the core scheduler to correctly map the dependencies across different stages in the application, and automatically parallelize the flow of operators without user intervention.\n\nThis capability also has the property of enabling certain optimizations to the engine while reducing the burden on the application developer. Win, and win again!\n\nFor example, consider the following job:\n\n<pre>rdd1.map(splitlines).filter(\"ERROR\")\nrdd2.map(splitlines).groupBy(key)\nrdd2.join(rdd1, key).take(10)</pre>\n\n<p align=\"center\"><img style=\"width: 90%; height: auto;\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-devs1.png\" alt=\"spark-devs1\" /></p>\n\nThis simple application expresses a complex flow of six stages. But the actual flow is completely hidden from the user — the system automatically determines the correct parallelization across stages and constructs the graph correctly. In contrast, alternate engines would require you to manually construct the entire graph as well as indicate the proper parallelization.\n\n<h2>Interactive Shell</h2>\n\nSpark also lets you access your datasets through a simple yet specialized Spark shell for Scala and Python. With the Spark shell, developers and users can get started accessing their data and manipulating datasets without the full effort of writing an end-to-end application. Exploring terabytes of data without compiling a single line of code means you can understand your application flow by literally test-driving your program before you write it up.\n\n<p align=\"center\"><img style=\"width: 90%; height: auto;\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-devs21.png\" alt=\"spark-devs21\" /></p>\n\nJust open up a shell, type a few commands, and you’re off to the races!\n\n<h2>Performance</h2>\n\nWhile this post has focused on how Spark not only improves performance but also programmability, we should’t ignore one of the best ways to make developers more efficient: performance!\n\nDevelopers often have to run applications many times over the development cycle, working with subsets of data as well as full data sets to repeatedly follow the develop/test/debug cycle. In a Big Data context, each of these cycles can be very onerous, with each test cycle, for example, being hours long.\n\nWhile there are various ways systems to alleviate this problem, one of the best is to simply run your program fast. Thanks to the performance benefits of Spark, the development lifecycle can be materially shortened merely due to the fact that the test/debug cycles are much shorter.\n\nAnd your end-users will love you too!\n\n<p align=\"center\"><img style=\"width: 90%; height: auto;\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-dev3.png\" alt=\"spark-dev3\" /></p>\n\n<h2>Example: WordCount</h2>\n\nTo give you a sense of the practical impact of these benefits in a concrete example, the following two snippets of code reflect a WordCount implementation in MapReduce versus one in Spark. The difference is self-explanatory:\n\n<strong>WordCount the MapReduce way:</strong>\n\n<pre>public static class WordCountMapClass extends MapReduceBase\n  implements Mapper<LongWritable, Text, Text, IntWritable> {\n  private final static IntWritable one = newIntWritable(1);\n  private Text word = newText();\n  public void map(LongWritable key,Text value,\n                  OutputCollector <Text, IntWritable> output,\n                  Reporter reporter) throws IOException {\n    String line = value.toString();\n    StringTokenizer itr = newStringTokenizer(line);\n    while (itr.hasMoreTokens()) {\n      word.set(itr.nextToken());\n      output.collect(word, one);\n    }\n  }\n}\n\npublic static class WordCountReduce extends MapReduceBase\n  implements Reducer<Text, IntWritable, Text, IntWritable> {\n  public void reduce(Text key,Iterator<IntWritable> values,\n                     OutputCollector<Text,IntWritable> output,\n                     Reporter reporter) throws IOException{\n    int sum = 0;\n    while (values.hasNext()) {\n      sum += values.next().get();\n    }\n    output.collect(key,newIntWritable(sum));\n  }\n}</pre>\n\n<strong>WordCount the Spark way:</strong>\n\n<pre>val spark = newSparkContext(master, appName, home, jars)\nval file = spark.textFile(\"hdfs://...\")\nval counts = file.flatMap(line => line.split(\" \"))\n                 .map(word =>(word,1))\n                 .reduceByKey(_ + _)\ncounts.saveAsTextFile(\"hdfs://...\")</pre>\n\nOne cantankerous data scientist at Cloudera, Uri Laserson, wrote his first PySpark job recently after several years of tussling with raw MapReduce. Two days into Spark, he declared his intent to never write another MapReduce job again.\n\nUri, we got your back, buddy: <a href=\"http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/\">Spark will ship inside CDH 5.</a>\n\n<h2>Further Reading</h2>\n\n<ul>\n\t<li><a href=\"http://spark.apache.org/docs/latest/quick-start.html\">Spark Quick Start</a></li>\n\t<li><a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package\">Spark API for Scala</a></li>\n        <li><a href=\"http://spark.apache.org/docs/latest/api/java/index.html\">Spark API for Java</a></li>\n\t<li><a href=\"http://spark.apache.org/docs/latest/api/python/index.html\">Spark API for Python</a></li>\n</ul>\n\n<em>Jai Ranganathan is Director of Product at Cloudera.</em>\n\n<em>Matei Zaharia is CTO of Databricks.</em></td></tr><tr><td>List(Databricks Press Office)</td><td>List(Announcements, Company Blog)</td><td>List(2014-03-19, 2014-03-19, UTC)</td><td><strong>BERKELEY, Calif. – March 18, 2014 –</strong> Databricks, the company founded by the creators of Apache Spark that is revolutionizing what enterprises can do with Big Data, today announced the Databricks <a href=\"/certification/\">“Certified on Spark” Program</a> for applications built on top of the Apache Spark platform. This program ensures that certified applications will work with a multitude of commercially supported Spark distributions.\n\n“Pioneering application developers that are leveraging the power of Spark have had to choose between two sub-optimal choices: they either have to package Spark platform support with their application or attempt to maintain integration/certification individually with a rapidly increasing set of commercially supported Spark distributions,” said Ion Stoica, Databricks CEO. “The Databricks ‘Certified on Spark’ program enables developers to certify solely against the 100% open-source Apache Spark distribution, and ensures interoperability with Apache Spark-compatible distributions. Databricks will handle the task of certifying the compatibility of each commercial Spark distribution with the Apache version and will soon announce the initial set of distributions that meet this criteria.”\n\nThe pioneering partners of the Databricks certification program include Adatao, Alpine Data Labs, and Tresata - advanced analytics application vendors that have been at the forefront in leveraging the power of Spark to deliver faster and deeper insights for their enterprise customers.\n\n“Certified on Spark” also provides multiple benefits for enterprise users including:\n<ul>\n \t<li>Decoupling Spark distribution (and commercial support) from application licensing</li>\n \t<li>Full transparency into which applications are truly designed to work with and leverage the power of Spark</li>\n \t<li>A rapidly increasing set of certified applications to incentivize distribution vendors to maintain compatibility with Apache Spark and avoid forking and fragmentation, ultimately resulting in greater compatibility and shared innovation for users</li>\n</ul>\n“At Databricks we are fully committed to keeping Spark 100% open source and to bringing it to the widest possible set of users,” said Matei Zaharia, Databricks CTO and the creator of Spark. “The ‘Certified on Spark’ program is key to maintaining a healthy and unified Apache Spark platform, and it is an expression of our community focused efforts, such as organizing meetups and the upcoming Spark Summit, and driving the future of open-source development of Spark.”\n\nApplication developers that are interested in applying for the “Certified on Spark” program should visit <a href=\"http://www.databricks.com\">www.databricks.com</a> and select “Apply for Certification”. Enterprise users can also visit the Databricks site regularly to see the latest set of certified application vendors and read “application spotlight” blog articles that deep-dive into specific examples of Spark-powered applications.</td></tr><tr><td>List(Ion Stoica)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-03-03, 2014-03-03, UTC)</td><td><div class=\"blogContent\">\n\nWe are delighted with the recent <a href=\"https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50\">announcement</a> of the Apache Software Foundation that <a href=\"http://spark.apache.org\">Apache Spark</a> has become a top-level Apache project. This is a recognition of the fantastic work done by the Spark open source community, which now counts over 140 developers from 30+ companies.\n\nIn short time, Spark has become an increasingly popular solution for numerous big data applications, including machine learning, interactive queries, and stream processing. Spark now is an integral part of the Hadoop ecosystem, with many organizations employing Spark to perform sophisticated processing on their Hadoop data.\n\nAt Databricks we are looking forward to continuing our work with the open source community to accelerate the development and adoption of Apache Spark. Currently employing the lead developers and creators of many of the components of the Spark ecosystem, including Matei Zaharia, the creator of Spark, and Patrick Wendell, the release manager of Spark, we are committed to the success of Apache Spark.\n\n</div></td></tr><tr><td>List(Ahir Reddy, Reynold Xin)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-02-13, 2014-02-13, UTC)</td><td>The AMPLab at UC Berkeley, with help from Databricks, recently released an update to the <a href=\"https://amplab.cs.berkeley.edu/benchmark/\">Big Data Benchmark</a>. This benchmark uses Amazon EC2 to compare performance of five popular SQL query engines in the Big Data ecosystem on common types of queries, which can be reproduced through publicly available scripts and datasets.\n\nIn the past year, the community has invested heavily in performance optimizations of query engines. We are glad to see that all projects have evolved in this area. Although the queries used in the benchmark are simple, we are proud that Shark remains one of the fastest engines for these workloads, and has improved significantly since the last run.\n\nWhile this benchmark reaffirms Shark as a highly performant SQL query engine, we are working hard at Databricks to push the boundaries further. Stay tuned for some exciting news we will share soon with the community.\n<ul>\n\t<li><a href=\"https://amplab.cs.berkeley.edu/benchmark/\">Big Data Benchmark</a></li>\n</ul>\n&nbsp;</td></tr><tr><td>List(Pat McDonough)</td><td>List(Company Blog, Events)</td><td>List(2014-02-11, 2014-02-11, UTC)</td><td>The Databricks team is excited to take part in a number of activities throughout the 2014 O’Reilly Strata Conference in Santa Clara. From hands-on training, to office hours, to several talks (including a keynote), there are plenty of chances for attendees to learn how Apache Spark is bringing ease of use and outstanding performance to your big data. The schedule for the Databricks team includes:\n<ul>\n \t<li><a href=\"http://ampcamp.berkeley.edu/4/\">AMPCamp4</a>, Hosted at Strata</li>\n \t<li><a href=\"http://strataconf.com/strata2014/public/content/office-hours\">Office Hours</a> on Wednesday at 5:45pm</li>\n \t<li><a href=\"http://strataconf.com/strata2014/public/schedule/detail/33057\">How Companies are Using Spark, and Where the Edge in Big Data Will Be</a>, a keynote talk presented by Matei Zaharia on Thursday at 9:15am</li>\n \t<li><a href=\"http://strataconf.com/strata2014/public/schedule/detail/32375\">Querying Petabytes of Data in Seconds with BlinkDB</a>, co-presented by Reynold Xin on Thursday at 1:30pm</li>\n</ul>\n<h2 id=\"about-ampcamp-4\">About AMPCamp 4</h2>\nWe’ll be kicking off the conference on Tuesday by helping to host the fourth iteration of AMPCamp, where some of the newer components of the Berkeley Data Analytics stack will get some time in the spotlight. There are talks on BlinkDB, MLbase, GraphX, and Tachyon in the morning, and hands-on training for BlinkDB, MLbase, Spark, Spark Streaming, GraphX, and Shark in the afternoon.\n<h2 id=\"conference-talks--office-hours\">Conference Talks &amp; Office Hours</h2>\nOn Wednesday afternoon, attendees can come speak directly with a few of the members from our team in Strata’s Office Hours. Office hours provide a chance to meet face-to-face with the Databricks team and to ask any questions about the Spark project and ecosystem.\n\nTo learn the latest about Apache Spark, attendees should listen to talks featuring the Databricks team, including a keynote by our CTO Matei Zaharia. On Thursday morning, Matei will describe how organizations must now compete on the speed and sophistication with which they can draw value from data, and how Apache Spark makes a new class of applications possible.\n<h2 id=\"well-see-you-there\">We’ll See You There!</h2>\nWe’re looking forward to seeing you at the conference!</td></tr><tr><td>List(Ion Stoica)</td><td>List(Apache Spark, Ecosystem, Engineering Blog)</td><td>List(2014-01-22, 2014-01-22, UTC)</td><td>We are often asked how does <a href=\"http://spark.incubator.apache.org\">Apache Spark</a> fits in the Hadoop ecosystem, and how one can run Spark in a existing Hadoop cluster. This blog aims to answer these questions.\n\nFirst, Spark is intended to <em>enhance</em>, not replace, the Hadoop stack. From day one, Spark was designed to read and write data from and to HDFS, as well as other storage systems, such as HBase and Amazon’s S3. As such, Hadoop users can enrich their processing capabilities by combining Spark with Hadoop MapReduce, HBase, and other big data frameworks.\n\nSecond, we have constantly focused on making it as easy as possible for <em>every Hadoop user</em> to take advantage of Spark’s capabilities. No matter whether you run Hadoop 1.x or Hadoop 2.0 (YARN), and no matter whether you have administrative privileges to configure the Hadoop cluster or not, there is a way for you to run Spark! In particular, there are three ways to deploy Spark in a Hadoop cluster: standalone, YARN, and SIMR.\n\n<img class=\"aligncenter\" src=\"https://databricks.com/wp-content/uploads/2014/01/SparkHadoop.png\" alt=\"SparkHadoop1.png\" />\n\n<strong>Standalone deployment</strong>: With the standalone deployment one can statically allocate resources on all or a subset of machines in a Hadoop cluster and run Spark side by side with Hadoop MR. The user can then run arbitrary Spark jobs on her HDFS data. Its simplicity makes this the deployment of choice for many Hadoop 1.x users.\n\n<strong><a href=\"https://hadoop.apache.org/docs/current2/hadoop-yarn/hadoop-yarn-site/YARN.html\">Hadoop Yarn</a> deployment</strong>: Hadoop users who have already deployed or are planning to deploy <a href=\"https://hadoop.apache.org/docs/current2/hadoop-yarn/hadoop-yarn-site/YARN.html\">Hadoop Yarn</a> can simply run Spark on YARN without any pre-installation or administrative access required. This allows users to easily integrate Spark in their Hadoop stack and take advantage of the full power of Spark, as well as of other components running on top of Spark.\n\n<strong>Spark In MapReduce (<a href=\"https://databricks.com/blog/2014/01/01/simr.html\">SIMR</a>)</strong>: For the Hadoop users that are not running YARN yet, another option, in addition to the standalone deployment, is to use SIMR to launch Spark jobs inside MapReduce. With SIMR, users can start experimenting with Spark and use its shell within a couple of minutes after downloading it! This tremendously lowers the barrier of deployment, and lets virtually everyone play with Spark.\n<h2 id=\"interoperability-with-other-systems\">Interoperability with other Systems</h2>\nSpark interoperates not only with Hadoop, but with other popular big data technologies as well.\n<ul>\n \t<li><strong><a href=\"http://hive.apache.org/\">Apache Hive</a></strong>: Through <a href=\"https://github.com/amplab/shark/wiki\">Shark</a>, Spark enables Apache Hive users to run their unmodified queries much faster. Hive is a popular data warehouse solution running on top of Hadoop, while Shark is a system that allows the Hive framework to run on top of Spark instead of Hadoop. As a result, Shark can accelerate Hive queries by as much as 100x when the input data fits into memory, and up 10x when the input data is stored on disk.</li>\n \t<li><strong><a href=\"http://aws.amazon.com/\">AWS EC2</a></strong>: Users can easily run Spark (and Shark) on top of Amazon’s EC2 either using the <a href=\"http://spark.incubator.apache.org/docs/latest/ec2-scripts.html\">scripts</a> that come with Spark, or the hosted <a href=\"http://aws.amazon.com/articles/4926593393724923\">versions of Spark and Shark</a> on Amazon’s Elastic MapReduce.</li>\n \t<li><strong><a href=\"http://mesos.apache.org/\">Apache Mesos</a></strong>: Spark runs on top of Mesos, a cluster manager system which provides efficient resource isolation across distributed applications, including <a href=\"http://www.mcs.anl.gov/research/projects/mpi/\">MPI</a> and Hadoop. Mesos enables <em>fine grained</em> sharing which allows a Spark job to dynamically take advantage of the idle resources in the cluster during its execution. This leads to considerable performance improvements, especially for long running Spark jobs.</li>\n</ul></td></tr><tr><td>List(Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2013-12-20, 2013-12-20, UTC)</td><td>We are happy to announce the release of Apache Spark 0.8.1. In addition to performance and stability improvements, this release adds three new features. First, Spark now supports for the newest versions of YARN (2.2+). Second, the standalone cluster manager supports a high-availability mode in which it can tolerate master failures. Third, shuffles have been optimized to create fewer files, improving shuffle performance drastically in some settings.\n\nIn conjunction with the Apache Spark 0.8.1 release we are separately releasing <a href=\"https://databricks.com/blog/2014/01/01/simr.html\">Spark In MapReduce (SIMR)</a>, which enables seamlessly running Spark on Hadoop MapReduce v1 clusters without requiring the installation of Scala or Spark.\n\nWhile Apache Spark 0.8.1 is a minor release, it includes these larger features for the benefit of Scala 2.9 users. The next major release of Apache Spark, 0.9.0, will be based on Scala 2.10.\n\nThis release was a community effort, featuring contributions from more than 40 developers. For much more information about Apache Spark 0.8.1, head over to the <a href=\"http://spark.incubator.apache.org/releases/spark-release-0-8-1.html\">release notes</a> or <a href=\"http://spark.incubator.apache.org/downloads.html\">download Apache Spark 0.8.1</a> and try it out for yourself.</td></tr><tr><td>List(Andy Konwinski)</td><td>List(Company Blog, Customers, Events)</td><td>List(2013-12-19, 2013-12-19, UTC)</td><td>Earlier this month we held the <a href=\"http://spark-summit.org/2013\">first Spark Summit</a>, a conference to bring the Apache Spark community together. We are excited to share some statistics and highlights from the event.\n<ul>\n \t<li>450 participants from over 180 companies attended</li>\n \t<li>Participants came from 13 countries</li>\n \t<li>Spark training was sold out at 200 participants from 80 companies</li>\n \t<li>20 organizations sponsored the event, including all major Hadoop platform vendors</li>\n \t<li>20 different organizations gave talks</li>\n</ul>\nVideos and slides for all talks are now available on the <a href=\"http://spark-summit.org/2013\">Summit 2013 page</a>.\n\nThe Summit included Keynotes from Databricks, the UC Berkeley AMPLab, and Yahoo, as well as presentations from 18 other companies including Amazon, Red Hat, and Adobe. Talk topics covered a wide range including specialized applications such as mapping and manipulating the brain, product launches, and research projects about future directions for the project. We are very excited to see Spark and related projects come such a long way from research prototypes originally developed in AMPLab at Berkeley to being used in production by startups and large companies alike.\n<h2 id=\"the-state-of-spark-and-where-were-going-next\">The State of Spark, and Where We’re Going Next</h2>\n<iframe style=\"float: left; padding: 10px;\" src=\"//www.youtube.com/embed/nU6vO2EJAb4\" width=\"300\" height=\"173\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nIn the first keynote of the day, Matei Zaharia, who started the Spark project and is now CTO at Databricks, gave a summary of recent growth in the projec<span style=\"letter-spacing: 3px;\">t</span><a style=\"vertical-align: super; font-size: 60%;\" href=\"#footnote-1\">1</a>, highlighting key contributions from across the community. In particular, Spark recently reached 100 contributors, making it the second-largest open source development community in the Big Data space after Hadoop, and it’s also overtaken Hadoop in the past 6 months. Matei also previewed what is coming up in the Spark development roadmap, including features under development for the upcoming 0.8.1 and 0.9 releases including high availability for the Spark Master in standalone mode, external hashing and sorting, support for Scala 2.10, and more. Finally, Matei discussed features of Spark that differentiate it from other projects, such as the focus on unification of diverse programming models.\n<h2 id=\"spark-and-hadoop\">Spark and Hadoop</h2>\n<iframe style=\"float: right; padding-left: 10px;\" src=\"//www.youtube.com/embed/qs40jiN2iwM\" width=\"300\" height=\"173\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nIt was exciting to hear from Eric Baldeschwieler, whose background includes leading the Yahoo team that took Hadoop from being a prototype project to what it is today, as well as serving as both CEO and CTO of Hortonworks. In his keynote, Eric presented his view of how Spark is on track to become the “lingua franca” for Big Data. He also talked about how Spark updates Hadoop with important features such as effective utilization of RAM, low latency queries, and streaming ingest. Finally, he discussed how the Spark and Hadoop projects relate to each other now and going forward.\n<h2 id=\"spark-training-day\">Spark Training Day</h2>\nOn the sold-out second day, 200 attendees heard 4 talks on using, deploying, and administering Spark, and also participated in hands-on training led by the creators of Spark. Amazon, a sponsor of the Summit, donated EC2 resources so participants could each have their own 6 node Spark cluster to practice using Spark, Spark Streaming and Shark on real Wikipedia and Twitter data. We have made the hands-on exercises <a href=\"http://spark-summit.org/exercises\">available on the summit website</a> for free. Using these, anybody can use their own Amazon AWS account to launch a 6 node Spark cluster and get experience analyzing real data.\n\n<iframe style=\"float: left; padding-right: 10px;\" src=\"//www.youtube.com/embed/zGW8glN-Mo8\" width=\"300\" height=\"173\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nThe final talk of the training day was given by core Spark developer and Databricks cofounder Patrick Wendell. Patrick’s talk was about Administering Spark and was prepared in response to requests for more advanced technical material as part of the training. In it he dove into the core software components of the project, the different resource managers that Spark runs on, what type of hardware to use when running Spark, how to link against Spark when writing applications, monitoring Spark clusters running in production, and more.\n<h2 id=\"other-interesting-talks\">Other Interesting Talks</h2>\nIn addition to those above, there were many more great talks at the Summit, here are a few more that highligh the diversity of topics covered:\n<ul class=\"talk-list\">\n \t<li><span class=\"talk-title\">Mapping and manipulating the brain at scale</span> (<a href=\"http://spark-summit.org/talk/freeman-mapping-and-manipulating-the-brain-at-scale/\">abstract</a>, <a href=\"http://www.jeremyfreeman.net/share/talks/spark5/\">slides</a>, <a href=\"http://www.youtube.com/watch?v=7mmcEl_1CPw\">video</a>). <span class=\"speaker-info\">Jeremy Freeman, HHMI Janelia Farm Research Campus</span></li>\n \t<li><span class=\"talk-title\">Beyond Word Count – Productionalizing Spark Streaming</span> (<a href=\"http://spark-summit.org/talk/weald-beyond-word-count-productionalizing-spark-streaming/\">abstract</a>, <a href=\"http://spark-summit.org/wp-content/uploads/2013/10/Productionalizing-Spark-Streaming-Spark-Summit-2013-copy.pdf\">slides</a>, <a href=\"http://www.youtube.com/watch?v=OhpjgaBVUtU\">video</a>). <span class=\"speaker-info\">Ryan Weald, Sharethrough</span></li>\n \t<li><span class=\"talk-title\">Hadoop and Spark Join Forces in Yahoo</span> (<a href=\"http://spark-summit.org/talk/feng-hadoop-and-spark-join-forces-at-yahoo/\">abstract</a>, <a href=\"http://spark-summit.org/wp-content/uploads/2013/10/Feng-Andy-SparkSummit-Keynote.pdf\">slides</a>, <a href=\"http://www.youtube.com/watch?v=GbFtbIepk-s\">video</a>). <span class=\"speaker-info\">Andy Feng, Distinguished Architect, Cloud Services, Yahoo</span></li>\n \t<li><span class=\"talk-title\">Next-Generation Spark Scheduling with Sparrow</span> (<a href=\"http://spark-summit.org/talk/ousterhout-next-generation-spark-scheduling-with-sparrow/\">abstract</a>, <a href=\"http://spark-summit.org/wp-content/uploads/2013/10/Kay_Sparrow_Spark_Summit.pdf\">slides</a>, <a href=\"http://www.youtube.com/watch?v=ayjH_bG-RC0\">video</a>). <span class=\"speaker-info\">Kay Ousterhout, UC Berkeley AMPLab</span></li>\n</ul>\n<h2 id=\"the-upcoming-spark-summit-2014\">The Upcoming Spark Summit 2014</h2>\nThe next Summit will be this Summer 2014, and you can <a href=\"http://spark-summit.org/2014/pre-register\">pre-register now</a>. We look forward to seeing you there!\n<h2 id=\"other-summit-resources\">Other Summit Resources</h2>\n<ul>\n \t<li>Follow the <a href=\"http://twitter.com/spark_summit\">@spark_summit</a> twitter handle</li>\n \t<li>Like the <a href=\"http://facebook.com/ApacheSparkSummit\">Summit Facebook page</a></li>\n \t<li>More <a href=\"https://www.dropbox.com/sc/k6l01hiv4zbhhfg/hhCzeS_U9P\">Summit 2013 photos</a></li>\n \t<li>A look <a href=\"http://strata.oreilly.com/2013/11/behind-the-scenes-of-the-first-spark-summit.html\">behind the scenes look at the first Spark Summit</a></li>\n</ul>\n<h2 id=\"footnotes\">Footnotes</h2>\n<ol>\n \t<li>For more on this, check out <a href=\"/blog/2013/10/27/the-growing-spark-community.html\" name=\"footnote-1\">our recent blog post about the growth of the Spark community</a>.</li>\n</ol></td></tr><tr><td>List(Pat McDonough)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2013-11-22, 2013-11-22, UTC)</td><td>[sidenote]A version of this post appears on the <a href=\"http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/\">Cloudera Blog</a>.[/sidenote]\n\n<hr/>\n\nApache Hadoop has revolutionized big data processing, enabling users to store and process huge amounts of data at very low costs. MapReduce has proven to be an ideal platform to implement complex batch applications as diverse as sifting through system logs, running ETL, computing web indexes, and powering personal recommendation systems. However, its reliance on persistent storage to provide fault tolerance and its one-pass computation model make MapReduce a poor fit for low-latency applications and iterative computations, such as machine learning and graph algorithms.\n\nApache Spark addresses these limitations by generalizing the MapReduce computation model, while dramatically improving performance and ease of use.\n<h2 id=\"fast-and-easy-big-data-processing-with-spark\">Fast and Easy Big Data Processing with Spark</h2>\nAt its core, Spark provides a general programming model that enables developers to write application by composing arbitrary operators, such as mappers, reducers, joins, group-bys, and filters. This composition makes it easy to express a wide array of computations, including iterative machine learning, streaming, complex queries, and batch.\n\nIn addition, Spark keeps track of the data that each of the operators produces, and enables applications to reliably store this data in memory. This is the key to Spark’s performance, as it allows applications to avoid costly disk accesses. As illustrated in the figure below, this feature enables:\n\n<img style=\"float: left; width: 280px; margin: 15px 5px;\" src=\"/wp-content/uploads/2013/11/using-ram.png\" alt=\"Efficiently Leveraging Memory\" />\n<ul>\n \t<li>Low-latency computations by caching the working dataset in memory and then performing computations at memory speeds, and</li>\n \t<li>Efficient iterative algorithm by having subsequent iterations share data through memory, or repeatedly accessing the same dataset</li>\n</ul>\nSpark’s ease-of-use comes from its general programming model, which does not constrain users to structure their applications into a bunch of map and reduce operations. Spark’s parallel programs look very much like sequential programs, which make them easier to develop and reason about. Finally, Spark allows users to easily combine batch, interactive, and streaming jobs in the same application. As a result, a Spark job can be up to 100x faster and requires writing 2-10x less code than an equivalent Hadoop job.\n<h2 id=\"using-spark-for-advanced-data-analysis-and-data-science\">Using Spark for Advanced Data Analysis and Data Science</h2>\n<h3 id=\"interactive-data-analysis\">Interactive Data Analysis</h3>\nOne of Spark’s most useful features is the interactive shell, bringing Spark’s capabilities to the user immediately – no IDE and code compilation required. The shell can be used as the primary tool for exploring data interactively, or as means to test portions of an application you’re developing.\n\nThe screenshot to the right shows a Spark Python shell in which the user loads a file and then counts the number of lines that contain “Holiday”.\n\n<img style=\"display: block; width: 480px; margin: 1em auto;\" src=\"/wp-content/uploads/2013/11/spark-python-shell.png\" alt=\"Spark's Python Shell\" />\n\nAs illustrated in this example, Spark can read and write data from and to HDFS. Thus, as soon as Spark is installed, a Hadoop user can immediately start analyzing HDFS data. Then, by caching a dataset in memory, a user can perform a large variety of complex computations interactively!\n\nSpark also provides a Scala shell, and APIs in Java, Scala, and Python for stand-alone applications.\n<h3 id=\"faster-batch\">Faster Batch</h3>\nSome of the earliest deployments of Spark have focused on how to improve performance in existing MapReduce applications. Remember that MapReduce is actually a generic execution framework and is not exclusive to it’s most well-known implementation in core Hadoop. Spark provides MapReduce as well, and because it can efficiently use memory (while using lineage to recover from failure if necessary), some implementations are simply faster in Spark’s MapReduce as compared to Hadoop’s MapReduce right off the bat, before you even get in to leveraging cache for iterative programs.\n\nThe example below illustrates Spark’s implementation of MapReduce’s most famous example, word count. You can see that Spark supports operator chaining. This becomes very useful when doing a bit of pre- or post-processing on your data, such as filtering data prior to running a complex MapReduce job.\n\n<pre>val file = sc.textFile(\"hdfs://.../pagecounts-*.gz\");\nval counts = file.flatMap(line => line.split(\" \"));\n  .map(word => (word, 1))\n  .reduceByKey(_ + _)\ncounts.saveAsTextFile(\"hdfs://.../word-count\");</pre>\n\nSpark’s batch capabilities have been proven in real-world scenarios. A very large Silicon Valley Internet company did a plain-vanilla port of a single MR job implementing feature extraction in a model training pipeline, and saw a 3x speedup.\n<h3 id=\"iterative-algorithms\">Iterative Algorithms</h3>\n<img style=\"float: right; width: 140px; margin: 0 10px;\" src=\"https://databricks.com/wp-content/uploads/2013/11/logistic-regression-performance.png\" alt=\"Logistic Regression Performance\" />\nSpark allow users and applications to explicitly cache a dataset by calling the cache() operation. This means that your applications can now access data from RAM instead of disk, which can dramatically improve the performance of iterative algorithms that access the same dataset repeatedly. This use case covers an important class of applications, as all machine learning and graph algorithms are iterative in nature.\n\nTwo of the world’s largest Internet companies leverage Spark’s efficient iterative execution to provide content recommendations and ad targeting. Machine-learning algorithms such as logistic regression have run 100x faster than previous Hadoop-based implementations (see the plot to the right), while other algorithms such as collaborative filtering or alternating direction method of multipliers have run over 15x faster.\n\nThe following example uses logistic regression to find the best hyperplane that separates two sets of points in a multi-dimensional feature space. Note the cached dataset “points” is accessed repeatedly from memory, whereas in MapReduce, each iteration will read data from the disk, which incurs a huge overhead.\n\n<pre>val points = sc.textFile(\"...\").map(parsePoint).cache()\nvar w = Vector.random(D) //current separating plane\nfor (i <- 1 to ITERATIONS) {\n  val gradient = points.map(p =>\n    (1 / (1 + exp(-p.y*(w dot p.x))) - 1) * p.y * p.x)\n    .reduce(_ + _)\n    w -= gradient\n}\nprintln(\"Final separating plane: \" + w)</pre>\n\n<h3 id=\"real-time-stream-processing\">Real-Time Stream Processing</h3>\nWith a low-latency data analysis system at your disposal, it’s natural to extend the engine towards processing live data streams. Spark has an API for working with streams, providing exactly-once semantics and full recovery of stateful operators. It also has the distinct advantage of giving you the same Spark APIs to process your streams, including reuse of your regular Spark application code.\n\nThe code snippet below shows a simple job processing a network stream, filtering for words beginning with a hashtag and performing a word count on every 10 seconds of data. Compare this to the previous word-count example and you’ll see how almost the exact same code is used, but this time processing a live data stream.\n\n<pre> val ssc = new StreamingContext(\n  args(0), \"NetworkHashCount\",\n  Seconds(10), System.getenv(\"SPARK_HOME\"),\n  Seq(System.getenv(\"SPARK_EXAMPLES_JAR\")))\n\nval lines = ssc.socketTextStream(\"localhost\", 9999)\nval words = lines.flatMap(_.split(\" \"))\n  .filter(_.startsWith(\"#\"))\nval wordCounts = words.map(x => (x, 1))\n  .reduceByKey(_ + _)\nwordCounts.print()\nssc.start()</pre>\n\nAlthough the Spark Streaming API was released less than a year ago, users have deployed it in production to provide monitoring and alerting against stateful, aggregated data from system logs, achieving very fast processing with only seconds of latency.\n<h3 id=\"faster-decision-making\">Faster Decision-Making</h3>\nMany companies use big data to make or facilitate user’s decisions in the form of recommendation systems, ad targeting, or predictive analytics. One of the key properties of any decision is latency — that is, the time it takes to make the decision from the moment the input data is available. Reducing decision latency can significantly increase their effectiveness, and ultimately increase the company’s return on investment. Since many of these decisions are based on complex computations (such as machine learning and statistical algorithms), Spark is an ideal fit to speed up decisions.\n\nNot surprisingly, Spark has been deployed to improve decision quality as well as to reduce latency. Examples range from ad targeting, to improving the quality of video delivery over the Internet.\n<h3 id=\"unified-pipelines\">Unified Pipelines</h3>\nMany of today’s Big Data deployments go beyond MapReduce by integrating other frameworks for streaming, batch, and interactive computation. Users can dramatically reduce the complexity of their data processing pipelines by replacing several systems with Spark.\n\nFor instance, today, many companies use MapReduce to generate reports and answer historical queries, and deploy a separate system for stream processing to follow key metrics in real-time. This approach requires one to maintain and manage two different systems, as well as develop applications for two different computation models. It would also require one to make sure the results provided by the two stacks are consistent (for example, a count computed by the streaming application and the same count computed by MapReduce).\n\nRecently, users have deployed Spark to implement stream processing as well as batch processing for providing historical reports. This not only simplifies deployment and maintenance, but dramatically simplifies application development. For example, maintaining the consistency of real-time and historical metrics is no longer a problem as they are computed using the same code. A final benefit of the unification is improved performance, as there is no need to move the data between different systems: once in-memory, the data can be shared between the streaming computations and historical (or interactive) queries.\n<h3 id=\"your-turn-go-get-started\">Your Turn: Go Get Started</h3>\nSpark is very easy to get started writing powerful Big Data applications. Your existing Hadoop and/or programming skills will have you productively interacting with your data in minutes. Go get started today:\n<ul>\n \t<li><a href=\"http://spark.incubator.apache.org/downloads.html\">Download Spark</a></li>\n \t<li><a href=\"http://spark.incubator.apache.org/docs/latest/quick-start.html\">Quick Start</a></li>\n \t<li><a href=\"http://spark-summit.org\">Spark Summit (2013 Conference Talks and Training)</a></li>\n</ul></td></tr><tr><td>List(Ion Stoica)</td><td>List(Company Blog, Partners)</td><td>List(2013-10-29, 2013-10-29, UTC)</td><td>Today, Cloudera announced that it will distribute and support Apache Spark. We are very excited about this announcement, and what it brings to the Spark platform and the open source community. So what does this announcement mean for Spark?\n\nFirst, it validates the maturity of the Spark platform. Started as a research project at UC Berkeley in 2009, Spark is the first general purpose cluster computing engine that can run sophisticated computations at memory speeds on Hadoop clusters. Spark started with the goal of providing efficient support for iterative algorithms (such as machine learning) and interactive queries, workloads not well supported by MapReduce. Since then, Spark has grown to support other applications such as streaming, and has gained rapid industry adoption. Today, Spark is used in production by numerous companies, and it counts on an ever growing open source community with over 90 contributors from 25 companies.\n\nSecond, it will make the Spark platform available to a wide range of enterprise customers both in US and internationally. By being distributed in conjunction with <a href=\"http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html\">Cloudera’s CDH</a>, Spark will enjoy the same enterprise-grade support as the other components in Cloudera’s stack. Databricks is fully committed to working with Cloudera to guarantee that its customers will have the best possible support. Furthermore, we are looking forward to this partnership to enable new categories of exciting applications, and address unique usage scenarios.\n\nThird, this partnership underlines and strengthens the integration of Spark into the Hadoop ecosystem. Spark has the ability to read Hadoop files, share data with other Hadoop frameworks, and support existing Hadoop workloads, including Hive queries. This integration is beneficial not only for Spark, but for the Hadoop ecosystem as a whole, as Spark brings new capabilities to the Hadoop ecosystem through its ability to run on top of Hadoop YARN. This could give Hadoop users the opportunity to run jobs up to 100x faster than MapReduce, while writing 2-5x less code. For example, a data scientist could leverage Spark’s simple yet powerful API to rapidly develop machine learning algorithms, and then run them at memory speeds on her Hadoop data.\n\nFinally, we want to reiterate our full commitment to open source. The success Spark has enjoyed thus far has only been possible because of a vibrant open source community, who has contributed a continuous stream of new functionality and bug fixes. We believe this partnership will ignite a new wave of growth of our community and accelerate the development of the Apache Spark platform to support an ever growing number of customers.\n\nWe have no doubt that we are just at the beginning of a journey to give users the tools to solve tomorrow’s big data challenges. The next stop in this journey is the <a href=\"http://spark-summit.org\">Spark Summit</a>. Sponsored by leading big data companies and Spark users, including Databricks and Cloudera, this is the first conference that will bring together the Spark community. Come and join us on this journey!</td></tr><tr><td>List(Matei Zaharia)</td><td>List(Announcements, Company Blog)</td><td>List(2013-10-28, 2013-10-28, UTC)</td><td>This year has seen unprecedented growth in both the user and contributor communities around <a href=\"http://spark.incubator.apache.org\">Apache Spark</a>. This rapid growth validates the tremendous potential of the platform, and shows the great excitement around it.\n\nWhile Spark started as a research project by a few grad students at UC Berkeley in 2009, today <strong>over 90 developers from 25 companies have contributed to Spark</strong>. This is not counting contributors to Shark (Hive on Spark), of which there are 25. Indeed, out of the many new big data engines created in the past few years, <strong>Spark has the largest development community after Hadoop MapReduce</strong>. We believe that new components in the project, like <a href=\"http://spark.incubator.apache.org/docs/latest/streaming-programming-guide.html\">Spark Streaming</a> and <a href=\"http://spark.incubator.apache.org/docs/latest/mllib-guide.html\">MLlib</a>, will only increase this growth.\n<h2>Growth by Numbers</h2>\nTo give a sense of the growth of the project, the graph below shows the number of contributors to each major Spark release in the past year.\n<div style=\"text-align: center;\"><strong>Past Year Spark Releases</strong>\n<img class=\"aligncenter wp-image-82\" src=\"/wp-content/uploads/2013/10/growth-of-spark-graphic.png\" alt=\"growth-of-spark-graphic\" width=\"507\" height=\"300\" /></div>\nThe number of contributors quadrupled between October 2012 and 2013, and each of our releases has been getting bigger in terms of features. In addition, an increasing number of Spark’s major features have been contributed by the user community. These include Hadoop YARN support in Apache Spark 0.8; metrics collection; new machine learning algorithms and examples; fair scheduling; and column-oriented compression in Shark. At Databricks, we plan to continue working with the open source community to expand Apache Spark.\n\nA final indicator of growth is conferences and events. The <a href=\"http://ampcamp.berkeley.edu\">AMP Camp</a> training camp at Berkeley was sold out with members from over 100 companies attending, while our <a href=\"http://www.meetup.com/spark-users/\">San Francisco user meetup</a> has grown to 1,300 members. We’re excited to continue organizing such events.\n<h2>Bringing the Community Together: The First Spark Summit</h2>\n<img style=\"float: right;\" src=\"/wp-content/uploads/2013/10/Summit-Logo-FINALtr-150x150px.png\" alt=\"Summit-Logo-FINALtr-150x150px\" width=\"150\" height=\"150\" />\nTo celebrate the growth around Spark and bring users and contributors together, we’re excited to host the <a href=\"http://spark-summit.org\">first Spark Summit</a>, on December 2nd and 3rd, 2013. Sponsored by leading big data companies and production users of Spark, this will be the first conference around the Spark stack. In addition to talks from users and developers, the Summit and will include a day of <a href=\"http://www.spark-summit.org/agenda/\">hands-on Spark training</a>. We’re looking forward to your continuous involvement to expand Spark and tackle tomorrow’s big data challenges. So whether you’re a big data veteran or new to Spark, come by to learn how to use it to solve your problems.</td></tr><tr><td>List(Ion Stoica, Matei Zaharia)</td><td>List(Announcements, Company Blog)</td><td>List(2013-10-27, 2013-10-27, UTC)</td><td>When we announced that the original team behind <a href=\"http://spark.incubator.apache.org\">Apache Spark</a> is starting a company around the project, we got a lot of excited questions. What areas will the company focus on, and what will it mean for the open source project? Today, in our first blog post at Databricks, we’re happy to share some of our goals, and say a little about what we’re doing next with Spark.\n\nTo start with, our mission at Databricks is simple: we want to build the very best computing platform for extracting value from data. Big data is a tremendous opportunity that is still largely untapped, and we’ve been working for the past six years to transform what can be done with it. Going forward, we are fully committed to building out the open source Apache Spark platform to achieve this goal.\n<h2 id=\"how-we-think-about-big-data-speed-and-sophistication\">How We Think about Big Data: Speed and Sophistication</h2>\nIn the past few years, open source technologies like Hadoop have made it dramatically easier to store large volumes of data. This capability is transforming a wide range of industries, from brick-and-mortar enterprises to the web. Over time though, simply collecting big data will not be enough to maintain a competitive edge. The question will be what can you do with this data.\n\nWe believe that two axes will determine how well an organization can draw value from data: <strong>speed</strong> and <strong>sophistication</strong>. By speed, we mean not only the speed at which we compute and return answers, but also the speed of development: how quickly can users take a new idea from the drawing board to a production application? By sophistication, we mean what type of analysis can be done. Today’s big data systems do not support the sophisticated analysis functions in tools like R and Matlab, limiting their scope. Enabling these types of analyses would greatly increase their value.\n\nThrough the Apache Spark project, we’ve been working to address both axes in a way that works seamlessly with the Hadoop stack. Released in 2010, Spark remains the only widely deployed engine for Hadoop to support in-memory computing and general execution graphs, as well as the easiest way to program applications on Hadoop data, with APIs in Scala, Java and Python. Released shortly after, <a href=\"http://shark.cs.berkeley.edu\">Shark</a> was the first system to speed up Hive by 100x, and is the only one of the new “SQL on Hadoop” engines to retain full Hive compatibility (by building directly on Hive) and to support in-memory computation. Looking forward, libraries like <a href=\"http://spark.incubator.apache.org/docs/latest/mllib-guide.html\">MLlib</a> and <a href=\"http://www.meetup.com/spark-users/events/124935592/\">GraphX</a> are making it easy to call sophisticated machine learning and graph algorithms from Spark, while running them at memory speeds. These tools have already given numerous organizations the ability to do faster and richer data analysis, and we hope to bring them to hundreds more.\n<h2 id=\"what-were-working-on\">What We’re Working On</h2>\nAt Databricks, we’re committed to bringing Spark to an ever-wider set of users and greatly increasing its capabilities. Through both the recent <a href=\"http://spark.incubator.apache.org/releases/spark-release-0-8-0.html\">Apache Spark 0.8 release</a> and our ongoing work, we’ve been building out quite a few new features. Expect to see a focus in the following areas:\n<ul>\n \t<li><strong>Deployment:</strong> We want to make Spark effortless to deploy for any user, whether with or without an existing Hadoop cluster. Apache Spark 0.8 made significant strides in this respect with improved support for Mesos, EC2 and Hadoop YARN.</li>\n \t<li><strong>High availability</strong>: One exciting feature that we’ve already merged into Apache Spark 0.8.1 is <a href=\"https://github.com/apache/incubator-spark/pull/19\">high availability for the master node</a>. In general, due to the many users who are running Spark in availability-critical settings (e.g. streaming or user-facing applications), we want to make availability throughout the stack easier.</li>\n \t<li><strong>New features:</strong> Besides these top-level goals, we have an exciting roadmap of features, such as Scala 2.10 support, new machine learning algorithms, graph computation, and updates to <a href=\"http://spark.incubator.apache.org/docs/latest/streaming-programming-guide.html\">Spark Streaming</a>, coming soon.</li>\n</ul>\nMost importantly, we believe that, despite the effort in the past few years, big data processing is still in its infancy, and there is tremendous room for tools that are faster, easier to use, and capable of richer computation. We hope you join us in defining the next generation of big data systems and unlocking the speed and sophistication that we believe is possible for big data.</td></tr><tr><td>List(Arsalan Tavakoli-Shiraji)</td><td>List(Company Blog, Partners)</td><td>List(2014-04-11, 2014-04-11, UTC)</td><td>Today, MapR announced that it will distribute and support the Apache Spark platform as part of the MapR Distribution for Hadoop in partnership with Databricks. We’re thrilled to start on this journey with MapR for a multitude of reasons.\n\nOne of our primary goals at Databricks is to drive broad adoption of Spark and ensure everybody who uses it has a fantastic experience. This partnership will enable all of MapR’s enterprise customers, existing and new, to leverage Spark with the backing of the same great enterprise support available for the rest of MapR’s Hadoop Distribution. As Tomer mentioned in his <a href=\"/blog/2014/04/10/MapR-Integrates-Spark-Stack.html\">blog post</a>, Spark is one of the most common topics in discussions with MapR’s existing customers and many are even already running it in production!\n\nA core part of Spark’s value proposition is the ability to easily build a unified end-to-end workflow where critical functions are first class citizens that are seamlessly integrated into the platform. An important part of this workflow is the ability to provide SQL-based interactive queries delivered by Shark, which also serves as the gateway for a wealth of traditional and new SQL-based tools to run on top of Spark. At Databricks, we continue to work on innovating across the entire Spark platform, including Shark, and customers now have an enterprise support option for Shark available.\n\nFinally, we see this partnership as continued validation of Spark’s emergence as the leading open source processing engine in the Big Data community. Spark is the most active project in the Hadoop ecosystem in the past year with over 170 contributors, and we’re heartened to see a rapidly growing attendance at community events pointing to entirely new classes of Spark enterprise use cases. The <a href=\"/certification/\">Databricks “Certified on Spark”</a> program has seen incredible interest from application developers who are leveraging Spark to deliver deeper insights, faster to their customers.\n\nAt Databricks we are fully committed to open source, and we’re excited to partner with MapR - a company with strong support for open source Big Data projects - to together help drive continued growth and innovation of Spark. Join us at the upcoming <a href=\"http://spark-summit.org/2014\" target=\"_blank\">Spark Summit</a>, the largest conference focused on Spark, to learn more!</td></tr></tbody></table></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["## Nested Data\n\nThink of nested data as columns within columns. \n\nFor instance, look at the `dates` column."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/kqmfblujy9?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/kqmfblujy9?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT dates FROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dates</th></tr></thead><tbody><tr><td>List(2014-04-10, 2014-04-10, UTC)</td></tr><tr><td>List(2014-04-10, 2014-04-10, UTC)</td></tr><tr><td>List(2014-04-01, 2014-04-01, UTC)</td></tr><tr><td>List(2014-03-27, 2014-03-27, UTC)</td></tr><tr><td>List(2014-02-04, 2014-02-04, UTC)</td></tr><tr><td>List(2014-01-02, 2014-01-02, UTC)</td></tr><tr><td>List(2014-03-26, 2014-03-26, UTC)</td></tr><tr><td>List(2014-03-21, 2014-03-21, UTC)</td></tr><tr><td>List(2014-03-19, 2014-03-19, UTC)</td></tr><tr><td>List(2014-03-03, 2014-03-03, UTC)</td></tr></tbody></table></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["Pull out a specific subfield with \"dot\" notation."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT dates.createdOn, dates.publishedOn, dates.tz\nFROM DatabricksBlog\nlimit 20"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>createdOn</th><th>publishedOn</th><th>tz</th></tr></thead><tbody><tr><td>2014-04-10</td><td>2014-04-10</td><td>UTC</td></tr><tr><td>2014-04-10</td><td>2014-04-10</td><td>UTC</td></tr><tr><td>2014-04-01</td><td>2014-04-01</td><td>UTC</td></tr><tr><td>2014-03-27</td><td>2014-03-27</td><td>UTC</td></tr><tr><td>2014-02-04</td><td>2014-02-04</td><td>UTC</td></tr><tr><td>2014-01-02</td><td>2014-01-02</td><td>UTC</td></tr><tr><td>2014-03-26</td><td>2014-03-26</td><td>UTC</td></tr><tr><td>2014-03-21</td><td>2014-03-21</td><td>UTC</td></tr><tr><td>2014-03-19</td><td>2014-03-19</td><td>UTC</td></tr><tr><td>2014-03-03</td><td>2014-03-03</td><td>UTC</td></tr><tr><td>2014-02-13</td><td>2014-02-13</td><td>UTC</td></tr><tr><td>2014-02-11</td><td>2014-02-11</td><td>UTC</td></tr><tr><td>2014-01-22</td><td>2014-01-22</td><td>UTC</td></tr><tr><td>2013-12-20</td><td>2013-12-20</td><td>UTC</td></tr><tr><td>2013-12-19</td><td>2013-12-19</td><td>UTC</td></tr><tr><td>2013-11-22</td><td>2013-11-22</td><td>UTC</td></tr><tr><td>2013-10-29</td><td>2013-10-29</td><td>UTC</td></tr><tr><td>2013-10-28</td><td>2013-10-28</td><td>UTC</td></tr><tr><td>2013-10-27</td><td>2013-10-27</td><td>UTC</td></tr><tr><td>2014-04-11</td><td>2014-04-11</td><td>UTC</td></tr></tbody></table></div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["Both `createdOn` and `publishedOn` are stored as strings.\n\nCast those values to SQL timestamps:\n\nIn this case, use a single `SELECT` statement to:\n0. Cast `dates.publishedOn` to a `timestamp` data type.\n0. \"Flatten\" the `dates.publishedOn` column to just `publishedOn`."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT title, \n       cast(dates.publishedOn AS timestamp) AS publishedOn \nFROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>publishedOn</th></tr></thead><tbody><tr><td>MapR Integrates the Complete Apache Spark Stack</td><td>2014-04-10T00:00:00.000+0000</td></tr><tr><td>Apache Spark 0.9.1 Released</td><td>2014-04-10T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Alpine Data Labs</td><td>2014-04-01T00:00:00.000+0000</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>2014-03-27T00:00:00.000+0000</td></tr><tr><td>Apache Spark 0.9.0 Released</td><td>2014-02-04T00:00:00.000+0000</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>2014-01-02T00:00:00.000+0000</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>2014-03-26T00:00:00.000+0000</td></tr><tr><td>Apache Spark: A Delight for Developers</td><td>2014-03-21T00:00:00.000+0000</td></tr><tr><td>Databricks announces \"Certified on Apache Spark\" Program</td><td>2014-03-19T00:00:00.000+0000</td></tr><tr><td>Apache Spark Now a Top-level Apache Project</td><td>2014-03-03T00:00:00.000+0000</td></tr></tbody></table></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["Create the temporary view `DatabricksBlog2` to capture the conversion and flattening of the `publishedOn` column."],"metadata":{}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW DatabricksBlog2 AS\n  SELECT *, \n         cast(dates.publishedOn AS timestamp) AS publishedOn \n  FROM DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["Now that we have this temporary view, we can use `DESCRIBE` to check its schema and confirm the timestamp conversion."],"metadata":{}},{"cell_type":"code","source":["%sql\nDESCRIBE DatabricksBlog2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>authors</td><td>array<string></td><td>null</td></tr><tr><td>categories</td><td>array<string></td><td>null</td></tr><tr><td>content</td><td>string</td><td>null</td></tr><tr><td>creator</td><td>string</td><td>null</td></tr><tr><td>dates</td><td>struct<createdOn:string,publishedOn:string,tz:string></td><td>null</td></tr><tr><td>description</td><td>string</td><td>null</td></tr><tr><td>id</td><td>bigint</td><td>null</td></tr><tr><td>link</td><td>string</td><td>null</td></tr><tr><td>slug</td><td>string</td><td>null</td></tr><tr><td>status</td><td>string</td><td>null</td></tr><tr><td>title</td><td>string</td><td>null</td></tr><tr><td>publishedOn</td><td>timestamp</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["-sandbox\nNow the dates are represented by a `timestamp` data type, query for articles within certain date ranges (such as getting a list of all articles published in 2013), and format the date for presentation purposes.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See the Spark documentation, <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\" target=\"_blank\">built-in functions</a>, for a long list of date-specific functions."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT authors, categories,\nTRANSFORM (categories, category -> LOWER(category)) AS lwr_categories\nFROM DatabricksBlog\nlimit 20"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>authors</th><th>categories</th><th>lwr_categories</th></tr></thead><tbody><tr><td>List(Tomer Shiran (VP of Product Management at MapR))</td><td>List(Company Blog, Partners)</td><td>List(company blog, partners)</td></tr><tr><td>List(Tathagata Das)</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(apache spark, engineering blog, machine learning)</td></tr><tr><td>List(Steven Hillion)</td><td>List(Company Blog, Partners)</td><td>List(company blog, partners)</td></tr><tr><td>List(Michael Armbrust, Reynold Xin)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Ali Ghodsi, Ahir Reddy)</td><td>List(Apache Spark, Ecosystem, Engineering Blog)</td><td>List(apache spark, ecosystem, engineering blog)</td></tr><tr><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td><td>List(Company Blog, Customers)</td><td>List(company blog, customers)</td></tr><tr><td>List(Jai Ranganathan, Matei Zaharia)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Databricks Press Office)</td><td>List(Announcements, Company Blog)</td><td>List(announcements, company blog)</td></tr><tr><td>List(Ion Stoica)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Ahir Reddy, Reynold Xin)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Pat McDonough)</td><td>List(Company Blog, Events)</td><td>List(company blog, events)</td></tr><tr><td>List(Ion Stoica)</td><td>List(Apache Spark, Ecosystem, Engineering Blog)</td><td>List(apache spark, ecosystem, engineering blog)</td></tr><tr><td>List(Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Andy Konwinski)</td><td>List(Company Blog, Customers, Events)</td><td>List(company blog, customers, events)</td></tr><tr><td>List(Pat McDonough)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(apache spark, engineering blog)</td></tr><tr><td>List(Ion Stoica)</td><td>List(Company Blog, Partners)</td><td>List(company blog, partners)</td></tr><tr><td>List(Matei Zaharia)</td><td>List(Announcements, Company Blog)</td><td>List(announcements, company blog)</td></tr><tr><td>List(Ion Stoica, Matei Zaharia)</td><td>List(Announcements, Company Blog)</td><td>List(announcements, company blog)</td></tr><tr><td>List(Arsalan Tavakoli-Shiraji)</td><td>List(Company Blog, Partners)</td><td>List(company blog, partners)</td></tr></tbody></table></div>"]}}],"execution_count":27},{"cell_type":"code","source":["%sql\nSELECT title,\n  FILTER (categories, category -> category = \"Apache Spark\") filtered\nFROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>filtered</th></tr></thead><tbody><tr><td>MapR Integrates the Complete Apache Spark Stack</td><td>List()</td></tr><tr><td>Apache Spark 0.9.1 Released</td><td>List(Apache Spark)</td></tr><tr><td>Application Spotlight: Alpine Data Labs</td><td>List()</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>List(Apache Spark)</td></tr><tr><td>Apache Spark 0.9.0 Released</td><td>List(Apache Spark)</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>List(Apache Spark)</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>List()</td></tr><tr><td>Apache Spark: A Delight for Developers</td><td>List(Apache Spark)</td></tr><tr><td>Databricks announces \"Certified on Apache Spark\" Program</td><td>List()</td></tr><tr><td>Apache Spark Now a Top-level Apache Project</td><td>List(Apache Spark)</td></tr></tbody></table></div>"]}}],"execution_count":28},{"cell_type":"code","source":["%sql\nSELECT title,\n  EXISTS (authors, author -> author = \"Reynold Xin\" \n    OR author = \"Ion Stoica\") selected\nFROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>selected</th></tr></thead><tbody><tr><td>MapR Integrates the Complete Apache Spark Stack</td><td>false</td></tr><tr><td>Apache Spark 0.9.1 Released</td><td>false</td></tr><tr><td>Application Spotlight: Alpine Data Labs</td><td>false</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>true</td></tr><tr><td>Apache Spark 0.9.0 Released</td><td>false</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>false</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>false</td></tr><tr><td>Apache Spark: A Delight for Developers</td><td>false</td></tr><tr><td>Databricks announces \"Certified on Apache Spark\" Program</td><td>false</td></tr><tr><td>Apache Spark Now a Top-level Apache Project</td><td>true</td></tr></tbody></table></div>"]}}],"execution_count":29},{"cell_type":"code","source":["%sql\nSELECT title, \n       date_format(publishedOn, \"MMM dd, yyyy\") AS date, \n       link,\n       year(publishedOn) as year\nFROM DatabricksBlog2\nWHERE year(publishedOn) = 2013\nORDER BY publishedOn\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>date</th><th>link</th><th>year</th></tr></thead><tbody><tr><td>Databricks and the Apache Spark Platform</td><td>Oct 27, 2013</td><td>https://databricks.com/blog/2013/10/27/databricks-and-the-apache-spark-platform.html</td><td>2013</td></tr><tr><td>The Growing Apache Spark Community</td><td>Oct 28, 2013</td><td>https://databricks.com/blog/2013/10/27/the-growing-spark-community.html</td><td>2013</td></tr><tr><td>Databricks and Cloudera Partner to Support Apache Spark</td><td>Oct 29, 2013</td><td>https://databricks.com/blog/2013/10/28/databricks-and-cloudera-partner-to-support-spark.html</td><td>2013</td></tr><tr><td>Putting Apache Spark to Use: Fast In-Memory Computing for Your Big Data Applications</td><td>Nov 22, 2013</td><td>https://databricks.com/blog/2013/11/21/putting-spark-to-use.html</td><td>2013</td></tr><tr><td>Highlights From Spark Summit 2013</td><td>Dec 19, 2013</td><td>https://databricks.com/blog/2013/12/18/spark-summit-2013-follow-up.html</td><td>2013</td></tr><tr><td>Apache Spark 0.8.1 Released</td><td>Dec 20, 2013</td><td>https://databricks.com/blog/2013/12/19/release-0_8_1.html</td><td>2013</td></tr></tbody></table></div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["## Array Data\n\nThe table also contains array columns. \n\nEasily determine the size of each array using the built-in `size(..)` function with array columns."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/w9vj8mjpf7?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/w9vj8mjpf7?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT size(authors), \n       authors \nFROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>size(authors)</th><th>authors</th></tr></thead><tbody><tr><td>1</td><td>List(Tomer Shiran (VP of Product Management at MapR))</td></tr><tr><td>1</td><td>List(Tathagata Das)</td></tr><tr><td>1</td><td>List(Steven Hillion)</td></tr><tr><td>2</td><td>List(Michael Armbrust, Reynold Xin)</td></tr><tr><td>1</td><td>List(Patrick Wendell)</td></tr><tr><td>2</td><td>List(Ali Ghodsi, Ahir Reddy)</td></tr><tr><td>2</td><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td></tr><tr><td>2</td><td>List(Jai Ranganathan, Matei Zaharia)</td></tr><tr><td>1</td><td>List(Databricks Press Office)</td></tr><tr><td>1</td><td>List(Ion Stoica)</td></tr></tbody></table></div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["Pull the first element from the array `authors` using an array subscript operator."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT authors[0] AS primaryAuthor \nFROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>primaryAuthor</th></tr></thead><tbody><tr><td>Tomer Shiran (VP of Product Management at MapR)</td></tr><tr><td>Tathagata Das</td></tr><tr><td>Steven Hillion</td></tr><tr><td>Michael Armbrust</td></tr><tr><td>Patrick Wendell</td></tr><tr><td>Ali Ghodsi</td></tr><tr><td>Russell Cardullo (Data Infrastructure Engineer at Sharethrough)</td></tr><tr><td>Jai Ranganathan</td></tr><tr><td>Databricks Press Office</td></tr><tr><td>Ion Stoica</td></tr></tbody></table></div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["### Explode\n\nThe `explode` function allows you to split an array column into multiple rows, copying all the other columns into each new row. \n\nFor example, you can split the column `authors` into the column `author`, with one author per row."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/h8tv263d04?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/h8tv263d04?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT title, \n       authors, \n       explode(authors) AS author, \n       link \nFROM DatabricksBlog\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>authors</th><th>author</th><th>link</th></tr></thead><tbody><tr><td>MapR Integrates the Complete Apache Spark Stack</td><td>List(Tomer Shiran (VP of Product Management at MapR))</td><td>Tomer Shiran (VP of Product Management at MapR)</td><td>https://databricks.com/blog/2014/04/10/mapr-integrates-spark-stack.html</td></tr><tr><td>Apache Spark 0.9.1 Released</td><td>List(Tathagata Das)</td><td>Tathagata Das</td><td>https://databricks.com/blog/2014/04/09/spark-0_9_1-released.html</td></tr><tr><td>Application Spotlight: Alpine Data Labs</td><td>List(Steven Hillion)</td><td>Steven Hillion</td><td>https://databricks.com/blog/2014/03/31/application-spotlight-alpine.html</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>List(Michael Armbrust, Reynold Xin)</td><td>Michael Armbrust</td><td>https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>List(Michael Armbrust, Reynold Xin)</td><td>Reynold Xin</td><td>https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html</td></tr><tr><td>Apache Spark 0.9.0 Released</td><td>List(Patrick Wendell)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2014/02/03/release-0_9_0.html</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>List(Ali Ghodsi, Ahir Reddy)</td><td>Ali Ghodsi</td><td>https://databricks.com/blog/2014/01/01/simr.html</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>List(Ali Ghodsi, Ahir Reddy)</td><td>Ahir Reddy</td><td>https://databricks.com/blog/2014/01/01/simr.html</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td><td>Russell Cardullo (Data Infrastructure Engineer at Sharethrough)</td><td>https://databricks.com/blog/2014/03/25/sharethrough-and-spark-streaming.html</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td><td>Michael Ruggiero (Data Infrastructure Engineer at Sharethrough)</td><td>https://databricks.com/blog/2014/03/25/sharethrough-and-spark-streaming.html</td></tr></tbody></table></div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["It's more obvious to restrict the output to articles that have multiple authors, and sort by the title."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT title, \n       authors, \n       explode(authors) AS author, \n       link \nFROM DatabricksBlog \nWHERE size(authors) > 1 \nORDER BY title\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>authors</th><th>author</th><th>link</th></tr></thead><tbody><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Matei Zaharia</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Holden Karau</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Andy Konwinski</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>AMPLab updates the Big Data Benchmark</td><td>List(Ahir Reddy, Reynold Xin)</td><td>Ahir Reddy</td><td>https://databricks.com/blog/2014/02/12/big-data-benchmark.html</td></tr><tr><td>AMPLab updates the Big Data Benchmark</td><td>List(Ahir Reddy, Reynold Xin)</td><td>Reynold Xin</td><td>https://databricks.com/blog/2014/02/12/big-data-benchmark.html</td></tr><tr><td>Announcing Apache Spark Packages</td><td>List(Xiangrui Meng, Patrick Wendell)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2014/12/22/announcing-spark-packages.html</td></tr><tr><td>Announcing Apache Spark Packages</td><td>List(Xiangrui Meng, Patrick Wendell)</td><td>Xiangrui Meng</td><td>https://databricks.com/blog/2014/12/22/announcing-spark-packages.html</td></tr><tr><td>Apache Spark 1.1: Bringing Hadoop Input/Output Formats to PySpark</td><td>List(Nick Pentreath (Graphflow), Kan Zhang (IBM))</td><td>Kan Zhang (IBM)</td><td>https://databricks.com/blog/2014/09/17/spark-1-1-bringing-hadoop-inputoutput-formats-to-pyspark.html</td></tr><tr><td>Apache Spark 1.1: Bringing Hadoop Input/Output Formats to PySpark</td><td>List(Nick Pentreath (Graphflow), Kan Zhang (IBM))</td><td>Nick Pentreath (Graphflow)</td><td>https://databricks.com/blog/2014/09/17/spark-1-1-bringing-hadoop-inputoutput-formats-to-pyspark.html</td></tr></tbody></table></div>"]}}],"execution_count":40},{"cell_type":"markdown","source":["### Lateral View\nThe data has multiple columns with nested objects.  In this case, the data has multiple dates, authors, and categories.\n\nTake a look at the blog entry **Apache Spark 1.1: The State of Spark Streaming**:"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT dates.publishedOn, title, authors, categories\nFROM DatabricksBlog\nWHERE title = \"Apache Spark 1.1: The State of Spark Streaming\"\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>publishedOn</th><th>title</th><th>authors</th><th>categories</th></tr></thead><tbody><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>List(Arsalan Tavakoli-Shiraji, Tathagata Das, Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog, Streaming)</td></tr></tbody></table></div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["Next, use `LATERAL VIEW` to explode multiple columns at once, in this case, the columns `authors` and `categories`."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT dates.publishedOn, title, author, category\nFROM DatabricksBlog\nLATERAL VIEW explode(authors) exploded_authors_view AS author\nLATERAL VIEW explode(categories) exploded_categories AS category\nWHERE title = \"Apache Spark 1.1: The State of Spark Streaming\"\nORDER BY author, category\nlimit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>publishedOn</th><th>title</th><th>author</th><th>category</th></tr></thead><tbody><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Arsalan Tavakoli-Shiraji</td><td>Apache Spark</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Arsalan Tavakoli-Shiraji</td><td>Engineering Blog</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Arsalan Tavakoli-Shiraji</td><td>Streaming</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Patrick Wendell</td><td>Apache Spark</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Patrick Wendell</td><td>Engineering Blog</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Patrick Wendell</td><td>Streaming</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Tathagata Das</td><td>Apache Spark</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Tathagata Das</td><td>Engineering Blog</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Tathagata Das</td><td>Streaming</td></tr></tbody></table></div>"]}}],"execution_count":44},{"cell_type":"markdown","source":["## Exercise 1\n\nIdentify all the articles written or co-written by Michael Armbrust."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 1\n\nStarting with the table `DatabricksBlog`, create a temporary view called `ArticlesByMichael` where:\n0. Michael Armbrust is the author\n0. The data set contains the column `title` (it may contain others)\n0. It contains only one record per article\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** See the Spark documentation, <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\" target=\"_blank\">built-in functions</a>.  \n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** Include the column `authors` in your view, to help you debug your solution."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nresultsDF = spark.sql(\"select title from ArticlesByMichael order by title\")\ndbTest(\"SQL-L5-articlesByMichael-count\", 3, resultsDF.count())\n\nresults = [r[0] for r in resultsDF.collect()]\ndbTest(\"SQL-L5-articlesByMichael-0\", \"Exciting Performance Improvements on the Horizon for Spark SQL\", results[0])\ndbTest(\"SQL-L5-articlesByMichael-1\", \"Spark SQL Data Sources API: Unified Data Access for the Apache Spark Platform\", results[1])\ndbTest(\"SQL-L5-articlesByMichael-2\", \"Spark SQL: Manipulating Structured Data Using Apache Spark\", results[2])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["### Step 2\nShow the list of Michael Armbrust's articles."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["## Exercise 2\n\nIdentify the complete set of categories used in the Databricks blog articles."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1\n\nStarting with the table `DatabricksBlog`, create another view called `UniqueCategories` where:\n0. The data set contains the one column `category` (and no others)\n0. This list of categories should be unique"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nresultsCount = spark.sql(\"SELECT category FROM UniqueCategories order by category\")\n\ndbTest(\"SQL-L5-uniqueCategories-count\", 12, resultsCount.count())\n\nresults = [r[0] for r in resultsCount.collect()]\ndbTest(\"SQL-L5-uniqueCategories-0\", \"Announcements\", results[0])\ndbTest(\"SQL-L5-uniqueCategories-1\", \"Apache Spark\", results[1])\ndbTest(\"SQL-L5-uniqueCategories-2\", \"Company Blog\", results[2])\n\ndbTest(\"SQL-L5-uniqueCategories-9\", \"Platform\", results[9])\ndbTest(\"SQL-L5-uniqueCategories-10\", \"Product\", results[10])\ndbTest(\"SQL-L5-uniqueCategories-11\", \"Streaming\", results[11])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["### Step 2\nShow the complete list of categories."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["## Exercise 3\n\nCount how many times each category is referenced in the Databricks blog."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 1\n\nStarting with the table `DatabricksBlog`, create a temporary view called `TotalArticlesByCategory` where:\n0. The new table contains two columns, `category` and `total`\n0. The `category` column is a single, distinct category (similar to the last exercise)\n0. The `total` column is the total number of articles in that category\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** You need either multiple views or a `LATERAL VIEW` to solve this.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Because articles can be tagged with multiple categories, the sum of the totals adds up to more than the total number of articles."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nresultsDF = spark.sql(\"SELECT category, total FROM TotalArticlesByCategory ORDER BY category\")\ndbTest(\"SQL-L5-articlesByCategory-count\", 12, resultsDF.count())\n\nresults = [ (r[0]+\" w/\"+str(r[1])) for r in resultsDF.collect()]\n\ndbTest(\"SQL-L5-articlesByCategory-0\", \"Announcements w/72\", results[0])\ndbTest(\"SQL-L5-articlesByCategory-1\", \"Apache Spark w/132\", results[1])\ndbTest(\"SQL-L5-articlesByCategory-2\", \"Company Blog w/224\", results[2])\n\ndbTest(\"SQL-L5-articlesByCategory-9\", \"Platform w/4\", results[9])\ndbTest(\"SQL-L5-articlesByCategory-10\", \"Product w/83\", results[10])\ndbTest(\"SQL-L5-articlesByCategory-11\", \"Streaming w/21\", results[11])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["### Step 2\nDisplay the totals of each category, order by `category`."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["## Summary\n\n* Spark SQL allows you to query and manipulate structured and semi-structured data\n* Spark SQL's built-in functions provide powerful primitives for querying complex schemas"],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n**Q:** What is the syntax for accessing nested columns?  \n**A:** Use the dot notation: ```SELECT dates.publishedOn```\n\n**Q:** What is the syntax for accessing the first element in an array?  \n**A:** Use the [subscript] notation:  ```SELECT authors[0]```\n\n**Q:** What is the syntax for expanding an array into multiple rows?  \n**A:** Use the explode keyword, either:  \n```SELECT explode(authors) as Author``` or  \n```LATERAL VIEW explode(authors) exploded_authors_view AS author```"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Querying Data Lakes with SQL]($./SSQL 06 - Data Lakes)."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"https://docs.databricks.com/spark/latest/spark-sql/index.html\" target=\"_blank\">Spark SQL Reference</a>\n* <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\" target=\"_blank\">Spark SQL, DataFrames and Datasets Guide</a>\n* <a href=\"https://stackoverflow.com/questions/36876959/sparksql-can-i-explode-two-different-variables-in-the-same-query\" target=\"_blank\">SparkSQL: Can I explode two different variables in the same query? (StackOverflow)</a>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"SSQL 05 - Querying JSON","notebookId":3953685866790579},"nbformat":4,"nbformat_minor":0}
