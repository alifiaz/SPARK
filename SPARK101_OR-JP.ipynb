{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oâ€™Reilly - JOSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---+------+\n",
      "| id|letter|\n",
      "+---+------+\n",
      "|  0|     a|\n",
      "|  1|     b|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"create\").getOrCreate()\n",
    "# Create dataframe from tuples\n",
    "df = spark.createDataFrame([(0,'a'),(1,'b')],['id','letter'])\n",
    "print(type(df))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "| age|   name|weight|\n",
      "+----+-------+------+\n",
      "|null|Michael|   100|\n",
      "|  30|   Andy|    40|\n",
      "|  19| Justin|  null|\n",
      "|  16|    Ali|    50|\n",
      "|  40| Rocker|    60|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('people.json')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf = spark.read.csv('Name_Age.csv',inferSchema=True,header=True)\n",
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|  John| 43|\n",
      "| Cindy| 25|\n",
      "|Lauren| 32|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "| age|   name|weight|\n",
      "+----+-------+------+\n",
      "|null|Michael|   100|\n",
      "|  30|   Andy|    40|\n",
      "|  19| Justin|  null|\n",
      "|  16|    Ali|    50|\n",
      "|  40| Rocker|    60|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.csv(\"new_data_output.csv\",header=True)\n",
    "spark.read.csv(\"new_data_output.csv\",header=True,inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical Properties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- weight: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(df))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name', 'weight']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------+------------------+\n",
      "|summary|               age|  name|            weight|\n",
      "+-------+------------------+------+------------------+\n",
      "|  count|                 4|     5|                 4|\n",
      "|   mean|             26.25|  null|              62.5|\n",
      "| stddev|10.965856099730654|  null|26.299556396765833|\n",
      "|    min|                16|   Ali|                40|\n",
      "|    max|                40|Rocker|               100|\n",
      "+-------+------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting Columns and Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "|  16|\n",
      "|  40|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select('age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael', weight=100),\n",
       " Row(age=30, name='Andy', weight=40)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  16|    Ali|\n",
      "|  40| Rocker|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['age','name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating and Renaming Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+------+\n",
      "| age|   name|weight|newage|\n",
      "+----+-------+------+------+\n",
      "|null|Michael|   100|  null|\n",
      "|  30|   Andy|    40|    30|\n",
      "|  19| Justin|  null|    19|\n",
      "|  16|    Ali|    50|    16|\n",
      "|  40| Rocker|    60|    40|\n",
      "+----+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('newage',df['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+\n",
      "|supernewage|   name|weight|\n",
      "+-----------+-------+------+\n",
      "|       null|Michael|   100|\n",
      "|         30|   Andy|    40|\n",
      "|         19| Justin|  null|\n",
      "|         16|    Ali|    50|\n",
      "|         40| Rocker|    60|\n",
      "+-----------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('age','supernewage').show()\n",
    "# Not assigining operations to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+---------+\n",
      "| age|   name|weight|doubleage|\n",
      "+----+-------+------+---------+\n",
      "|null|Michael|   100|     null|\n",
      "|  30|   Andy|    40|       60|\n",
      "|  19| Justin|  null|       38|\n",
      "|  16|    Ali|    50|       32|\n",
      "|  40| Rocker|    60|       80|\n",
      "+----+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('doubleage',df['age']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+\n",
      "| age|   name|weight|nweight|\n",
      "+----+-------+------+-------+\n",
      "|null|Michael|   100|    101|\n",
      "|  30|   Andy|    40|     41|\n",
      "|  19| Justin|  null|   null|\n",
      "|  16|    Ali|    50|     51|\n",
      "|  40| Rocker|    60|     61|\n",
      "+----+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf=df.withColumn('nweight',df['weight']+1)\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using SQL with DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrame as a temporary view\n",
    "df.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string, weight: bigint]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_results=spark.sql(\"select * from people\")\n",
    "sql_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "| age|   name|weight|\n",
      "+----+-------+------+\n",
      "|null|Michael|   100|\n",
      "|  30|   Andy|    40|\n",
      "|  19| Justin|  null|\n",
      "|  16|    Ali|    50|\n",
      "|  40| Rocker|    60|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "|age|name|weight|\n",
      "+---+----+------+\n",
      "| 30|Andy|    40|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from people where age=30').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|age|  name|weight|\n",
      "+---+------+------+\n",
      "| 30|  Andy|    40|\n",
      "| 40|Rocker|    60|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from people where age>20').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('citigroup_stock_2007to2010.csv', inferSchema=True, header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+---------+-------+------------------+\n",
      "|      Date|              Open|     High|      Low|    Close| Volume|         Adj Close|\n",
      "+----------+------------------+---------+---------+---------+-------+------------------+\n",
      "|2007-03-02|50.920002000000004|51.209998|49.850001|49.970002|2617300|        447.823116|\n",
      "|2007-03-05|         49.600001|50.550001|    49.18|49.250001|2491500|441.37058099999996|\n",
      "|2007-03-13|         50.010001|    50.09|48.750001|48.750001|2928600|        436.889662|\n",
      "|2007-03-14|         48.800001|49.320002|48.050001|    49.08|3044100|        439.847058|\n",
      "|2007-03-16|         50.010001|    50.38|    49.19|    49.53|3081400|443.87988499999994|\n",
      "+----------+------------------+---------+---------+---------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Close<50').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    Close|\n",
      "+---------+\n",
      "|49.970002|\n",
      "|49.250001|\n",
      "|48.750001|\n",
      "|    49.08|\n",
      "|    49.53|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Close<50').select('Close').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    Close|\n",
      "+---------+\n",
      "|49.970002|\n",
      "|49.250001|\n",
      "|48.750001|\n",
      "|    49.08|\n",
      "|    49.53|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Close']<50).select('Close').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+-------+------------------+\n",
      "|      Date|     Open|     High|      Low|    Close| Volume|         Adj Close|\n",
      "+----------+---------+---------+---------+---------+-------+------------------+\n",
      "|2007-03-13|50.010001|    50.09|48.750001|48.750001|2928600|        436.889662|\n",
      "|2007-07-26|    48.49|    48.68|46.500001|47.810001|7731600|432.76932300000004|\n",
      "|2007-07-27|47.710001|    48.19|46.710001|46.970002|5591700|425.16576799999996|\n",
      "|2007-07-30|47.320002|47.650001|46.700001|    47.19|4024800|427.15716100000003|\n",
      "|2007-07-31|47.570002|48.260001|46.500001|46.570002|4678800|        421.545025|\n",
      "+----------+---------+---------+---------+---------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Close']<49) & (df['open']>45)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Date='2007-01-03', Open=55.659998, High=56.28, Low=54.720002, Close=55.250001, Volume=2282100, Adj Close=490.291686)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=df.filter(df['Low']>25).collect()\n",
    "print(type(result))\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2007-01-03'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to Date and Timestamps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('dates').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+------------------+------------------+-------+------------------+\n",
      "|      Date|              Open|     High|               Low|             Close| Volume|         Adj Close|\n",
      "+----------+------------------+---------+------------------+------------------+-------+------------------+\n",
      "|2007-01-03|         55.659998|    56.28|         54.720002|         55.250001|2282100|        490.291686|\n",
      "|2007-01-04|         55.250001|56.150001|         54.720002|         55.059998|1658600|488.60559299999994|\n",
      "|2007-01-05|         55.000001|55.050001|         54.459998|54.770002000000005|1317800|        486.032149|\n",
      "|2007-01-08|         54.600001|55.150001|         54.300001|         55.050001|1236900|488.51687400000003|\n",
      "|2007-01-09|55.009997999999996|55.150001|54.190003000000004|         54.570002|1963000|        484.257338|\n",
      "+----------+------------------+---------+------------------+------------------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('citigroup_stock_2007to2010.csv', header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='2007-01-03', Open=55.659998, High=56.28, Low=54.720002, Close=55.250001, Volume=2282100, Adj Close=490.291686)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|      Date|dayofmonth(Date)|\n",
      "+----------+----------------+\n",
      "|2007-01-03|               3|\n",
      "|2007-01-04|               4|\n",
      "|2007-01-05|               5|\n",
      "|2007-01-08|               8|\n",
      "|2007-01-09|               9|\n",
      "+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "+----------+---------------+\n",
      "|      Date|dayofyear(Date)|\n",
      "+----------+---------------+\n",
      "|2007-01-03|              3|\n",
      "|2007-01-04|              4|\n",
      "|2007-01-05|              5|\n",
      "|2007-01-08|              8|\n",
      "|2007-01-09|              9|\n",
      "+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofmonth, dayofyear, weekofyear, month, year\n",
    "print(df.select('Date',dayofmonth(df['Date'])).show(5))\n",
    "print(df.select('Date',dayofyear(df['Date'])).show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Date|year(Date)|\n",
      "+----------+----------+\n",
      "|2007-01-03|      2007|\n",
      "|2007-01-04|      2007|\n",
      "|2007-01-05|      2007|\n",
      "|2007-01-08|      2007|\n",
      "|2007-01-09|      2007|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+\n",
      "|month(Date)|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('Date',year(df['Date'])).show(5),df.select(month(df['Date'])).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working With Timestamps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, minute, year, format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Date|hour(Date)|\n",
      "+----------+----------+\n",
      "|2007-01-03|         0|\n",
      "|2007-01-04|         0|\n",
      "|2007-01-05|         0|\n",
      "|2007-01-08|         0|\n",
      "|2007-01-09|         0|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+\n",
      "|minute(Date)|\n",
      "+------------+\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('Date',hour(df['Date'])).show(5), df.select(minute(df['Date'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+------------------+------------------+-------+------------------+----+\n",
      "|      Date|              Open|     High|               Low|             Close| Volume|         Adj Close|Year|\n",
      "+----------+------------------+---------+------------------+------------------+-------+------------------+----+\n",
      "|2007-01-03|         55.659998|    56.28|         54.720002|         55.250001|2282100|        490.291686|2007|\n",
      "|2007-01-04|         55.250001|56.150001|         54.720002|         55.059998|1658600|488.60559299999994|2007|\n",
      "|2007-01-05|         55.000001|55.050001|         54.459998|54.770002000000005|1317800|        486.032149|2007|\n",
      "|2007-01-08|         54.600001|55.150001|         54.300001|         55.050001|1236900|488.51687400000003|2007|\n",
      "|2007-01-09|55.009997999999996|55.150001|54.190003000000004|         54.570002|1963000|        484.257338|2007|\n",
      "+----------+------------------+---------+------------------+------------------+-------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AVerage Closing prices per Year\n",
    "newdf=df.withColumn('Year', year(df['Date']))\n",
    "newdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|avg(Year)|        avg(Close)|\n",
      "+---------+------------------+\n",
      "|   2007.0|47.782032505976076|\n",
      "|   2008.0| 19.04865647826085|\n",
      "|   2009.0|3.6571825396825375|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.groupBy('Year').mean()[['avg(Year)','avg(Close)']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|  Year|Mean Close|\n",
      "+------+----------+\n",
      "|2007.0|     47.78|\n",
      "|2008.0|     19.05|\n",
      "|2009.0|      3.66|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same Calcualations nicely formatted\n",
    "result=newdf.groupBy('Year').mean()[['avg(Year)','avg(Close)']]\n",
    "result=result.withColumnRenamed('avg(Year)','Year')\n",
    "result=result.select('Year', format_number('avg(Close)',2).alias('Mean Close'))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to Aggregate and GroupBy Concepts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n",
      "None\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "+-------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('sales.csv',inferSchema=True,header=True)\n",
    "print(df.printSchema())\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x148c875c5c8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy('Company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   GOOG|            220.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "|     FB|            610.0|\n",
      "|   APPL|            370.0|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   GOOG|            220.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "|     FB|            610.0|\n",
      "|   APPL|            370.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy('Company').mean().show(),df.groupBy('Company').mean()[['Company','avg(Sales)']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|sum(Sales)|\n",
      "+-------+----------+\n",
      "|   GOOG|     660.0|\n",
      "|   MSFT|     967.0|\n",
      "|     FB|    1220.0|\n",
      "|   APPL|    1480.0|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+-----+\n",
      "|Company|count|\n",
      "+-------+-----+\n",
      "|   GOOG|    3|\n",
      "|   MSFT|    3|\n",
      "|     FB|    2|\n",
      "|   APPL|    4|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_company=df.groupBy('Company')\n",
    "by_company.sum().show(),by_company.count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark Built in Aggregate Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev,format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "+-------+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT Sales)|\n",
      "+---------------------+\n",
      "|                   11|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Company)|\n",
      "+-----------------------+\n",
      "|                      4|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct('Company')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    Average Sales|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg('Sales').alias('Average Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         Sales Std|\n",
      "+------------------+\n",
      "|250.08742410799007|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_std = df.select(stddev(\"Sales\").alias('Sales Std'))\n",
    "sales_std.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Sales Std|\n",
      "+---------+\n",
      "|   250.09|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_std.select(format_number('Sales Std',2).alias('Sales Std')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting and Ordering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('Sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df['Sales'].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   APPL|  Chris|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('Company').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to Missing_Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"MissingData.csv\",header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(thresh=2).show()\n",
    "#Have Atleast two non-null values to be considered for drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop(how='any').show(),df.na.drop(how='all').show()\n",
    "# any: drop rows containing any Null values\n",
    "# all: drop rows if all values are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+\n",
      "|  Id|     Name|Sales|\n",
      "+----+---------+-----+\n",
      "|emp1|     John| null|\n",
      "|emp2|NEW VALUE| null|\n",
      "|emp3|NEW VALUE|345.0|\n",
      "|emp4|    Cindy|456.0|\n",
      "+----+---------+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.fill('NEW VALUE').show(),df.na.fill(0).show()\n",
    "# New values gets filled where data type is string\n",
    "#  0 gets filled only for numeric nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No Name| null|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('No Name', subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(Sales)=400.5)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "mean_val = df.select(mean(df['Sales'])).collect()\n",
    "mean_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.5"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_sales = mean_val[0][0]\n",
    "mean_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(mean_sales,['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"tesla\").getOrCreate()\n",
    "df = spark.read.csv('tesla_stock_2013_2017.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
      "Row(Date='2013-01-02', Open=35.0, High=35.450001, Low=34.709998999999996, Close=35.360001000000004, Volume=1194800, Adj Close=35.360001000000004)\n",
      "\n",
      "\n",
      "Row(Date='2013-01-03', Open=35.18, High=35.450001, Low=34.75, Close=34.77, Volume=742000, Adj Close=34.77)\n",
      "\n",
      "\n",
      "Row(Date='2013-01-04', Open=34.799999, High=34.799999, Low=33.919998, Close=34.400002, Volume=674000, Adj Close=34.400002)\n",
      "\n",
      "\n",
      "Row(Date='2013-01-07', Open=34.799999, High=34.799999, Low=33.900002, Close=34.34, Volume=442000, Adj Close=34.34)\n",
      "\n",
      "\n",
      "Row(Date='2013-01-08', Open=34.5, High=34.5, Low=33.110001000000004, Close=33.68, Volume=1284000, Adj Close=33.68)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What are the column names?\n",
    "print(df.columns)\n",
    "\n",
    "#Print out the first 5 rows.\n",
    "for row in df.head(5):\n",
    "    print(row)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+------------------+------------------+-----------------+-----------------+-----------------+\n",
      "|summary|      Date|             Open|              High|               Low|            Close|           Volume|        Adj Close|\n",
      "+-------+----------+-----------------+------------------+------------------+-----------------+-----------------+-----------------+\n",
      "|  count|      1008|             1008|              1008|              1008|             1008|             1008|             1008|\n",
      "|   mean|      null| 191.930555563492|195.27951362400796|188.38569448115112|191.8851191478172|6062732.837301588|191.8851191478172|\n",
      "| stddev|      null|61.51794180545916| 62.12424917982305| 60.76776038944237|61.45363572561643|4545340.194292408|61.45363572561643|\n",
      "|    min|2013-01-02|        33.080002|         33.380001|32.110001000000004|            32.91|           440200|            32.91|\n",
      "|    max|2016-12-30|       287.670013|        291.420013|        280.399994|       286.040009|         37163900|       286.040009|\n",
      "+-------+----------+-----------------+------------------+------------------+-----------------+-----------------+-----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Use describe() to learn about the DataFrame.\n",
    "print(df.describe().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='2014-09-04', Open=284.01001, High=291.420013, Low=280.399994, Close=286.040009, Volume=8341700, Adj Close=286.040009)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What day had the Peak High in Price?\n",
    "df.orderBy(df[\"High\"].desc()).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014-09-04'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.orderBy(df[\"High\"].desc()).head(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|191.8851191478172|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What is the mean of the Close column?\n",
    "from pyspark.sql.functions import mean\n",
    "df.select(mean(\"Close\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|max(Volume)|min(Volume)|\n",
      "+-----------+-----------+\n",
      "|   37163900|     440200|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What is the max and min of the Volume column?\n",
    "from pyspark.sql.functions import max,min\n",
    "df.select(max(\"Volume\"),min(\"Volume\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many days was the Close lower than 60 dollars? (Use Spark SQL notation)\n",
    "df.filter(\"Close < 60\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+------------------+------------------+-------+------------------+----+\n",
      "|      Date|     Open|     High|               Low|             Close| Volume|         Adj Close|Year|\n",
      "+----------+---------+---------+------------------+------------------+-------+------------------+----+\n",
      "|2013-01-02|     35.0|35.450001|34.709998999999996|35.360001000000004|1194800|35.360001000000004|2013|\n",
      "|2013-01-03|    35.18|35.450001|             34.75|             34.77| 742000|             34.77|2013|\n",
      "|2013-01-04|34.799999|34.799999|         33.919998|         34.400002| 674000|         34.400002|2013|\n",
      "|2013-01-07|34.799999|34.799999|         33.900002|             34.34| 442000|             34.34|2013|\n",
      "|2013-01-08|     34.5|     34.5|33.110001000000004|             33.68|1284000|             33.68|2013|\n",
      "+----------+---------+---------+------------------+------------------+-------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What was the max High for every year? ( Use GroupBy)\n",
    "\n",
    "from pyspark.sql.functions import year\n",
    "yeardf = df.withColumn(\"Year\",year(df[\"Date\"]))\n",
    "yeardf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+----------+------------------+----------+-----------+--------------+---------+\n",
      "|Year|         max(Open)| max(High)|          max(Low)|max(Close)|max(Volume)|max(Adj Close)|max(Year)|\n",
      "+----+------------------+----------+------------------+----------+-----------+--------------+---------+\n",
      "|2013|193.96000700000002|     194.5|        188.369995|193.369995|   37163900|    193.369995|     2013|\n",
      "|2014|        287.670013|291.420013|        280.399994|286.040009|   32681700|    286.040009|     2014|\n",
      "|2015|        280.200012|286.649994|        276.299988| 282.26001|   15649600|     282.26001|     2015|\n",
      "|2016|        266.450012|269.339996|254.50999500000003|265.420013|   23742400|    265.420013|     2016|\n",
      "+----+------------------+----------+------------------+----------+-----------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_df = yeardf.groupBy('Year').max().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Year| max(High)|\n",
      "+----+----------+\n",
      "|2013|     194.5|\n",
      "|2014|291.420013|\n",
      "|2015|286.649994|\n",
      "|2016|269.339996|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_df = yeardf.groupBy('Year').max()\n",
    "max_df.select('Year','max(High)').show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
