{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark by Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dd6t0uFzuR4X"
   },
   "source": [
    "#### Install Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tt7ZS1_wGgjn"
   },
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdOOq4twHN1K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "3ACYMwhgHTYz",
    "outputId": "fadb41ad-0e4e-40b7-b606-0a12d8dcf7af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\t\t       spark-2.3.1-bin-hadoop2.7.tgz.1\n",
      "spark-2.3.1-bin-hadoop2.7      spark-2.3.1-bin-hadoop2.7.tgz.2\n",
      "spark-2.3.1-bin-hadoop2.7.tgz  spark-2.3.1-bin-hadoop2.7.tgz.3\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "wjfF7LLgHZe3",
    "outputId": "d634229f-9344-44ee-a7c6-e2a883a8da65"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-H2CH5CEV:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-preview2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "Gs7fzvxcHfvw",
    "outputId": "cd3bac8f-10c5-4357-f336-0698bda65f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-H2CH5CEV:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-preview2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27cea4d8788>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and preprocessing Chicago's Reported Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Block</th>\n",
       "      <th>IUCR</th>\n",
       "      <th>Primary Type</th>\n",
       "      <th>Description</th>\n",
       "      <th>Location Description</th>\n",
       "      <th>Arrest</th>\n",
       "      <th>Domestic</th>\n",
       "      <th>...</th>\n",
       "      <th>Ward</th>\n",
       "      <th>Community Area</th>\n",
       "      <th>FBI Code</th>\n",
       "      <th>X Coordinate</th>\n",
       "      <th>Y Coordinate</th>\n",
       "      <th>Year</th>\n",
       "      <th>Updated On</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11866088</td>\n",
       "      <td>JC478887</td>\n",
       "      <td>2019-10-18</td>\n",
       "      <td>011XX N SPRINGFIELD AVE</td>\n",
       "      <td>281</td>\n",
       "      <td>CRIMINAL SEXUAL ASSAULT</td>\n",
       "      <td>NON-AGGRAVATED</td>\n",
       "      <td>RESIDENCE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>1150179</td>\n",
       "      <td>1907222</td>\n",
       "      <td>2019</td>\n",
       "      <td>8/5/2020 15:51</td>\n",
       "      <td>41.90132451</td>\n",
       "      <td>-87.723822</td>\n",
       "      <td>(41.901324508, -87.723822)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12120312</td>\n",
       "      <td>JD314345</td>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>058XX N PAULINA ST</td>\n",
       "      <td>1153</td>\n",
       "      <td>DECEPTIVE PRACTICE</td>\n",
       "      <td>FINANCIAL IDENTITY THEFT OVER $ 300</td>\n",
       "      <td>RESIDENCE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>77</td>\n",
       "      <td>11</td>\n",
       "      <td>1164169</td>\n",
       "      <td>1938740</td>\n",
       "      <td>2019</td>\n",
       "      <td>8/1/2020 15:46</td>\n",
       "      <td>41.98752728</td>\n",
       "      <td>-87.67154152</td>\n",
       "      <td>(41.987527284, -87.671541524)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12118728</td>\n",
       "      <td>JD308178</td>\n",
       "      <td>2019-12-10</td>\n",
       "      <td>005XX E 32ND ST</td>\n",
       "      <td>2826</td>\n",
       "      <td>OTHER OFFENSE</td>\n",
       "      <td>HARASSMENT BY ELECTRONIC MEANS</td>\n",
       "      <td>RESIDENCE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "      <td>1180622</td>\n",
       "      <td>1883679</td>\n",
       "      <td>2019</td>\n",
       "      <td>7/31/2020 15:45</td>\n",
       "      <td>41.83607329</td>\n",
       "      <td>-87.61272873</td>\n",
       "      <td>(41.836073285, -87.612728731)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12118262</td>\n",
       "      <td>JD312068</td>\n",
       "      <td>2019-11-25</td>\n",
       "      <td>008XX N LATROBE AVE</td>\n",
       "      <td>1153</td>\n",
       "      <td>DECEPTIVE PRACTICE</td>\n",
       "      <td>FINANCIAL IDENTITY THEFT OVER $ 300</td>\n",
       "      <td>RESIDENCE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>1141201</td>\n",
       "      <td>1905176</td>\n",
       "      <td>2019</td>\n",
       "      <td>7/30/2020 15:44</td>\n",
       "      <td>41.89588035</td>\n",
       "      <td>-87.75684996</td>\n",
       "      <td>(41.895880347, -87.756849962)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Case Number       Date                    Block  IUCR  \\\n",
       "0  11866088    JC478887 2019-10-18  011XX N SPRINGFIELD AVE   281   \n",
       "1  12120312    JD314345 2019-11-28       058XX N PAULINA ST  1153   \n",
       "2  12118728    JD308178 2019-12-10          005XX E 32ND ST  2826   \n",
       "3  12118262    JD312068 2019-11-25      008XX N LATROBE AVE  1153   \n",
       "\n",
       "              Primary Type                          Description  \\\n",
       "0  CRIMINAL SEXUAL ASSAULT                       NON-AGGRAVATED   \n",
       "1       DECEPTIVE PRACTICE  FINANCIAL IDENTITY THEFT OVER $ 300   \n",
       "2            OTHER OFFENSE       HARASSMENT BY ELECTRONIC MEANS   \n",
       "3       DECEPTIVE PRACTICE  FINANCIAL IDENTITY THEFT OVER $ 300   \n",
       "\n",
       "  Location Description Arrest Domestic  ... Ward Community Area FBI Code  \\\n",
       "0            RESIDENCE  FALSE    FALSE  ...   37             23        2   \n",
       "1            RESIDENCE  FALSE    FALSE  ...   40             77       11   \n",
       "2            RESIDENCE  FALSE    FALSE  ...    4             35       26   \n",
       "3            RESIDENCE  FALSE    FALSE  ...   37             25       11   \n",
       "\n",
       "  X Coordinate Y Coordinate  Year       Updated On     Latitude     Longitude  \\\n",
       "0      1150179      1907222  2019   8/5/2020 15:51  41.90132451    -87.723822   \n",
       "1      1164169      1938740  2019   8/1/2020 15:46  41.98752728  -87.67154152   \n",
       "2      1180622      1883679  2019  7/31/2020 15:45  41.83607329  -87.61272873   \n",
       "3      1141201      1905176  2019  7/30/2020 15:44  41.89588035  -87.75684996   \n",
       "\n",
       "                        Location  \n",
       "0     (41.901324508, -87.723822)  \n",
       "1  (41.987527284, -87.671541524)  \n",
       "2  (41.836073285, -87.612728731)  \n",
       "3  (41.895880347, -87.756849962)  \n",
       "\n",
       "[4 rows x 22 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp,col,lit\n",
    "rc = spark.read.csv('reported-crimes.csv',header=True).withColumn('Date',to_timestamp(col('Date'),'MM/dd/yyyy hh:mm')). \\\n",
    "filter(col('Date') <= lit('2020-11-11'))\n",
    "rc.toPandas().head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2390"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62413"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc1 = spark.read.csv('reported-crimes.csv',header=True).filter(col('Primary Type') == 'THEFT')\n",
    "rc1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- Arrest: string (nullable = true)\n",
      " |-- Domestic: string (nullable = true)\n",
      " |-- Beat: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Ward: string (nullable = true)\n",
      " |-- Community Area: string (nullable = true)\n",
      " |-- FBI Code: string (nullable = true)\n",
      " |-- X Coordinate: string (nullable = true)\n",
      " |-- Y Coordinate: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Updated On: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType, BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "          ('ID', StringType()),\n",
    "          ('Case Number', StringType()),\n",
    "          ('Date', TimestampType()),\n",
    "          ('Block', StringType()),\n",
    "          ('IUCR', StringType()),\n",
    "          ('Primary Type', StringType()),\n",
    "          ('Description', StringType()),\n",
    "          ('Location Description', StringType()),\n",
    "          ('Arrest', BooleanType()),\n",
    "          ('Domestic', BooleanType()),\n",
    "          ('Beat', StringType()),\n",
    "          ('District', StringType()),\n",
    "          ('Ward', StringType()),\n",
    "          ('Community Area', StringType()),\n",
    "          ('FBI Code', StringType()),\n",
    "          ('X Coordinate', StringType()),\n",
    "          ('Y Coordinate', StringType()),\n",
    "          ('Year', IntegerType()),\n",
    "          ('Updated On', StringType()),\n",
    "          ('Latitude', DoubleType()),\n",
    "          ('Longitude', DoubleType()),\n",
    "          ('Location', StringType()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(ID,StringType,true),StructField(Case Number,StringType,true),StructField(Date,TimestampType,true),StructField(Block,StringType,true),StructField(IUCR,StringType,true),StructField(Primary Type,StringType,true),StructField(Description,StringType,true),StructField(Location Description,StringType,true),StructField(Arrest,BooleanType,true),StructField(Domestic,BooleanType,true),StructField(Beat,StringType,true),StructField(District,StringType,true),StructField(Ward,StringType,true),StructField(Community Area,StringType,true),StructField(FBI Code,StringType,true),StructField(X Coordinate,StringType,true),StructField(Y Coordinate,StringType,true),StructField(Year,IntegerType,true),StructField(Updated On,StringType,true),StructField(Latitude,DoubleType,true),StructField(Longitude,DoubleType,true),StructField(Location,StringType,true)))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = StructType([StructField(x[0], x[1], True) for x in labels])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- Arrest: boolean (nullable = true)\n",
      " |-- Domestic: boolean (nullable = true)\n",
      " |-- Beat: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Ward: string (nullable = true)\n",
      " |-- Community Area: string (nullable = true)\n",
      " |-- FBI Code: string (nullable = true)\n",
      " |-- X Coordinate: string (nullable = true)\n",
      " |-- Y Coordinate: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Updated On: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc = spark.read.csv('reported-crimes.csv', schema=schema)\n",
    "rc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|IUCR|\n",
      "+----+\n",
      "|IUCR|\n",
      "|2017|\n",
      "|1185|\n",
      "|2014|\n",
      "|2826|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.select('IUCR').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|IUCR|\n",
      "+----+\n",
      "|IUCR|\n",
      "|2017|\n",
      "|1185|\n",
      "|2014|\n",
      "|2826|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.select(rc.IUCR).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|IUCR|\n",
      "+----+\n",
      "|IUCR|\n",
      "|2017|\n",
      "|1185|\n",
      "|2014|\n",
      "|2826|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.select(col('IUCR')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display only the first 4 rows of the column names Case Number, Date and Arrest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------+\n",
      "|Case Number|               Date|Arrest|\n",
      "+-----------+-------------------+------+\n",
      "|   JC478887|2019-10-18 00:00:00| FALSE|\n",
      "|   JD312016|2019-10-10 00:00:00| FALSE|\n",
      "|   JC516490|2019-10-30 00:00:00|  TRUE|\n",
      "|   JC475135|2019-10-16 00:00:00|  TRUE|\n",
      "+-----------+-------------------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.select('Case Number', 'Date', 'Arrest').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------+\n",
      "|Case Number|               Date|Arrest|\n",
      "+-----------+-------------------+------+\n",
      "|   JC478887|2019-10-18 00:00:00| FALSE|\n",
      "|   JD312016|2019-10-10 00:00:00| FALSE|\n",
      "|   JC516490|2019-10-30 00:00:00|  TRUE|\n",
      "|   JC475135|2019-10-16 00:00:00|  TRUE|\n",
      "+-----------+-------------------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.select(col('Case Number'), col('Date'), col('Arrest')).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add a column with name One, with entries all 1s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "rc=rc.withColumn('One', lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------+\n",
      "|One|               Date|Arrest|\n",
      "+---+-------------------+------+\n",
      "|  1|2019-10-18 00:00:00| FALSE|\n",
      "|  1|2019-10-10 00:00:00| FALSE|\n",
      "|  1|2019-10-30 00:00:00|  TRUE|\n",
      "|  1|2019-10-16 00:00:00|  TRUE|\n",
      "+---+-------------------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.select('One', 'Date', 'Arrest').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the column IUCR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Case Number',\n",
       " 'Date',\n",
       " 'Block',\n",
       " 'Primary Type',\n",
       " 'Description',\n",
       " 'Location Description',\n",
       " 'Arrest',\n",
       " 'Domestic',\n",
       " 'Beat',\n",
       " 'District',\n",
       " 'Ward',\n",
       " 'Community Area',\n",
       " 'FBI Code',\n",
       " 'X Coordinate',\n",
       " 'Y Coordinate',\n",
       " 'Year',\n",
       " 'Updated On',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Location',\n",
       " 'One']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.drop('IUCR').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get 12-Nov crimes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "duplicate_crimes = spark.read.csv(\n",
    "    'reported-crimes.csv',header=True).withColumn(\n",
    "    'Date',to_timestamp(col('Date'),'MM/dd/yyyy hh:mm')).filter(col('Date') == lit('2019-11-12'))\n",
    "duplicate_crimes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.filter(col('Date') == lit('2019-11-12')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc = rc.union(duplicate_crimes)\n",
    "rc.filter(col('Date') == lit('2019-11-12')).count() # should be 93*2=186 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the top 10 number of reported crimes by Primary type, in descending order of occurence?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|       Primary Type|count|\n",
      "+-------------------+-----+\n",
      "|              THEFT| 1835|\n",
      "|            BATTERY| 1008|\n",
      "| DECEPTIVE PRACTICE|  801|\n",
      "|            ASSAULT|  548|\n",
      "|      OTHER OFFENSE|  481|\n",
      "|          NARCOTICS|  471|\n",
      "|    CRIMINAL DAMAGE|  416|\n",
      "|           BURGLARY|  247|\n",
      "|  CRIMINAL TRESPASS|  182|\n",
      "|MOTOR VEHICLE THEFT|  168|\n",
      "+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.groupBy('Primary Type').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What percentage of reported crimes resulted in an arrest?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Arrest|\n",
      "+------+\n",
      "| FALSE|\n",
      "|  TRUE|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.select('Arrest').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+\n",
      "|Arrest|count|percent|\n",
      "+------+-----+-------+\n",
      "| FALSE| 5229|  0.797|\n",
      "|  TRUE| 1333|  0.203|\n",
      "+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "count_arrests = rc.groupBy(\"Arrest\").count().withColumn('percent', func.round(col('count')/rc.count(),3))\n",
    "count_arrests.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the top 3 locations for reported crimes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|Location Description|count|\n",
      "+--------------------+-----+\n",
      "|           RESIDENCE| 1226|\n",
      "|              STREET| 1045|\n",
      "|           APARTMENT|  898|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.groupby('Location Description').count().orderBy(\"count\",ascending=False).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column\n",
      "DataFrame\n",
      "DataType\n",
      "PandasUDFType\n",
      "PythonEvalType\n",
      "SparkContext\n",
      "StringType\n",
      "UserDefinedFunction\n",
      "__all__\n",
      "__builtins__\n",
      "__cached__\n",
      "__doc__\n",
      "__file__\n",
      "__loader__\n",
      "__name__\n",
      "__package__\n",
      "__spec__\n",
      "_binary_mathfunctions\n",
      "_collect_list_doc\n",
      "_collect_set_doc\n",
      "_create_binary_mathfunction\n",
      "_create_column_from_literal\n",
      "_create_column_from_name\n",
      "_create_function\n",
      "_create_function_over_column\n",
      "_create_udf\n",
      "_create_window_function\n",
      "_functions\n",
      "_functions_1_4_over_column\n",
      "_functions_1_6_over_column\n",
      "_functions_2_1_over_column\n",
      "_functions_2_4\n",
      "_functions_deprecated\n",
      "_functions_over_column\n",
      "_lit_doc\n",
      "_options_to_str\n",
      "_string_functions\n",
      "_test\n",
      "_to_java_column\n",
      "_to_seq\n",
      "_window_functions\n",
      "_wrap_deprecated_function\n",
      "abs\n",
      "acos\n",
      "add_months\n",
      "approx_count_distinct\n",
      "array\n",
      "array_contains\n",
      "array_distinct\n",
      "array_except\n",
      "array_intersect\n",
      "array_join\n",
      "array_max\n",
      "array_min\n",
      "array_position\n",
      "array_remove\n",
      "array_repeat\n",
      "array_sort\n",
      "array_union\n",
      "arrays_overlap\n",
      "arrays_zip\n",
      "asc\n",
      "asc_nulls_first\n",
      "asc_nulls_last\n",
      "ascii\n",
      "asin\n",
      "atan\n",
      "atan2\n",
      "avg\n",
      "base64\n",
      "basestring\n",
      "bin\n",
      "bitwiseNOT\n",
      "blacklist\n",
      "broadcast\n",
      "bround\n",
      "cbrt\n",
      "ceil\n",
      "coalesce\n",
      "col\n",
      "collect_list\n",
      "collect_set\n",
      "column\n",
      "concat\n",
      "concat_ws\n",
      "conv\n",
      "corr\n",
      "cos\n",
      "cosh\n",
      "count\n",
      "countDistinct\n",
      "covar_pop\n",
      "covar_samp\n",
      "crc32\n",
      "create_map\n",
      "cume_dist\n",
      "current_date\n",
      "current_timestamp\n",
      "date_add\n",
      "date_format\n",
      "date_sub\n",
      "date_trunc\n",
      "datediff\n",
      "dayofmonth\n",
      "dayofweek\n",
      "dayofyear\n",
      "decode\n",
      "degrees\n",
      "dense_rank\n",
      "desc\n",
      "desc_nulls_first\n",
      "desc_nulls_last\n",
      "element_at\n",
      "encode\n",
      "exp\n",
      "explode\n",
      "explode_outer\n",
      "expm1\n",
      "expr\n",
      "factorial\n",
      "first\n",
      "flatten\n",
      "floor\n",
      "format_number\n",
      "format_string\n",
      "from_csv\n",
      "from_json\n",
      "from_unixtime\n",
      "from_utc_timestamp\n",
      "functools\n",
      "get_json_object\n",
      "greatest\n",
      "grouping\n",
      "grouping_id\n",
      "hash\n",
      "hex\n",
      "hour\n",
      "hypot\n",
      "ignore_unicode_prefix\n",
      "initcap\n",
      "input_file_name\n",
      "instr\n",
      "isnan\n",
      "isnull\n",
      "json_tuple\n",
      "kurtosis\n",
      "lag\n",
      "last\n",
      "last_day\n",
      "lead\n",
      "least\n",
      "length\n",
      "levenshtein\n",
      "lit\n",
      "locate\n",
      "log\n",
      "log10\n",
      "log1p\n",
      "log2\n",
      "lower\n",
      "lpad\n",
      "ltrim\n",
      "map_concat\n",
      "map_entries\n",
      "map_from_arrays\n",
      "map_from_entries\n",
      "map_keys\n",
      "map_values\n",
      "max\n",
      "md5\n",
      "mean\n",
      "min\n",
      "minute\n",
      "monotonically_increasing_id\n",
      "month\n",
      "months_between\n",
      "nanvl\n",
      "next_day\n",
      "ntile\n",
      "pandas_udf\n",
      "percent_rank\n",
      "posexplode\n",
      "posexplode_outer\n",
      "pow\n",
      "quarter\n",
      "radians\n",
      "rand\n",
      "randn\n",
      "rank\n",
      "regexp_extract\n",
      "regexp_replace\n",
      "repeat\n",
      "reverse\n",
      "rint\n",
      "round\n",
      "row_number\n",
      "rpad\n",
      "rtrim\n",
      "schema_of_csv\n",
      "schema_of_json\n",
      "second\n",
      "sequence\n",
      "sha1\n",
      "sha2\n",
      "shiftLeft\n",
      "shiftRight\n",
      "shiftRightUnsigned\n",
      "shuffle\n",
      "signum\n",
      "sin\n",
      "since\n",
      "sinh\n",
      "size\n",
      "skewness\n",
      "slice\n",
      "sort_array\n",
      "soundex\n",
      "spark_partition_id\n",
      "split\n",
      "sqrt\n",
      "stddev\n",
      "stddev_pop\n",
      "stddev_samp\n",
      "struct\n",
      "substring\n",
      "substring_index\n",
      "sum\n",
      "sumDistinct\n",
      "sys\n",
      "tan\n",
      "tanh\n",
      "toDegrees\n",
      "toRadians\n",
      "to_csv\n",
      "to_date\n",
      "to_json\n",
      "to_str\n",
      "to_timestamp\n",
      "to_utc_timestamp\n",
      "translate\n",
      "trim\n",
      "trunc\n",
      "udf\n",
      "unbase64\n",
      "unhex\n",
      "unix_timestamp\n",
      "upper\n",
      "var_pop\n",
      "var_samp\n",
      "variance\n",
      "warnings\n",
      "weekofyear\n",
      "when\n",
      "window\n",
      "xxhash64\n",
      "year\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "fn=dir(functions)\n",
    "for i in fn:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Functions\n",
    "**Display the Primary Type column in lower and upper characters, and the first 4 characters of the column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function substring in module pyspark.sql.functions:\n",
      "\n",
      "substring(str, pos, len)\n",
      "    Substring starts at `pos` and is of length `len` when str is String type or\n",
      "    returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "    when str is Binary type.\n",
      "    \n",
      "    .. note:: The position is not zero based, but 1 based index.\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "    [Row(s='ab')]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, upper, substring\n",
    "help(substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------------------------+-----------------------------+\n",
      "|lower(Primary Type)       |upper(Primary Type)       |substring(Primary Type, 1, 4)|\n",
      "+--------------------------+--------------------------+-----------------------------+\n",
      "|criminal sexual assault   |CRIMINAL SEXUAL ASSAULT   |CRIM                         |\n",
      "|deceptive practice        |DECEPTIVE PRACTICE        |DECE                         |\n",
      "|other offense             |OTHER OFFENSE             |OTHE                         |\n",
      "|deceptive practice        |DECEPTIVE PRACTICE        |DECE                         |\n",
      "|burglary                  |BURGLARY                  |BURG                         |\n",
      "|criminal damage           |CRIMINAL DAMAGE           |CRIM                         |\n",
      "|offense involving children|OFFENSE INVOLVING CHILDREN|OFFE                         |\n",
      "+--------------------------+--------------------------+-----------------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.select(lower(col('Primary Type')), upper(col('Primary Type')), substring(col('Primary Type'),1,4)).show(7, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Functions\n",
    "\n",
    "\n",
    "Show the oldest date and the most recent date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|          min(Date)|          max(Date)|\n",
      "+-------------------+-------------------+\n",
      "|2019-10-10 00:00:00|2019-12-31 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "rc.select(min(\"Date\"), max(\"Date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date**\n",
    "\n",
    "\n",
    "**What is 3 days earlier that the oldest date and 3 days later than the most recent date?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|date_add(min(Date), -3)|\n",
      "+-----------------------+\n",
      "|             2019-10-07|\n",
      "+-----------------------+\n",
      "\n",
      "+----------------------+\n",
      "|date_add(max(Date), 3)|\n",
      "+----------------------+\n",
      "|            2020-01-03|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, lit, date_sub\n",
    "rc.select(date_add(min(\"Date\"),-3)).show()\n",
    "rc.select(date_add(max(\"Date\"),3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------+\n",
      "|date_sub(min(Date), 3)|date_add(max(Date), 3)|\n",
      "+----------------------+----------------------+\n",
      "|            2019-10-07|            2020-01-03|\n",
      "+----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.select(date_sub(min(col('Date')), 3), date_add(max(col('Date')), 3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "We2yp-o7Hgu9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdrFO3yrwptm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_01-Lab_environment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
