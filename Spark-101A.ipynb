{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = '/Users/danielsullivan/LinkedIn Learning/Spark SQL/data'\n",
    "\n",
    "data_path='C:/Users/Ali/lynda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = data_path + \"/location_temp.csv\"\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(event_date='03/04/2019 19:48:06', location_id='loc0', temp_celcius='29'),\n",
       " Row(event_date='03/04/2019 19:53:06', location_id='loc0', temp_celcius='27'),\n",
       " Row(event_date='03/04/2019 19:58:06', location_id='loc0', temp_celcius='28'),\n",
       " Row(event_date='03/04/2019 20:03:06', location_id='loc0', temp_celcius='30'),\n",
       " Row(event_date='03/04/2019 20:08:06', location_id='loc0', temp_celcius='27')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_no_header = data_path + \"/utilization.csv\"\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"false\").option(\"inferSchema\",\"true\").load(file_path_no_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='03/05/2019 08:06:14', _c1=100, _c2=0.57, _c3=0.51, _c4=47),\n",
       " Row(_c0='03/05/2019 08:11:14', _c1=100, _c2=0.47, _c3=0.62, _c4=43),\n",
       " Row(_c0='03/05/2019 08:16:14', _c1=100, _c2=0.56, _c3=0.57, _c4=62),\n",
       " Row(_c0='03/05/2019 08:21:14', _c1=100, _c2=0.57, _c3=0.56, _c4=50),\n",
       " Row(_c0='03/05/2019 08:26:14', _c1=100, _c2=0.35, _c3=0.46, _c4=43)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+----+---+\n",
      "|                _c0|_c1| _c2| _c3|_c4|\n",
      "+-------------------+---+----+----+---+\n",
      "|03/05/2019 08:06:14|100|0.57|0.51| 47|\n",
      "|03/05/2019 08:11:14|100|0.47|0.62| 43|\n",
      "|03/05/2019 08:16:14|100|0.56|0.57| 62|\n",
      "|03/05/2019 08:21:14|100|0.57|0.56| 50|\n",
      "|03/05/2019 08:26:14|100|0.35|0.46| 43|\n",
      "+-------------------+---+----+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumnRenamed(\"_c0\", \"event_datetime\") \\\n",
    "        .withColumnRenamed (\"_c1\", \"server_id\")       \\\n",
    "        .withColumnRenamed(\"_c2\", \"cpu_utilization\")  \\\n",
    "        .withColumnRenamed(\"_c3\", \"free_memory\")      \\\n",
    "        .withColumnRenamed(\"_c4\", \"session_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|       0.51|           47|\n",
      "|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n",
      "|03/05/2019 08:16:14|      100|           0.56|       0.57|           62|\n",
      "|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n",
      "|03/05/2019 08:26:14|      100|           0.35|       0.46|           43|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df1_path = data_path + \"/location_temp.json\"\n",
    "df1.write.json(json_df1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df2_path = data_path + \"/utilization.json\"\n",
    "df2.write.json(json_df2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data into DataFrames JSON Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df2_path = data_path + \"/utilization.json\"\n",
    "df2 = spark.read.format(\"json\").load(json_df2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cpu_utilization', 'event_datetime', 'free_memory', 'server_id', 'session_count']\n",
      "500000\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df2.columns),print(df2.count())\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic DataFrame operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, cpu_utilization: string, event_datetime: string, free_memory: string, server_id: string, session_count: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.62|03/05/2019 10:16:14|       0.69|      100|           56|\n",
      "|           0.48|03/05/2019 12:56:14|       0.38|      100|           44|\n",
      "|           0.55|03/05/2019 13:16:14|       0.69|      100|           66|\n",
      "|           0.47|03/05/2019 13:56:14|       0.67|      100|           66|\n",
      "|           0.39|03/05/2019 14:46:14|       0.43|      100|           39|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_sample=df2.sample(False, fraction=0.1)\n",
    "df2_sample.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.78|03/05/2019 08:06:40|       0.22|      115|           62|\n",
      "|           0.52|03/05/2019 08:07:06|       0.49|      130|           77|\n",
      "|           0.81|03/05/2019 08:07:12|       0.15|      133|           78|\n",
      "|            0.8|03/05/2019 08:07:35|       0.21|      145|           94|\n",
      "|           0.75|03/05/2019 08:11:33|       0.28|      111|           48|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_sort = df2_sample.sort('event_datetime')\n",
    "df2_sort.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter data with DataFrame API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = data_path + \"/location_temp.csv\"\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1[\"location_id\"]==\"loc0\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1[\"location_id\"]==\"loc0\").count(),df1.filter(df1[\"location_id\"]==\"loc1\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregate data with DataFrame API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|location_id|count|\n",
      "+-----------+-----+\n",
      "|     loc196| 1000|\n",
      "|     loc226| 1000|\n",
      "|     loc463| 1000|\n",
      "|     loc150| 1000|\n",
      "|     loc292| 1000|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"location_id\").count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.orderBy(\"location_id\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|avg(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|     loc196|           29.225|\n",
      "|     loc226|           25.306|\n",
      "|     loc463|           23.317|\n",
      "|     loc150|           32.188|\n",
      "|     loc292|           29.159|\n",
      "+-----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupby('location_id').agg({'temp_celcius':'mean'}).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|max(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|     loc196|               36|\n",
      "|     loc226|               32|\n",
      "|     loc463|               30|\n",
      "|     loc150|               39|\n",
      "|     loc292|               36|\n",
      "+-----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupby('location_id').agg({'temp_celcius': 'max'}).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample data from DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50236"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s1 = df1.sample(fraction=0.1,withReplacement=False)\n",
    "df_s1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|location_id| avg(temp_celcius)|\n",
      "+-----------+------------------+\n",
      "|     loc196| 29.23469387755102|\n",
      "|     loc226|24.822916666666668|\n",
      "|     loc463|23.137614678899084|\n",
      "|     loc150| 32.24074074074074|\n",
      "|     loc292|28.926605504587155|\n",
      "+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_s1.groupBy(\"location_id\").agg({'temp_celcius': 'mean'}).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|location_id| avg(temp_celcius)|\n",
      "+-----------+------------------+\n",
      "|       loc0|28.900990099009903|\n",
      "|       loc1|28.137254901960784|\n",
      "|      loc10|25.153153153153152|\n",
      "|     loc100|27.178571428571427|\n",
      "|     loc101|              25.0|\n",
      "|     loc102|30.479338842975206|\n",
      "+-----------+------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_s1.groupby('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|avg(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|       loc0|           29.176|\n",
      "|       loc1|           28.246|\n",
      "|      loc10|           25.337|\n",
      "|     loc100|           27.297|\n",
      "|     loc101|           25.317|\n",
      "|     loc102|           30.327|\n",
      "+-----------+-----------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"location_id\").agg({'temp_celcius': 'mean'}).orderBy(\"location_id\").show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save data from DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.csv('df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! ls df1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.part-00000-5e239386-0003-4bd8-98b6-668e73dffa1c-c000.csv.crc',\n",
       " '.part-00001-5e239386-0003-4bd8-98b6-668e73dffa1c-c000.csv.crc',\n",
       " '.part-00002-5e239386-0003-4bd8-98b6-668e73dffa1c-c000.csv.crc',\n",
       " '.part-00003-5e239386-0003-4bd8-98b6-668e73dffa1c-c000.csv.crc',\n",
       " '._SUCCESS.crc',\n",
       " 'part-00000-5e239386-0003-4bd8-98b6-668e73dffa1c-c000.csv',\n",
       " 'part-00001-5e239386-0003-4bd8-98b6-668e73dffa1c-c000.csv',\n",
       " 'part-00002-5e239386-0003-4bd8-98b6-668e73dffa1c-c000.csv',\n",
       " 'part-00003-5e239386-0003-4bd8-98b6-668e73dffa1c-c000.csv',\n",
       " '_SUCCESS']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.part-00000-943a1099-30ed-4a84-ac71-21a080bc25b9-c000.json.crc',\n",
       " '.part-00001-943a1099-30ed-4a84-ac71-21a080bc25b9-c000.json.crc',\n",
       " '.part-00002-943a1099-30ed-4a84-ac71-21a080bc25b9-c000.json.crc',\n",
       " '.part-00003-943a1099-30ed-4a84-ac71-21a080bc25b9-c000.json.crc',\n",
       " '._SUCCESS.crc',\n",
       " 'part-00000-943a1099-30ed-4a84-ac71-21a080bc25b9-c000.json',\n",
       " 'part-00001-943a1099-30ed-4a84-ac71-21a080bc25b9-c000.json',\n",
       " 'part-00002-943a1099-30ed-4a84-ac71-21a080bc25b9-c000.json',\n",
       " 'part-00003-943a1099-30ed-4a84-ac71-21a080bc25b9-c000.json',\n",
       " '_SUCCESS']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.write.json('df1.json')\n",
    "os.listdir('df1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Querying DataFrames with SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark SQL Query Dataframes\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data_path='C:/Users/Ali/lynda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df2_path = data_path + \"/utilization.json\"\n",
    "df = spark.read.format(\"json\").load(json_df2_path)\n",
    "print(type(df))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "root\n",
      " |-- cpu_utilization: double (nullable = true)\n",
      " |-- event_datetime: string (nullable = true)\n",
      " |-- free_memory: double (nullable = true)\n",
      " |-- server_id: long (nullable = true)\n",
      " |-- session_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Creates or replaces a local temporary view with this :class:DataFrame\\n\\nThe lifetime of this temporary table is tied to the :class:`SparkSession`\\nthat was used to create this :class:DataFrame'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"utilization\")\n",
    "'''Creates or replaces a local temporary view with this :class:DataFrame\n",
    "\n",
    "The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
    "that was used to create this :class:DataFrame'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT * FROM utilization LIMIT 10\")\n",
    "print(type(df_sql))\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|      100|           47|\n",
      "|      100|           43|\n",
      "|      100|           62|\n",
      "|      100|           50|\n",
      "|      100|           43|\n",
      "|      100|           48|\n",
      "|      100|           58|\n",
      "|      100|           58|\n",
      "|      100|           62|\n",
      "|      100|           45|\n",
      "+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, session_count FROM utilization LIMIT 10\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|sid| sc|\n",
      "+---+---+\n",
      "|100| 47|\n",
      "|100| 43|\n",
      "|100| 62|\n",
      "|100| 50|\n",
      "|100| 43|\n",
      "|100| 48|\n",
      "|100| 58|\n",
      "|100| 58|\n",
      "|100| 62|\n",
      "|100| 45|\n",
      "|100| 47|\n",
      "|100| 60|\n",
      "|100| 57|\n",
      "|100| 44|\n",
      "|100| 47|\n",
      "|100| 66|\n",
      "|100| 65|\n",
      "|100| 66|\n",
      "|100| 42|\n",
      "|100| 63|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id as sid, session_count as sc FROM utilization\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering DataFrames with SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df2_path = data_path + \"/utilization.json\"\n",
    "df = spark.read.format(\"json\").load(json_df2_path)\n",
    "type(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.66|03/05/2019 08:06:48|       0.31|      120|           54|\n",
      "|           0.58|03/05/2019 08:11:48|       0.38|      120|           64|\n",
      "|           0.55|03/05/2019 08:16:48|       0.61|      120|           54|\n",
      "|            0.7|03/05/2019 08:21:48|       0.35|      120|           80|\n",
      "|            0.6|03/05/2019 08:26:48|       0.39|      120|           71|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"utilization\")\n",
    "df_sql = spark.sql(\"SELECT * FROM utilization WHERE server_id = 120\")\n",
    "print(type(df_sql))\n",
    "df_sql.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|      100|           71|\n",
      "|      100|           71|\n",
      "|      100|           71|\n",
      "|      100|           71|\n",
      "|      100|           72|\n",
      "+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, session_count FROM utilization WHERE session_count > 70\")\n",
    "df_sql.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239659"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|      120|           80|\n",
      "|      120|           71|\n",
      "|      120|           73|\n",
      "|      120|           72|\n",
      "|      120|           78|\n",
      "+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, session_count FROM utilization WHERE session_count > 70 AND server_id = 120\")\n",
    "df_sql.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, session_count \\\n",
    "                   FROM utilization \\\n",
    "                   WHERE session_count > 70 AND server_id = 120 \\\n",
    "                   ORDER BY session_count DESC\")\n",
    "df_sql.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregating Data with SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  500000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"utilization\")\n",
    "df_count = spark.sql(\"SELECT count(*) FROM utilization\")\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  239659|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT count(*) \\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|count(1)|\n",
      "+---------+--------+\n",
      "|      112|    7425|\n",
      "|      113|    9418|\n",
      "|      130|    2891|\n",
      "|      126|    6365|\n",
      "|      149|    8288|\n",
      "|      110|    2826|\n",
      "|      136|    4316|\n",
      "|      144|    6220|\n",
      "|      116|    1167|\n",
      "|      145|    9304|\n",
      "|      143|     144|\n",
      "|      107|    5646|\n",
      "|      146|    7072|\n",
      "|      103|    8744|\n",
      "|      139|    7383|\n",
      "|      114|    2128|\n",
      "|      115|    5284|\n",
      "|      104|    7366|\n",
      "|      120|    2733|\n",
      "|      128|    3719|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, count(*) \\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70 \\\n",
    "                    GROUP BY server_id\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|count(1)|\n",
      "+---------+--------+\n",
      "|      101|    9808|\n",
      "|      113|    9418|\n",
      "|      145|    9304|\n",
      "|      103|    8744|\n",
      "|      102|    8586|\n",
      "|      133|    8583|\n",
      "|      108|    8375|\n",
      "|      149|    8288|\n",
      "|      137|    8248|\n",
      "|      148|    8027|\n",
      "|      123|    7918|\n",
      "|      118|    7913|\n",
      "|      112|    7425|\n",
      "|      139|    7383|\n",
      "|      104|    7366|\n",
      "|      142|    7084|\n",
      "|      121|    7084|\n",
      "|      146|    7072|\n",
      "|      126|    6365|\n",
      "|      144|    6220|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, count(*) \\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70 \\\n",
    "                    GROUP BY server_id \\\n",
    "                    ORDER BY count(*) DESC\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining Dataframes with SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "json_df_path = data_path + \"/utilization.json\"\n",
    "df_util = spark.read.format(\"json\").load(json_df_path)\n",
    "df_util.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|server_id|server_name|\n",
      "+---------+-----------+\n",
      "|      100| 100 Server|\n",
      "|      101| 101 Server|\n",
      "|      102| 102 Server|\n",
      "|      103| 103 Server|\n",
      "|      104| 104 Server|\n",
      "+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df_path = data_path + \"/server_name.csv\"\n",
    "df_server = spark.read.csv(csv_df_path,header=True)\n",
    "df_server.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server.createOrReplaceTempView(\"server_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|server_id|\n",
      "+---------+\n",
      "|      100|\n",
      "|      101|\n",
      "|      102|\n",
      "|      103|\n",
      "|      104|\n",
      "|      105|\n",
      "|      106|\n",
      "|      107|\n",
      "|      108|\n",
      "|      109|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count = spark.sql(\"SELECT DISTINCT server_id FROM utilization ORDER BY server_id\")\n",
    "df_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(server_id)|max(server_id)|\n",
      "+--------------+--------------+\n",
      "|           100|           149|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT min(server_id), max(server_id) FROM utilization\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|server_id|server_name|\n",
      "+---------+-----------+\n",
      "|      100| 100 Server|\n",
      "|      101| 101 Server|\n",
      "|      102| 102 Server|\n",
      "|      103| 103 Server|\n",
      "|      104| 104 Server|\n",
      "|      105| 105 Server|\n",
      "|      106| 106 Server|\n",
      "|      107| 107 Server|\n",
      "|      108| 108 Server|\n",
      "|      109| 109 Server|\n",
      "|      110| 110 Server|\n",
      "|      111| 111 Server|\n",
      "|      112| 112 Server|\n",
      "|      113| 113 Server|\n",
      "|      114| 114 Server|\n",
      "|      115| 115 Server|\n",
      "|      116| 116 Server|\n",
      "|      117| 117 Server|\n",
      "|      118| 118 Server|\n",
      "|      119| 119 Server|\n",
      "+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM server_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+-----------+\n",
      "|server_id|server_name|\n",
      "+---------+-----------+\n",
      "|      100| 100 Server|\n",
      "|      101| 101 Server|\n",
      "+---------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM utilization\").show(2),spark.sql(\"SELECT * FROM server_name\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+\n",
      "|server_id|server_name|session_count|\n",
      "+---------+-----------+-------------+\n",
      "|      100| 100 Server|           47|\n",
      "|      100| 100 Server|           43|\n",
      "|      100| 100 Server|           62|\n",
      "|      100| 100 Server|           50|\n",
      "|      100| 100 Server|           43|\n",
      "|      100| 100 Server|           48|\n",
      "|      100| 100 Server|           58|\n",
      "|      100| 100 Server|           58|\n",
      "|      100| 100 Server|           62|\n",
      "|      100| 100 Server|           45|\n",
      "|      100| 100 Server|           47|\n",
      "|      100| 100 Server|           60|\n",
      "|      100| 100 Server|           57|\n",
      "|      100| 100 Server|           44|\n",
      "|      100| 100 Server|           47|\n",
      "|      100| 100 Server|           66|\n",
      "|      100| 100 Server|           65|\n",
      "|      100| 100 Server|           66|\n",
      "|      100| 100 Server|           42|\n",
      "|      100| 100 Server|           63|\n",
      "+---------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = spark.sql(\"SELECT u.server_id, sn.server_name, u.session_count \\\n",
    "                     FROM utilization u \\\n",
    "                     INNER JOIN server_name sn \\\n",
    "                     ON sn.server_id = u.server_id\")\n",
    "df_join.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De-Duplicating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+\n",
      "|cpu_utilization|server_name|session_count|\n",
      "+---------------+-----------+-------------+\n",
      "|             85| 101 server|           80|\n",
      "|             80| 101 server|           90|\n",
      "|             85| 102 server|           80|\n",
      "|             85| 102 server|           80|\n",
      "+---------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df_path = data_path + \"/copy.csv\"\n",
    "df_dup = spark.read.csv(csv_df_path,header=True)\n",
    "df_dup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+\n",
      "|cpu_utilization|server_name|session_count|\n",
      "+---------------+-----------+-------------+\n",
      "|             85| 101 server|           80|\n",
      "|             80| 101 server|           90|\n",
      "|             85| 102 server|           80|\n",
      "+---------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dup.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+\n",
      "|cpu_utilization|server_name|session_count|\n",
      "+---------------+-----------+-------------+\n",
      "|             85| 101 server|           80|\n",
      "|             85| 102 server|           80|\n",
      "+---------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dup.drop_duplicates(['server_name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working with NAs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+\n",
      "|cpu_utilization|server_name|session_count|\n",
      "+---------------+-----------+-------------+\n",
      "|             85| 101 server|           80|\n",
      "|             80| 101 server|           90|\n",
      "|             85| 102 server|           80|\n",
      "|             85| 102 server|           80|\n",
      "|             70| 103 server|           80|\n",
      "|             60| 104 server|           75|\n",
      "+---------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df_path = data_path + \"/copy.csv\"\n",
    "df_dup = spark.read.csv(csv_df_path,header=True)\n",
    "df_dup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 server|           80|  null|\n",
      "|             80| 101 server|           90|  null|\n",
      "|             85| 102 server|           80|  null|\n",
      "|             85| 102 server|           80|  null|\n",
      "|             70| 103 server|           80|  null|\n",
      "|             60| 104 server|           75|  null|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na = df_dup.withColumn('na_col', lit(None).cast(StringType()))\n",
    "df_na.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 server|           80|     A|\n",
      "|             80| 101 server|           90|     A|\n",
      "|             85| 102 server|           80|     A|\n",
      "|             85| 102 server|           80|     A|\n",
      "|             70| 103 server|           80|     A|\n",
      "|             60| 104 server|           75|     A|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na.fillna('A').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 server|           80|     A|\n",
      "|             80| 101 server|           90|     A|\n",
      "|             85| 102 server|           80|     A|\n",
      "|             85| 102 server|           80|     A|\n",
      "|             70| 103 server|           80|     A|\n",
      "|             60| 104 server|           75|     A|\n",
      "|             85| 101 server|           80|  null|\n",
      "|             80| 101 server|           90|  null|\n",
      "|             85| 102 server|           80|  null|\n",
      "|             85| 102 server|           80|  null|\n",
      "|             70| 103 server|           80|  null|\n",
      "|             60| 104 server|           75|  null|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df_na.fillna('A').union(df_na)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 server|           80|     A|\n",
      "|             80| 101 server|           90|     A|\n",
      "|             85| 102 server|           80|     A|\n",
      "|             85| 102 server|           80|     A|\n",
      "|             70| 103 server|           80|     A|\n",
      "|             60| 104 server|           75|     A|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"na_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 server|           80|     A|\n",
      "|             80| 101 server|           90|     A|\n",
      "|             85| 102 server|           80|     A|\n",
      "|             85| 102 server|           80|     A|\n",
      "|             70| 103 server|           80|     A|\n",
      "|             60| 104 server|           75|     A|\n",
      "|             85| 101 server|           80|  null|\n",
      "|             80| 101 server|           90|  null|\n",
      "|             85| 102 server|           80|  null|\n",
      "|             85| 102 server|           80|  null|\n",
      "|             70| 103 server|           80|  null|\n",
      "|             60| 104 server|           75|  null|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM na_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 server|           80|  null|\n",
      "|             80| 101 server|           90|  null|\n",
      "|             85| 102 server|           80|  null|\n",
      "|             85| 102 server|           80|  null|\n",
      "|             70| 103 server|           80|  null|\n",
      "|             60| 104 server|           75|  null|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM na_table WHERE na_col IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 server|           80|     A|\n",
      "|             80| 101 server|           90|     A|\n",
      "|             85| 102 server|           80|     A|\n",
      "|             85| 102 server|           80|     A|\n",
      "|             70| 103 server|           80|     A|\n",
      "|             60| 104 server|           75|     A|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM na_table WHERE na_col IS NOT NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df2_path = data_path + \"/utilization.json\"\n",
    "df_util = spark.read.format(\"json\").load(json_df2_path)\n",
    "print(df_util.count())\n",
    "df_util.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+------------------+------------------+------------------+\n",
      "|summary|    cpu_utilization|     event_datetime|       free_memory|         server_id|     session_count|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+\n",
      "|  count|             500000|             500000|            500000|            500000|            500000|\n",
      "|   mean| 0.6205177400000123|               null|0.3791280999999977|             124.5|          69.59616|\n",
      "| stddev|0.15875173872912837|               null|0.1583093127837622|14.430884120553253|14.850676696352865|\n",
      "|    min|               0.22|03/05/2019 08:06:14|               0.0|               100|                32|\n",
      "|    max|                1.0|04/09/2019 01:22:46|              0.78|               149|               105|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+\n",
      "|summary|    cpu_utilization|       free_memory|\n",
      "+-------+-------------------+------------------+\n",
      "|  count|             500000|            500000|\n",
      "|   mean| 0.6205177400000123|0.3791280999999977|\n",
      "| stddev|0.15875173872912837|0.1583093127837622|\n",
      "|    min|               0.22|               0.0|\n",
      "|    max|                1.0|              0.78|\n",
      "+-------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.describe('cpu_utilization','free_memory').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.47047715730807443"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the correlation of two columns\n",
    "df_util.stat.corr('cpu_utilization','free_memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5008320848876572"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_util.stat.corr('session_count','free_memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+\n",
      "| server_id_freqItems|session_count_freqItems|\n",
      "+--------------------+-----------------------+\n",
      "|[137, 146, 101, 1...|   [92, 101, 83, 95,...|\n",
      "+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding frequent items for columns, possibly with false positives.\n",
    "df_util.stat.freqItems(('server_id','session_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25217"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_util_sample = df_util.sample(fraction=0.05, withReplacement=False)\n",
    "df_util_sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory data analysis with Spark SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------------+\n",
      "|min(cpu_utilization)|max(cpu_utilization)|stddev_samp(cpu_utilization)|\n",
      "+--------------------+--------------------+----------------------------+\n",
      "|                0.22|                 1.0|         0.15875173872912837|\n",
      "+--------------------+--------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT min(cpu_utilization), max(cpu_utilization), stddev(cpu_utilization) FROM utilization').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+----------------------------+\n",
      "|server_id|min(cpu_utilization)|max(cpu_utilization)|stddev_samp(cpu_utilization)|\n",
      "+---------+--------------------+--------------------+----------------------------+\n",
      "|      112|                0.52|                0.92|         0.11528867845082576|\n",
      "|      113|                0.58|                0.98|         0.11544345150353694|\n",
      "|      130|                0.35|                0.75|         0.11568834774245991|\n",
      "|      126|                0.48|                0.88|         0.11542612970702051|\n",
      "|      149|                0.54|                0.94|         0.11543517500295467|\n",
      "|      110|                0.35|                0.75|         0.11533251724450215|\n",
      "|      136|                0.41|                 0.8|         0.11597405743182258|\n",
      "|      144|                0.47|                0.87|         0.11478654960489501|\n",
      "|      119|                0.22|                0.62|         0.11516031929842005|\n",
      "|      116|                 0.3|                 0.7|         0.11506079722349302|\n",
      "|      145|                0.58|                0.98|         0.11476448868786174|\n",
      "|      124|                0.24|                0.64|         0.11579377614906383|\n",
      "|      143|                0.26|                0.66|         0.11616596078044727|\n",
      "|      107|                0.45|                0.85|         0.11597417369783877|\n",
      "|      146|                 0.5|                 0.9|         0.11488129439634706|\n",
      "|      103|                0.56|                0.96|         0.11617507884178278|\n",
      "|      139|                0.51|                0.91|         0.11519660023052102|\n",
      "|      138|                0.24|                0.64|          0.1155955860444133|\n",
      "|      114|                0.33|                0.73|         0.11510268816097273|\n",
      "|      115|                0.44|                0.84|         0.11569664615014985|\n",
      "+---------+--------------------+--------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT server_id, min(cpu_utilization), max(cpu_utilization), stddev(cpu_utilization) \\\n",
    "           FROM utilization \\\n",
    "           GROUP BY server_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|server_id|bucket|\n",
      "+---------+------+\n",
      "|      100|     5|\n",
      "|      100|     4|\n",
      "|      100|     5|\n",
      "|      100|     5|\n",
      "|      100|     3|\n",
      "|      100|     4|\n",
      "|      100|     5|\n",
      "|      100|     4|\n",
      "|      100|     5|\n",
      "|      100|     5|\n",
      "|      100|     3|\n",
      "|      100|     6|\n",
      "|      100|     6|\n",
      "|      100|     5|\n",
      "|      100|     2|\n",
      "|      100|     4|\n",
      "|      100|     4|\n",
      "|      100|     6|\n",
      "|      100|     4|\n",
      "|      100|     5|\n",
      "+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT server_id, FLOOR(cpu_utilization*100/10) bucket FROM utilization').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|count(1)|bucket|\n",
      "+--------+------+\n",
      "|    8186|     2|\n",
      "|   37029|     3|\n",
      "|   68046|     4|\n",
      "|  104910|     5|\n",
      "|  116725|     6|\n",
      "|   88242|     7|\n",
      "|   56598|     8|\n",
      "|   20207|     9|\n",
      "|      57|    10|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT count(*), FLOOR(cpu_utilization*100/10) bucket FROM utilization GROUP BY bucket ORDER BY bucket').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Timeseries Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df2_path = data_path + \"/utilization.json\"\n",
    "df_util = spark.read.format(\"json\").load(json_df2_path)\n",
    "df_util.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+----------------------------+\n",
      "|server_id|min(cpu_utilization)|max(cpu_utilization)|stddev_samp(cpu_utilization)|\n",
      "+---------+--------------------+--------------------+----------------------------+\n",
      "|      112|                0.52|                0.92|         0.11528867845082576|\n",
      "|      113|                0.58|                0.98|         0.11544345150353694|\n",
      "|      130|                0.35|                0.75|         0.11568834774245991|\n",
      "|      126|                0.48|                0.88|         0.11542612970702051|\n",
      "|      149|                0.54|                0.94|         0.11543517500295467|\n",
      "|      110|                0.35|                0.75|         0.11533251724450215|\n",
      "|      136|                0.41|                 0.8|         0.11597405743182258|\n",
      "|      144|                0.47|                0.87|         0.11478654960489501|\n",
      "|      119|                0.22|                0.62|         0.11516031929842005|\n",
      "|      116|                 0.3|                 0.7|         0.11506079722349302|\n",
      "+---------+--------------------+--------------------+----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT server_id, min(cpu_utilization), max(cpu_utilization), stddev(cpu_utilization) \\\n",
    "           FROM utilization \\\n",
    "           GROUP BY server_id').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowing Operation\n",
    "sql_window = \"SELECT event_datetime, server_id, cpu_utilization,  \\\n",
    "         avg(cpu_utilization) OVER (PARTITION BY server_id) avg_server_util \\\n",
    "FROM  \\\n",
    "      utilization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|   avg_server_util|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "|03/05/2019 08:06:34|      112|           0.71|0.7153870000000067|\n",
      "|03/05/2019 08:11:34|      112|           0.78|0.7153870000000067|\n",
      "|03/05/2019 08:16:34|      112|           0.87|0.7153870000000067|\n",
      "|03/05/2019 08:21:34|      112|           0.82|0.7153870000000067|\n",
      "|03/05/2019 08:26:34|      112|           0.62|0.7153870000000067|\n",
      "|03/05/2019 08:31:34|      112|            0.9|0.7153870000000067|\n",
      "|03/05/2019 08:36:34|      112|           0.89|0.7153870000000067|\n",
      "|03/05/2019 08:41:34|      112|           0.81|0.7153870000000067|\n",
      "|03/05/2019 08:46:34|      112|           0.88|0.7153870000000067|\n",
      "|03/05/2019 08:51:34|      112|           0.89|0.7153870000000067|\n",
      "|03/05/2019 08:56:34|      112|           0.84|0.7153870000000067|\n",
      "|03/05/2019 09:01:34|      112|           0.71|0.7153870000000067|\n",
      "|03/05/2019 09:06:34|      112|           0.85|0.7153870000000067|\n",
      "|03/05/2019 09:11:34|      112|           0.72|0.7153870000000067|\n",
      "|03/05/2019 09:16:34|      112|           0.54|0.7153870000000067|\n",
      "|03/05/2019 09:21:34|      112|           0.58|0.7153870000000067|\n",
      "|03/05/2019 09:26:34|      112|           0.73|0.7153870000000067|\n",
      "|03/05/2019 09:31:34|      112|           0.86|0.7153870000000067|\n",
      "|03/05/2019 09:36:34|      112|           0.63|0.7153870000000067|\n",
      "|03/05/2019 09:41:34|      112|           0.75|0.7153870000000067|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(sql_window).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_window2 = \"SELECT event_datetime, server_id, cpu_utilization,  \\\n",
    "         avg(cpu_utilization) OVER (PARTITION BY server_id) avg_server_util, \\\n",
    "         cpu_utilization - avg(cpu_utilization) OVER (PARTITION BY server_id) delta_server_util \\\n",
    "         FROM utilization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|   avg_server_util|   delta_server_util|\n",
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "|03/05/2019 08:06:34|      112|           0.71|0.7153870000000067|-0.00538700000000...|\n",
      "|03/05/2019 08:11:34|      112|           0.78|0.7153870000000067| 0.06461299999999337|\n",
      "|03/05/2019 08:16:34|      112|           0.87|0.7153870000000067| 0.15461299999999334|\n",
      "|03/05/2019 08:21:34|      112|           0.82|0.7153870000000067|  0.1046129999999933|\n",
      "|03/05/2019 08:26:34|      112|           0.62|0.7153870000000067|-0.09538700000000666|\n",
      "|03/05/2019 08:31:34|      112|            0.9|0.7153870000000067| 0.18461299999999337|\n",
      "|03/05/2019 08:36:34|      112|           0.89|0.7153870000000067| 0.17461299999999336|\n",
      "|03/05/2019 08:41:34|      112|           0.81|0.7153870000000067|  0.0946129999999934|\n",
      "|03/05/2019 08:46:34|      112|           0.88|0.7153870000000067| 0.16461299999999335|\n",
      "|03/05/2019 08:51:34|      112|           0.89|0.7153870000000067| 0.17461299999999336|\n",
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(sql_window2).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_window3 = \"SELECT event_datetime, server_id, cpu_utilization,  \\\n",
    "                      avg(cpu_utilization) OVER (PARTITION BY server_id ORDER BY event_datetime \\\n",
    "                                    ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) avg_server_util \\\n",
    "                FROM  \\\n",
    "                      utilization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|   avg_server_util|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "|03/05/2019 08:06:34|      112|           0.71|             0.745|\n",
      "|03/05/2019 08:11:34|      112|           0.78|0.7866666666666666|\n",
      "|03/05/2019 08:16:34|      112|           0.87|0.8233333333333333|\n",
      "|03/05/2019 08:21:34|      112|           0.82|              0.77|\n",
      "|03/05/2019 08:26:34|      112|           0.62|0.7799999999999999|\n",
      "|03/05/2019 08:31:34|      112|            0.9|0.8033333333333333|\n",
      "|03/05/2019 08:36:34|      112|           0.89|0.8666666666666667|\n",
      "|03/05/2019 08:41:34|      112|           0.81|              0.86|\n",
      "|03/05/2019 08:46:34|      112|           0.88|              0.86|\n",
      "|03/05/2019 08:51:34|      112|           0.89|              0.87|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(sql_window3).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning - Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df2_path = data_path + \"/utilization.json\"\n",
    "df_util = spark.read.format(\"json\").load(json_df2_path)\n",
    "df_util.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_3b9bc23c56f5"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A feature transformer that merges multiple columns into a vector column.\n",
    "# Group based on cpu_utilization\", \"free_memory\", \"session_count\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"cpu_utilization\", \"free_memory\", \"session_count\"], outputCol=\"features\")\n",
    "vectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cpu_utilization: double, event_datetime: string, free_memory: double, server_id: bigint, session_count: bigint, features: vector]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcluster_df = vectorAssembler.transform(df_util)\n",
    "vcluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|        features|\n",
      "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|[0.57,0.51,47.0]|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|[0.47,0.62,43.0]|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|[0.56,0.57,62.0]|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|[0.57,0.56,50.0]|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|[0.35,0.46,43.0]|\n",
      "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(vcluster_df))\n",
    "vcluster_df.show(5)\n",
    "# Features is the new column output by Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(3)\n",
    "kmeans = kmeans.setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.71174897,  0.28808911, 86.87510507]),\n",
       " array([ 0.61918113,  0.38080285, 68.75004716]),\n",
       " array([ 0.51439668,  0.48445202, 50.49452021])]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmodel = kmeans.fit(vcluster_df)\n",
    "kmodel.clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning - Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df2_path = data_path + \"/utilization.json\"\n",
    "df_util = spark.read.format(\"json\").load(json_df2_path)\n",
    "df_util.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+--------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|features|\n",
      "+---------------+-------------------+-----------+---------+-------------+--------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|  [0.57]|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|  [0.47]|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|  [0.56]|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|  [0.57]|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|  [0.35]|\n",
      "+---------------+-------------------+-----------+---------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=[\"cpu_utilization\"], outputCol=\"features\")\n",
    "df_vutil = vectorAssembler.transform(df_util)\n",
    "df_vutil.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.regression.LinearRegression'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression_7f5b91cbc38c"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\",labelCol=\"session_count\")\n",
    "print(type(lr))\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([47.024])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel = lr.fit(df_vutil)\n",
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.41695103556927"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.837990225931833"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.summary.rootMeanSquaredError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
