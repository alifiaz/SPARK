{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<img src=\"https://files.training.databricks.com/images/Apache-Spark-Logo_TM_200px.png\" style=\"float: left: margin: 20px\"/>\n\n# ETL Process Overview\n\nApache Spark&trade; and Databricks&reg; allow you to create an end-to-end _extract, transform, load (ETL)_ pipeline.\n## In this lesson you:\n* Create a basic end-to-end ETL pipeline\n* Demonstrate the Spark approach to ETL pipelines\n\n## Audience\n* Primary Audience: Data Engineers\n* Additional Audiences: Data Scientists and Data Pipeline Engineers\n\n## Prerequisites\n* Web browser: Chrome\n* A cluster configured with **8 cores** and **DBR 6.2**"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the<br/>\nstart of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/rd9d11fwe6?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/rd9d11fwe6?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### The Spark Approach\n\nSpark offers a compute engine and connectors to virtually any data source. By leveraging easily scaled infrastructure and accessing data where it lives, Spark addresses the core needs of a big data application.\n\nThese principles comprise the Spark approach to ETL, providing a unified and scalable approach to big data pipelines: <br><br>\n\n1. Databricks and Spark offer a **unified platform** \n - Spark on Databricks combines ETL, stream processing, machine learning, and collaborative notebooks.\n - Data scientists, analysts, and engineers can write Spark code in Python, Scala, SQL, and R.\n2. Spark's unified platform is **scalable to petabytes of data and clusters of thousands of nodes**.  \n - The same code written on smaller data sets scales to large workloads, often with only small changes.\n2. Spark on Databricks decouples data storage from the compute and query engine.  \n - Spark's query engine **connects to any number of data sources** such as S3, Azure Blob Storage, Redshift, and Kafka.  \n - This **minimizes costs**; a dedicated cluster does not need to be maintained and the compute cluster is **easily updated to the latest version** of Spark.\n \n<img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/Workload_Tools_2-01.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"],"metadata":{}},{"cell_type":"markdown","source":["### A Basic ETL Job\n\nIn this lesson you use web log files from the <a href=\"https://www.sec.gov/dera/data/edgar-log-file-data-set.html\" target=\"_blank\">US Securities and Exchange Commission website</a> to do a basic ETL for a day of server activity. You will extract the fields of interest and load them into persistent storage."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/95uh9cxyb3?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/95uh9cxyb3?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["The Databricks File System (DBFS) is an HDFS-like interface to bulk data stores like Amazon's S3 and Azure's Blob storage service.\n\nPass the path `/mnt/training/EDGAR-Log-20170329/EDGAR-Log-20170329.csv` into `spark.read.csv`to access data stored in DBFS. Use the header option to specify that the first line of the file is the header."],"metadata":{}},{"cell_type":"code","source":["path = \"/mnt/training/EDGAR-Log-20170329/EDGAR-Log-20170329.csv\"\n\nlogDF = (spark\n  .read\n  .option(\"header\", True)\n  .csv(path)\n  .sample(withReplacement=False, fraction=0.3, seed=3) # using a sample to reduce data size\n)\n\nprint(type(logDF))\n#display(logDF)\nlogDF.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\n+---------------+----------+--------+----+---------+--------------------+--------------------+-----+------+---+-------+-------+----+-------+-------+\n             ip|      date|    time|zone|      cik|           accession|           extention| code|  size|idx|norefer|noagent|find|crawler|browser|\n+---------------+----------+--------+----+---------+--------------------+--------------------+-----+------+---+-------+-------+----+-------+-------+\n  101.71.41.ihh|2017-03-29|00:00:00| 0.0|1437491.0|0001245105-17-000052|xslF345X03/primar...|301.0| 687.0|0.0|    0.0|    0.0|10.0|    0.0|   null|\n104.196.240.dda|2017-03-29|00:00:00| 0.0|1270985.0|0001188112-04-001037|                .txt|200.0|7619.0|0.0|    0.0|    0.0|10.0|    0.0|   null|\n  107.23.85.jfd|2017-03-29|00:00:00| 0.0|1059376.0|0000905148-07-006108|          -index.htm|200.0|2727.0|1.0|    0.0|    0.0|10.0|    0.0|   null|\n  107.23.85.jfd|2017-03-29|00:00:00| 0.0|1059376.0|0000905148-08-001993|          -index.htm|200.0|2710.0|1.0|    0.0|    0.0|10.0|    0.0|   null|\n  107.23.85.jfd|2017-03-29|00:00:00| 0.0|1059376.0|0001104659-09-046963|          -index.htm|200.0|2715.0|1.0|    0.0|    0.0|10.0|    0.0|   null|\n+---------------+----------+--------+----+---------+--------------------+--------------------+-----+------+---+-------+-------+----+-------+-------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Next, review the server-side errors, which have error codes in the 500s."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nserverErrorDF = (logDF\n  .filter((col(\"code\") >= 500) & (col(\"code\") < 600))\n  .select(\"date\", \"time\", \"extention\", \"code\")\n)\n\nprint(type(serverErrorDF))\n#display(serverErrorDF)\nserverErrorDF.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\n+----------+--------+----------+-----+\n      date|    time| extention| code|\n+----------+--------+----------+-----+\n2017-03-29|00:00:12|      .txt|503.0|\n2017-03-29|00:00:16|-index.htm|503.0|\n2017-03-29|00:00:24|-index.htm|503.0|\n2017-03-29|00:00:44|-index.htm|503.0|\n2017-03-29|00:01:01|-index.htm|503.0|\n+----------+--------+----------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["### Data Validation\n\nOne aspect of ETL jobs is to validate that the data is what you expect.  This includes:<br><br>\n* Approximately the expected number of records\n* The expected fields are present\n* No unexpected missing values"],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/k3mf97q7nn?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/k3mf97q7nn?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nTake a look at the server-side errors by hour to confirm the data meets your expectations. Visualize it by selecting the bar graph icon once the table is displayed. <br><br>\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/visualization.png\" style=\"height: 400px\" style=\"margin-bottom: 20px; height: 150px; border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/></div>"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import from_utc_timestamp, hour, col\n\ncountsDF = (serverErrorDF\n  .select(hour(from_utc_timestamp(col(\"time\"), \"GMT\")).alias(\"hour\"))\n  .groupBy(\"hour\")\n  .count()\n  .orderBy(\"hour\")\n)\n\ndisplay(countsDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>hour</th><th>count</th></tr></thead><tbody><tr><td>0</td><td>2030</td></tr><tr><td>1</td><td>1638</td></tr><tr><td>2</td><td>1123</td></tr><tr><td>3</td><td>1093</td></tr><tr><td>4</td><td>1118</td></tr><tr><td>5</td><td>1168</td></tr><tr><td>6</td><td>1089</td></tr><tr><td>7</td><td>1054</td></tr><tr><td>8</td><td>1055</td></tr><tr><td>9</td><td>1022</td></tr><tr><td>10</td><td>1061</td></tr><tr><td>11</td><td>1141</td></tr><tr><td>12</td><td>1075</td></tr><tr><td>13</td><td>1086</td></tr><tr><td>14</td><td>1139</td></tr><tr><td>15</td><td>1309</td></tr><tr><td>16</td><td>1341</td></tr><tr><td>17</td><td>1434</td></tr><tr><td>18</td><td>1138</td></tr><tr><td>19</td><td>1235</td></tr><tr><td>20</td><td>1263</td></tr><tr><td>21</td><td>1068</td></tr><tr><td>22</td><td>1049</td></tr><tr><td>23</td><td>1106</td></tr></tbody></table></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["The distribution of errors by hour meets the expectations.  There is an uptick in errors around midnight, possibly due to server maintenance at this time."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Saving Back to DBFS\n\nA common and highly effective design pattern in the Databricks and Spark ecosystem involves loading structured data back to DBFS as a parquet file. Learn more about [the scalable and optimized data storage format parquet here](http://parquet.apache.org/).\n\nSave the parsed DataFrame back to DBFS as parquet using the `.write` method.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> All clusters have storage available to them in the `/tmp/` directory.  In the case of Community Edition clusters, this is a small, but helpful, amount of storage.  \n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> If you run out of storage, use the command `dbutils.fs.rm(\"/tmp/<my directory>\", True)` to recursively remove all items from a directory.  Note that this is a permanent action."],"metadata":{}},{"cell_type":"code","source":["targetPath = workingDir + \"/log20170329/serverErrorDF.parquet\"\n\n(serverErrorDF\n  .write\n  .mode(\"overwrite\") # overwrites a file if it already exists\n  .parquet(targetPath)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/qu6fxg1f6a?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/qu6fxg1f6a?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## Exercise 1: Perform an ETL Job\n\nWrite a basic ETL script that captures the 20 most active website users and load the results to DBFS."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Create a DataFrame of Aggregate Statistics\n\nCreate a DataFrame `ipCountDF` that uses `logDF` to create a count of each time a given IP address appears in the logs, with the counts sorted in descending order.  The result should have two columns: `ip` and `count`."],"metadata":{}},{"cell_type":"code","source":["# TODO\nfrom pyspark.sql.functions import desc, count\n\nipCountDF = (logDF.groupBy(\"ip\").agg(count(\"ip\").alias(\"count\")).orderBy(desc(\"count\")))\n#display(ipCountDF)\nipCountDF.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+------+\n             ip| count|\n+---------------+------+\n 213.152.28.bhe|518548|\n 158.132.91.haf|497361|\n   117.91.6.caf|239912|\n132.195.122.djf|197267|\n   117.91.2.aha|152731|\n+---------------+------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nip1, count1 = ipCountDF.first()\ncols = set(ipCountDF.columns)\n\ndbTest(\"ET1-P-02-01-01\", \"213.152.28.bhe\", ip1)\ndbTest(\"ET1-P-02-01-02\", True, count1 > 500000 and count1 < 550000)\ndbTest(\"ET1-P-02-01-03\", True, 'count' in cols)\ndbTest(\"ET1-P-02-01-03\", True, 'ip' in cols)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["-sandbox\n### Step 2: Save the Results\n\nUse your temporary folder to save the results back to DBFS as `workingDir + \"/ipCount.parquet\"`\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** If you run out of space, use `%fs rm -r /tmp/<my directory>` to recursively (and permanently) remove all items from a directory."],"metadata":{}},{"cell_type":"code","source":["writePath = workingDir + \"/ipCount.parquet\"\n\n(ipCountDF\n  .write\n  .mode(\"overwrite\") # overwrites a file if it already exists, always safe to do ... \n  .parquet(writePath)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nfrom pyspark.sql.functions import desc\n\nwritePath = workingDir + \"/ipCount.parquet\"\n\nipCountDF2 = (spark\n  .read\n  .parquet(writePath)\n  .orderBy(desc(\"count\"))\n)\nip1, count1 = ipCountDF2.first()\ncols = ipCountDF2.columns\n\ndbTest(\"ET1-P-02-02-01\", \"213.152.28.bhe\", ip1)\ndbTest(\"ET1-P-02-02-02\", True, count1 > 500000 and count1 < 550000)\ndbTest(\"ET1-P-02-02-03\", True, \"count\" in cols)\ndbTest(\"ET1-P-02-02-04\", True, \"ip\" in cols)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["Check the load worked by using listing the files in our **`writePath`**\n\nParquet divides your data into a number of files.\n\nIf successful, you see a `_SUCCESS` file as well as the data split across a number of parts."],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(writePath))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/_SUCCESS</td><td>_SUCCESS</td><td>0</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/_committed_4810861331833940264</td><td>_committed_4810861331833940264</td><td>3458</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/_started_4810861331833940264</td><td>_started_4810861331833940264</td><td>0</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00000-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1414-1-c000.snappy.parquet</td><td>part-00000-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1414-1-c000.snappy.parquet</td><td>5315</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00001-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1415-1-c000.snappy.parquet</td><td>part-00001-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1415-1-c000.snappy.parquet</td><td>4931</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00002-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1416-1-c000.snappy.parquet</td><td>part-00002-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1416-1-c000.snappy.parquet</td><td>5074</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00003-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1417-1-c000.snappy.parquet</td><td>part-00003-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1417-1-c000.snappy.parquet</td><td>5014</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00004-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1418-1-c000.snappy.parquet</td><td>part-00004-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1418-1-c000.snappy.parquet</td><td>5025</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00005-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1419-1-c000.snappy.parquet</td><td>part-00005-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1419-1-c000.snappy.parquet</td><td>4699</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00006-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1420-1-c000.snappy.parquet</td><td>part-00006-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1420-1-c000.snappy.parquet</td><td>4283</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00007-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1421-1-c000.snappy.parquet</td><td>part-00007-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1421-1-c000.snappy.parquet</td><td>4258</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00008-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1422-1-c000.snappy.parquet</td><td>part-00008-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1422-1-c000.snappy.parquet</td><td>4587</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00009-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1423-1-c000.snappy.parquet</td><td>part-00009-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1423-1-c000.snappy.parquet</td><td>4282</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00010-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1424-1-c000.snappy.parquet</td><td>part-00010-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1424-1-c000.snappy.parquet</td><td>4084</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00011-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1425-1-c000.snappy.parquet</td><td>part-00011-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1425-1-c000.snappy.parquet</td><td>4783</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00012-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1426-1-c000.snappy.parquet</td><td>part-00012-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1426-1-c000.snappy.parquet</td><td>4389</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00013-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1427-1-c000.snappy.parquet</td><td>part-00013-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1427-1-c000.snappy.parquet</td><td>4333</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00014-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1428-1-c000.snappy.parquet</td><td>part-00014-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1428-1-c000.snappy.parquet</td><td>3617</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00015-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1429-1-c000.snappy.parquet</td><td>part-00015-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1429-1-c000.snappy.parquet</td><td>4515</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00016-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1430-1-c000.snappy.parquet</td><td>part-00016-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1430-1-c000.snappy.parquet</td><td>4923</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00017-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1431-1-c000.snappy.parquet</td><td>part-00017-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1431-1-c000.snappy.parquet</td><td>3515</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00018-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1432-1-c000.snappy.parquet</td><td>part-00018-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1432-1-c000.snappy.parquet</td><td>6095</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00019-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1433-1-c000.snappy.parquet</td><td>part-00019-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1433-1-c000.snappy.parquet</td><td>3997</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00020-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1434-1-c000.snappy.parquet</td><td>part-00020-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1434-1-c000.snappy.parquet</td><td>3974</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00021-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1435-1-c000.snappy.parquet</td><td>part-00021-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1435-1-c000.snappy.parquet</td><td>4499</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00022-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1436-1-c000.snappy.parquet</td><td>part-00022-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1436-1-c000.snappy.parquet</td><td>5023</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00023-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1437-1-c000.snappy.parquet</td><td>part-00023-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1437-1-c000.snappy.parquet</td><td>5720</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00024-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1438-1-c000.snappy.parquet</td><td>part-00024-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1438-1-c000.snappy.parquet</td><td>6384</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00025-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1439-1-c000.snappy.parquet</td><td>part-00025-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1439-1-c000.snappy.parquet</td><td>7287</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00026-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1440-1-c000.snappy.parquet</td><td>part-00026-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1440-1-c000.snappy.parquet</td><td>8893</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00027-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1441-1-c000.snappy.parquet</td><td>part-00027-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1441-1-c000.snappy.parquet</td><td>10411</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00028-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1442-1-c000.snappy.parquet</td><td>part-00028-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1442-1-c000.snappy.parquet</td><td>13795</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00029-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1443-1-c000.snappy.parquet</td><td>part-00029-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1443-1-c000.snappy.parquet</td><td>18362</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00030-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1444-1-c000.snappy.parquet</td><td>part-00030-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1444-1-c000.snappy.parquet</td><td>25948</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00031-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1445-1-c000.snappy.parquet</td><td>part-00031-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1445-1-c000.snappy.parquet</td><td>40849</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00032-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1446-1-c000.snappy.parquet</td><td>part-00032-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1446-1-c000.snappy.parquet</td><td>90959</td></tr><tr><td>dbfs:/user/engr.alifiaz@gmail.com/etl_part_1/etl1_02_etl_process_overview_psp/ipCount.parquet/part-00033-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1447-1-c000.snappy.parquet</td><td>part-00033-tid-4810861331833940264-22919b88-b650-443b-86bb-d0c5255cbc1c-1447-1-c000.snappy.parquet</td><td>367014</td></tr></tbody></table></div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["## Review\n**Question:** What does ETL stand for and what are the stages of the process?  \n**Answer:** ETL stands for `extract-transform-load`\n0. *Extract* refers to ingesting data.  Spark easily connects to data in a number of different sources.\n0. *Transform* refers to applying structure, parsing fields, cleaning data, and/or computing statistics.\n0. *Load* refers to loading data to its final destination, usually a database or data warehouse.\n\n**Question:** How does the Spark approach to ETL deal with devops issues such as updating a software version?  \n**Answer:** By decoupling storage and compute, updating your Spark version is as easy as spinning up a new cluster.  Your old code will easily connect to S3, the Azure Blob, or other storage.  This also avoids the challenge of keeping a cluster always running, such as with Hadoop clusters.\n\n**Question:** How does the Spark approach to data applications differ from other solutions?  \n**Answer:** Spark offers a unified solution to use cases that would otherwise need individual tools. For instance, Spark combines machine learning, ETL, stream processing, and a number of other solutions all with one technology."],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Connecting to S3]($./ETL1 03 - Connecting to S3)."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I get more information on building ETL pipelines?  \n**A:** Check out the Spark Summit talk on <a href=\"https://databricks.com/session/building-robust-etl-pipelines-with-apache-spark\" target=\"_blank\">Building Robust ETL Pipelines with Apache Spark</a>\n\n**Q:** Where can I find out more information on moving from traditional ETL pipelines towards Spark?  \n**A:** Check out the Spark Summit talk <a href=\"https://databricks.com/session/get-rid-of-traditional-etl-move-to-spark\" target=\"_blank\">Get Rid of Traditional ETL, Move to Spark!</a>\n\n**Q:** What are the visualization options in Databricks?  \n**A:** Databricks provides a wide variety of <a href=\"https://docs.databricks.com/user-guide/visualizations/index.html\" target=\"_blank\">built-in visualizations</a>.  Databricks also supports a variety of 3rd party visualization libraries, including <a href=\"https://d3js.org/\" target=\"_blank\">d3.js</a>, <a href=\"https://matplotlib.org/\" target=\"_blank\">matplotlib</a>, <a href=\"http://ggplot.yhathq.com/\" target=\"_blank\">ggplot</a>, and <a href=\"https://plot.ly/\" target=\"_blank\">plotly<a/>."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"ETL1 02 - ETL Process Overview","notebookId":3953685866792922},"nbformat":4,"nbformat_minor":0}
