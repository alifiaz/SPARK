{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PySpark \n",
    "### Video series\n",
    "\n",
    "### Packt Publishing\n",
    "\n",
    "**Author**: Tomasz Drabas\n",
    "**Date**:   2018-01-30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Section 4: Spark DataFrames & Transformations\n",
    "\n",
    "In this section we will look at the Spark DataFrames and the transformations available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames\n",
    "### From RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"create\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rdd = sc.parallelize([\n",
    "      ['2017-02-01','Rachel', 19, 156, 'Sydney']\n",
    "    , ['2018-01-01','Albert',  3,  45, 'New York']\n",
    "    , ['2018-03-02','Jack',   61, 190, 'Krakow']\n",
    "    , ['2017-12-31','Skye',    8,  82, 'Harbin']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------+--------+\n",
      "|      Date|  Name|Age|Weight|Location|\n",
      "+----------+------+---+------+--------+\n",
      "|2017-02-01|Rachel| 19|   156|  Sydney|\n",
      "|2018-01-01|Albert|  3|    45|New York|\n",
      "|2018-03-02|  Jack| 61|   190|  Krakow|\n",
      "|2017-12-31|  Skye|  8|    82|  Harbin|\n",
      "+----------+------+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simple_df = spark.createDataFrame(\n",
    "    simple_rdd, \n",
    "    ['Date','Name', 'Age', 'Weight', 'Location']\n",
    ")\n",
    "simple_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From JSON string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+------+------+\n",
      "|Age|      Date|Location|  Name|Weight|\n",
      "+---+----------+--------+------+------+\n",
      "| 19|2017-02-01|  Sydney|Rachel|   156|\n",
      "|  3|2018-01-01|New York|Albert|    45|\n",
      "| 61|2018-03-02|  Krakow|  Jack|   190|\n",
      "|  8|2017-12-31|  Harbin|  Skye|    82|\n",
      "+---+----------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_string = [\n",
    "    '{\"Date\":\"2017-02-01\",\"Name\":\"Rachel\",\"Age\":19,\"Weight\":156,\"Location\":\"Sydney\"}', \n",
    "    '{\"Date\":\"2018-01-01\",\"Name\":\"Albert\",\"Age\":3 ,\"Weight\":45 ,\"Location\":\"New York\"}', \n",
    "    '{\"Date\":\"2018-03-02\",\"Name\":\"Jack\"  ,\"Age\":61,\"Weight\":190,\"Location\":\"Krakow\"}', \n",
    "    '{\"Date\":\"2017-12-31\",\"Name\":\"Skye\"  ,\"Age\":8 ,\"Weight\":82 ,\"Location\":\"Harbin\"}'\n",
    "]\n",
    "\n",
    "simple_df_json = spark.read.json(sc.parallelize(json_string))\n",
    "simple_df_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+------+-----+--------+------+\n",
      "|OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|\n",
      "+---------+-------+-------+------+-----+--------+------+\n",
      "| 1/6/2016|   East|  Jones|Pencil|   95|    1.99|189.05|\n",
      "| 3/2/2017|Central| Kivell|Binder|   50|   19.99| 999.5|\n",
      "| 2/9/2016|Central|Jardine|Pencil|   36|    4.99|179.64|\n",
      "|2/26/2016|Central|   Gill|   Pen|   27|   19.99|539.73|\n",
      "+---------+-------+-------+------+-----+--------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df = spark.read.csv(\n",
    "    'sample_data.csv'\n",
    "    , header=True\n",
    ")\n",
    "sample_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame schema\n",
    "### RDDs reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Rep: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Units: string (nullable = true)\n",
      " |-- UnitCost: string (nullable = true)\n",
      " |-- Total: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically specifying schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------+--------+\n",
      "|      Date|  Name|Age|Weight|Location|\n",
      "+----------+------+---+------+--------+\n",
      "|2017-02-01|Rachel| 19|   156|  Sydney|\n",
      "|2018-01-01|Albert|  3|    45|New York|\n",
      "|2018-03-02|  Jack| 61|   190|  Krakow|\n",
      "|2017-12-31|  Skye|  8|    82|  Harbin|\n",
      "+----------+------+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as typ\n",
    "import datetime as dt\n",
    "\n",
    "schema = [\n",
    "      ('Date', typ.DateType())\n",
    "    , ('Name', typ.StringType())\n",
    "    , ('Age',  typ.IntegerType())\n",
    "    , ('Weight', typ.IntegerType())\n",
    "    , ('Location', typ.StringType())\n",
    "]\n",
    "\n",
    "schema = typ.StructType([typ.StructField(e[0], e[1], True) for e in schema])\n",
    "\n",
    "simple_df_schema = spark.createDataFrame(\n",
    "      simple_rdd\n",
    "        .map(lambda row: \n",
    "             [dt.datetime.strptime(row[0], '%Y-%m-%d')] + row[1:]\n",
    "            )\n",
    "    , schema=schema\n",
    ")\n",
    "\n",
    "simple_df_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Weight: integer (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simple_df_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically inferring schema while reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Rep: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Units: string (nullable = true)\n",
      " |-- UnitCost: string (nullable = true)\n",
      " |-- Total: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Rep: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Units: integer (nullable = true)\n",
      " |-- UnitCost: double (nullable = true)\n",
      " |-- Total: double (nullable = true)\n",
      "\n",
      "+---------+-------+-------+------+-----+--------+------+\n",
      "|OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|\n",
      "+---------+-------+-------+------+-----+--------+------+\n",
      "| 1/6/2016|   East|  Jones|Pencil|   95|    1.99|189.05|\n",
      "| 3/2/2017|Central| Kivell|Binder|   50|   19.99| 999.5|\n",
      "| 2/9/2016|Central|Jardine|Pencil|   36|    4.99|179.64|\n",
      "|2/26/2016|Central|   Gill|   Pen|   27|   19.99|539.73|\n",
      "|3/15/2016|   West|Sorvino|Pencil|   56|    2.99|167.44|\n",
      "+---------+-------+-------+------+-----+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df_inferred = spark.read.csv(\n",
    "    'sample_data.csv'\n",
    "    , header=True\n",
    "    , inferSchema = True\n",
    ")\n",
    "\n",
    "sample_df_inferred.printSchema()\n",
    "sample_df_inferred.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, to_date\n",
    "#sample_df_inferred['OrderDate']=sample_df_inferred.select(to_date(sample_df_inferred.OrderDate, 'dd-MM-yyyy'))\n",
    "#sample_df_inferred= sample_df_inferred.withColumn('OrderDate1',to_date(sample_df_inferred.OrderDate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+------+-----+--------+------+------------------+\n",
      "|OrderDate| Region|   Rep|  Item|Units|UnitCost| Total|date_in_dateFormat|\n",
      "+---------+-------+------+------+-----+--------+------+------------------+\n",
      "| 1/6/2016|   East| Jones|Pencil|   95|    1.99|189.05|              null|\n",
      "| 3/2/2017|Central|Kivell|Binder|   50|   19.99| 999.5|              null|\n",
      "+---------+-------+------+------+-----+--------+------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df_inferred = sample_df_inferred.withColumn('date_in_dateFormat', \n",
    "                   to_date(unix_timestamp(col('OrderDate'), 'dd-MM-yyyy').cast(\"timestamp\")))\n",
    "sample_df_inferred.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+------+-----+--------+------+----------+\n",
      "|OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|OrderDate1|\n",
      "+---------+-------+-------+------+-----+--------+------+----------+\n",
      "|     null|   East|  Jones|Pencil|   95|    1.99|189.05|      null|\n",
      "|     null|Central| Kivell|Binder|   50|   19.99| 999.5|      null|\n",
      "|     null|Central|Jardine|Pencil|   36|    4.99|179.64|      null|\n",
      "|     null|Central|   Gill|   Pen|   27|   19.99|539.73|      null|\n",
      "|     null|   West|Sorvino|Pencil|   56|    2.99|167.44|      null|\n",
      "+---------+-------+-------+------+-----+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+------+-----+--------+------+\n",
      "|OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|\n",
      "+---------+-------+-------+------+-----+--------+------+\n",
      "|     null|   East|  Jones|Pencil|   95|    1.99|189.05|\n",
      "|     null|Central| Kivell|Binder|   50|   19.99| 999.5|\n",
      "|     null|Central|Jardine|Pencil|   36|    4.99|179.64|\n",
      "|     null|Central|   Gill|   Pen|   27|   19.99|539.73|\n",
      "+---------+-------+-------+------+-----+--------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "sample_df_inferred = (\n",
    "    sample_df_inferred\n",
    "    .withColumn('OrderDate'\n",
    "                , f.to_date('OrderDate', 'MM/dd/yy')\n",
    "               )\n",
    ")\n",
    "\n",
    "sample_df_inferred.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .agg(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        avg(Total)|\n",
      "+------------------+\n",
      "|456.46232558139553|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.agg({'Total': 'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+-----------------+\n",
      "|Total_min|Total_max|         Total_avg|     Total_stddev|\n",
      "+---------+---------+------------------+-----------------+\n",
      "|     9.03|  1879.06|456.46232558139553|447.0221038416717|\n",
      "+---------+---------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregations = [\n",
    "      ('Total', f.min,    'Total_min')\n",
    "    , ('Total', f.max,    'Total_max')\n",
    "    , ('Total', f.avg,    'Total_avg')\n",
    "    , ('Total', f.stddev, 'Total_stddev')\n",
    "]\n",
    "\n",
    "(sample_df_inferred.agg(*[e[1](e[0]).alias(e[2]) for e in aggregations]).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .sql(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+-----------------+\n",
      "|Total_min|Total_max|         Total_avg|        Total_std|\n",
      "+---------+---------+------------------+-----------------+\n",
      "|     9.03|  1879.06|456.46232558139553|447.0221038416717|\n",
      "+---------+---------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.createOrReplaceTempView('sample_df_inferred')\n",
    "\n",
    "spark.sql('''SELECT \n",
    "              MIN(Total)    AS Total_min\n",
    "            , MAX(Total)    AS Total_max\n",
    "            , AVG(Total)    AS Total_avg\n",
    "            , STDDEV(Total) AS Total_std\n",
    "            FROM sample_df_inferred''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+---------+---------+\n",
      "|     Rep|Total_min|Total_max|Total_avg|Total_std|\n",
      "+--------+---------+---------+---------+---------+\n",
      "|  Parent|   299.85|  1619.19|   1034.1|    672.2|\n",
      "|  Kivell|   479.04|   1005.9|   777.36|   266.95|\n",
      "|Thompson|    63.68|  1139.43|   601.56|   760.67|\n",
      "| Jardine|    54.89|  1879.06|   562.44|   749.73|\n",
      "|   Smith|    86.43|   1305.0|   547.14|    661.4|\n",
      "|  Morgan|   251.72|   686.95|   462.59|   217.93|\n",
      "|    Gill|     9.03|    719.2|   349.97|   304.93|\n",
      "| Sorvino|   139.93|    825.0|    320.9|   336.25|\n",
      "|   Jones|    19.96|   575.36|   295.38|   185.72|\n",
      "|  Howard|    57.71|   479.04|   268.38|   297.93|\n",
      "| Andrews|    18.06|   149.25|   109.59|    61.46|\n",
      "+--------+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.createOrReplaceTempView('sample_df_inferred')\n",
    "\n",
    "spark.sql('''SELECT\n",
    "              Rep\n",
    "            , MIN(Total) AS Total_min\n",
    "            , MAX(Total) AS Total_max\n",
    "            , ROUND(AVG(Total),2) AS Total_avg\n",
    "            , ROUND(STDDEV(Total),2) AS Total_std\n",
    "            FROM sample_df_inferred\n",
    "            GROUP BY Rep\n",
    "            ORDER BY Total_avg DESC''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|Total_min|Total_max|\n",
      "+---------+---------+\n",
      "|     9.03|  1879.06|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(sample_df_inferred.selectExpr(\n",
    "          'MIN(Total) AS Total_min'\n",
    "        , 'MAX(Total) AS Total_max'\n",
    "    )).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Temporary views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_df_inferred.createTempView('sample_df_inferred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_inferred.createOrReplaceTempView('sample_df_inferred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "| Region|Headquarters|\n",
      "+-------+------------+\n",
      "|Central|     Chicago|\n",
      "|   West|     Seattle|\n",
      "|   East|      Boston|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regions = spark.createDataFrame(\n",
    "    sc.parallelize([\n",
    "        ('Central', 'Chicago')\n",
    "        , ('West', 'Seattle')\n",
    "        , ('East', 'Boston')\n",
    "    ]),\n",
    "    ['Region', 'Headquarters']\n",
    ")\n",
    "regions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+------+-----+--------+------+\n",
      "|OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|\n",
      "+---------+-------+-------+------+-----+--------+------+\n",
      "|     null|   East|  Jones|Pencil|   95|    1.99|189.05|\n",
      "|     null|Central| Kivell|Binder|   50|   19.99| 999.5|\n",
      "|     null|Central|Jardine|Pencil|   36|    4.99|179.64|\n",
      "|     null|Central|   Gill|   Pen|   27|   19.99|539.73|\n",
      "|     null|   West|Sorvino|Pencil|   56|    2.99|167.44|\n",
      "+---------+-------+-------+------+-----+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+------+-----+--------+------+------------+\n",
      "| Region|OrderDate|     Rep|  Item|Units|UnitCost| Total|Headquarters|\n",
      "+-------+---------+--------+------+-----+--------+------+------------+\n",
      "|Central|     null|    Gill|Pencil|   53|    1.29| 68.37|     Chicago|\n",
      "|Central|     null|    Gill|Pencil|    7|    1.29|  9.03|     Chicago|\n",
      "|Central|     null|   Smith|Pencil|   67|    1.29| 86.43|     Chicago|\n",
      "|Central|     null| Andrews|Pencil|   14|    1.29| 18.06|     Chicago|\n",
      "|Central|     null| Andrews|Pencil|   66|    1.99|131.34|     Chicago|\n",
      "|   West|     null|Thompson|Pencil|   32|    1.99| 63.68|     Seattle|\n",
      "|   West|     null| Sorvino|   Pen|   76|    1.99|151.24|     Seattle|\n",
      "|   East|     null|   Jones|Pencil|   95|    1.99|189.05|      Boston|\n",
      "|Central|     null| Andrews|Pencil|   75|    1.99|149.25|     Chicago|\n",
      "|   East|     null|  Howard|Binder|   29|    1.99| 57.71|      Boston|\n",
      "+-------+---------+--------+------+-----+--------+------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.join(regions, on=['Region'], how='left_outer').orderBy('UnitCost').show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+------+------------------+------------------+------------------+\n",
      "|summary| Region|     Rep|  Item|             Units|          UnitCost|             Total|\n",
      "+-------+-------+--------+------+------------------+------------------+------------------+\n",
      "|  count|     43|      43|    43|                43|                43|                43|\n",
      "|   mean|   null|    null|  null|49.325581395348834|20.308604651162792|456.46232558139553|\n",
      "| stddev|   null|    null|  null|30.078247899067208| 47.34511769375187| 447.0221038416717|\n",
      "|    min|Central| Andrews|Binder|                 2|              1.29|              9.03|\n",
      "|    max|   West|Thompson|Pencil|                96|             275.0|           1879.06|\n",
      "+-------+-------+--------+------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Not for Null and DateTypem Columns\n",
    "sample_df_inferred.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Numeric Columns Only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|             Units|          UnitCost|             Total|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|                43|                43|                43|\n",
      "|   mean|49.325581395348834|20.308604651162792|456.46232558139553|\n",
      "| stddev|30.078247899067208| 47.34511769375187| 447.0221038416717|\n",
      "|    min|                 2|              1.29|              9.03|\n",
      "|    max|                96|             275.0|           1879.06|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = [e[0] \n",
    "         for e in sample_df_inferred.dtypes \n",
    "         if e[1] in ('int', 'double')\n",
    "        ]\n",
    "\n",
    "(sample_df_inferred.select(numeric_columns).describe().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|        mean_Units|     mean_UnitCost|        mean_Total|      stddev_Units|  stddev_UnitCost|     stddev_Total|\n",
      "+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|49.325581395348834|20.308604651162792|456.46232558139553|30.078247899067208|47.34511769375187|447.0221038416717|\n",
      "+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.agg(*\n",
    "    [f.mean(f.col(e)).alias('mean_' + e) for e in numeric_columns] +\n",
    "    [f.stddev(f.col(e)).alias('stddev_' + e) for e in numeric_columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|   kurtosis_Total|    skewness_Total|\n",
      "+-----------------+------------------+\n",
      "|1.551174226609806|1.4391370583659786|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.agg(*[f.kurtosis(f.col('Total')).alias('kurtosis_Total'),\n",
    "                        f.skewness(f.col('Total')).alias('skewness_Total')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+------+-----+--------+------+\n",
      "|OrderDate| Region|    Rep|  Item|Units|UnitCost| Total|\n",
      "+---------+-------+-------+------+-----+--------+------+\n",
      "|     null|   East|  Jones|Pencil|   95|    1.99|189.05|\n",
      "|     null|Central| Kivell|Binder|   50|   19.99| 999.5|\n",
      "|     null|Central|Jardine|Pencil|   36|    4.99|179.64|\n",
      "|     null|Central|   Gill|   Pen|   27|   19.99|539.73|\n",
      "+---------+-------+-------+------+-----+--------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Returns all records that are unique\n",
    "sample_df_inferred.distinct().show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| Region|\n",
      "+-------+\n",
      "|   East|\n",
      "|Central|\n",
      "|   West|\n",
      "+-------+\n",
      "\n",
      "+--------+\n",
      "|     Rep|\n",
      "+--------+\n",
      "|   Jones|\n",
      "|  Kivell|\n",
      "| Jardine|\n",
      "|    Gill|\n",
      "| Sorvino|\n",
      "| Andrews|\n",
      "|Thompson|\n",
      "|  Morgan|\n",
      "|  Howard|\n",
      "|  Parent|\n",
      "|   Smith|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df_inferred.select('Region').distinct().show()\n",
    "sample_df_inferred.select('Rep').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "| Region|     Rep|\n",
      "+-------+--------+\n",
      "|Central| Andrews|\n",
      "|Central|    Gill|\n",
      "|Central| Jardine|\n",
      "|Central|  Kivell|\n",
      "|Central|  Morgan|\n",
      "|Central|   Smith|\n",
      "|   East|  Howard|\n",
      "|   East|   Jones|\n",
      "|   East|  Parent|\n",
      "|   West| Sorvino|\n",
      "|   West|Thompson|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Combine\n",
    "(\n",
    "    sample_df_inferred\n",
    "    .select('Region', 'Rep')\n",
    "    .distinct()\n",
    "    .orderBy('Region', 'Rep')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
