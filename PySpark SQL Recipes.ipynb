{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark SQL Recipes With HiveQL, Dataframe and Graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"create\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmerDf = spark.read.csv('swimmerData.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Occupation: string (nullable = true)\n",
      " |-- swimTimeInSecond: double (nullable = true)\n",
      " |-- Speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+----------------+-----+\n",
      "| id|Gender| Occupation|swimTimeInSecond|Speed|\n",
      "+---+------+-----------+----------------+-----+\n",
      "|id1|  Male| programmer|           16.73|1.195|\n",
      "|id2|Female|    Manager|           15.56|1.285|\n",
      "|id3|  Male|    Manager|           15.15| 1.32|\n",
      "|id4|  Male|riskanalyst|           15.27| 1.31|\n",
      "+---+------+-----------+----------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDf.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the Schema of the DataFrame**\n",
    "\n",
    "1. Let’s look at the arguments of StructField(). The first argument is the column name. We provide the column name as id.\n",
    "2. The second argument is the datatype of the elements of the column. The datatype of the first column is StringType(). If some ID is missing then some element of a column might be null.\n",
    "3. The last argument, whose value is True, tells you that this column might have null values or missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,StringType,true),StructField(Gender,StringType,true),StructField(Occupation,StringType,true),StructField(swimTimeInSecond,DoubleType,true),StructField(Speed,DoubleType,true)))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idColumn = StructField(\"id\",StringType(),True)\n",
    "genderColumn = StructField(\"Gender\",StringType(),True)\n",
    "OccupationColumn = StructField(\"Occupation\",StringType(),True)\n",
    "swimTimeInSecondColumn = StructField(\"swimTimeInSecond\",DoubleType(),True)\n",
    "speed = StructField(\"Speed\",DoubleType(),True)\n",
    "columnList = [idColumn, genderColumn, OccupationColumn,swimTimeInSecondColumn,speed]\n",
    "swimmerDfSchema = StructType(columnList)\n",
    "swimmerDfSchema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Occupation: string (nullable = true)\n",
      " |-- swimTimeInSecond: double (nullable = true)\n",
      " |-- Speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDf = spark.read.csv('swimmerData.csv',header=True,schema=swimmerDfSchema)\n",
    "swimmerDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+----------------+-----+------------------+\n",
      "| id|Gender| Occupation|swimTimeInSecond|Speed|      swimmerSpeed|\n",
      "+---+------+-----------+----------------+-----+------------------+\n",
      "|id1|  Male| programmer|           16.73|1.195| 1.195457262402869|\n",
      "|id2|Female|    Manager|           15.56|1.285|1.2853470437017995|\n",
      "|id3|  Male|    Manager|           15.15| 1.32|1.3201320132013201|\n",
      "|id4|  Male|riskanalyst|           15.27| 1.31| 1.309757694826457|\n",
      "+---+------+-----------+----------------+-----+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDf1 = swimmerDf.withColumn('swimmerSpeed',20.0/swimmerDf.swimTimeInSecond)\n",
    "swimmerDf1.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+----------------+-----+------------+\n",
      "| id|Gender| Occupation|swimTimeInSecond|Speed|swimmerSpeed|\n",
      "+---+------+-----------+----------------+-----+------------+\n",
      "|id1|  Male| programmer|           16.73|1.195|       1.195|\n",
      "|id2|Female|    Manager|           15.56|1.285|       1.285|\n",
      "|id3|  Male|    Manager|           15.15| 1.32|        1.32|\n",
      "|id4|  Male|riskanalyst|           15.27| 1.31|        1.31|\n",
      "|id5|  Male| programmer|           15.65|1.278|       1.278|\n",
      "+---+------+-----------+----------------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "swimmerDf2 = swimmerDf1.withColumn('swimmerSpeed',\n",
    "round(swimmerDf1.swimmerSpeed, 3))\n",
    "\n",
    "swimmerDf2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|swimTimeInSecond|\n",
      "+----------------+\n",
      "|           16.73|\n",
      "|           15.56|\n",
      "|           15.15|\n",
      "|           15.27|\n",
      "+----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDf3 = swimmerDf2.select(\"swimTimeInSecond\")\n",
    "swimmerDf3.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|swimmerSpeed|\n",
      "+---+------------+\n",
      "|id1|       1.195|\n",
      "|id2|       1.285|\n",
      "|id3|        1.32|\n",
      "+---+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDf4 = swimmerDf2.select(\"id\",\"swimmerSpeed\")\n",
    "swimmerDf4.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'Gender', 'Occupation', 'swimTimeInSecond', 'Speed', 'swimmerSpeed']\n",
      "\n",
      "\n",
      "+---+------+----------+----------------+-----+------------+\n",
      "| id|Gender|Occupation|swimTimeInSecond|Speed|swimmerSpeed|\n",
      "+---+------+----------+----------------+-----+------------+\n",
      "|id1|  Male|programmer|           16.73|1.195|       1.195|\n",
      "|id2|Female|   Manager|           15.56|1.285|       1.285|\n",
      "+---+------+----------+----------------+-----+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(swimmerDf2.columns)\n",
    "print('\\n')\n",
    "swimmerDf2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|(swimmerSpeed * 2)|\n",
      "+---+------------------+\n",
      "|id1|              2.39|\n",
      "|id2|              2.57|\n",
      "|id3|              2.64|\n",
      "+---+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDf5 = swimmerDf2.select(\"id\",swimmerDf2.swimmerSpeed*2)\n",
    "swimmerDf5.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+----------------+-----+------------+\n",
      "|  id|Gender| Occupation|swimTimeInSecond|Speed|swimmerSpeed|\n",
      "+----+------+-----------+----------------+-----+------------+\n",
      "| id1|  Male| programmer|           16.73|1.195|       1.195|\n",
      "| id3|  Male|    Manager|           15.15| 1.32|        1.32|\n",
      "| id4|  Male|riskanalyst|           15.27| 1.31|        1.31|\n",
      "| id5|  Male| programmer|           15.65|1.278|       1.278|\n",
      "| id6|  Male|riskanalyst|           15.74|1.271|       1.271|\n",
      "| id8|  Male|    Manager|           17.11|1.169|       1.169|\n",
      "|id11|  Male| programmer|           15.96|1.253|       1.253|\n",
      "+----+------+-----------+----------------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDf3 = swimmerDf2.filter(swimmerDf2.Gender == 'Male')\n",
    "swimmerDf3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----------+----------------+-----+\n",
      "|  id|Gender|Occupation|swimTimeInSecond|Speed|\n",
      "+----+------+----------+----------------+-----+\n",
      "| id1|  Male|programmer|           16.73|1.195|\n",
      "| id5|  Male|programmer|           15.65|1.278|\n",
      "|id11|  Male|programmer|           15.96|1.253|\n",
      "+----+------+----------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDf4 = swimmerDf.filter((swimmerDf.Gender =='Male') & (swimmerDf.Occupation == 'programmer'))\n",
    "\n",
    "swimmerDf4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----------+----------------+-----+------------+\n",
      "|  id|Gender|Occupation|swimTimeInSecond|Speed|swimmerSpeed|\n",
      "+----+------+----------+----------------+-----+------------+\n",
      "| id1|  Male|programmer|           16.73|1.195|       1.195|\n",
      "| id5|  Male|programmer|           15.65|1.278|       1.278|\n",
      "| id7|Female|programmer|            16.8| 1.19|        1.19|\n",
      "| id9|Female|programmer|           16.83|1.188|       1.188|\n",
      "|id11|  Male|programmer|           15.96|1.253|       1.253|\n",
      "+----+------+----------+----------------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDf2.filter((swimmerDf2.Occupation ==\n",
    "'programmer') & (swimmerDf2.swimmerSpeed > 1.17) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------------+-----+------------+\n",
      "|Gender| Occupation|swimTimeInSecond|Speed|swimmerSpeed|\n",
      "+------+-----------+----------------+-----+------------+\n",
      "|  Male| programmer|           16.73|1.195|       1.195|\n",
      "|Female|    Manager|           15.56|1.285|       1.285|\n",
      "|  Male|    Manager|           15.15| 1.32|        1.32|\n",
      "|  Male|riskanalyst|           15.27| 1.31|        1.31|\n",
      "|  Male| programmer|           15.65|1.278|       1.278|\n",
      "|  Male|riskanalyst|           15.74|1.271|       1.271|\n",
      "+------+-----------+----------------+-----+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDf3 = swimmerDf2.drop(swimmerDf2.id)\n",
    "swimmerDf3.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+-----+------------+\n",
      "|Gender|swimTimeInSecond|Speed|swimmerSpeed|\n",
      "+------+----------------+-----+------------+\n",
      "|  Male|           16.73|1.195|       1.195|\n",
      "|Female|           15.56|1.285|       1.285|\n",
      "|  Male|           15.15| 1.32|        1.32|\n",
      "|  Male|           15.27| 1.31|        1.31|\n",
      "|  Male|           15.65|1.278|       1.278|\n",
      "|  Male|           15.74|1.271|       1.271|\n",
      "+------+----------------+-----+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDf5 = swimmerDf2.drop(\"id\", \"Occupation\")\n",
    "swimmerDf5.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempInCelsius = StructField(\"tempInCelsius\",DoubleType(),True)\n",
    "time = StructField(\"Time\",StringType(),True)\n",
    "columnList = [time,tempInCelsius ]\n",
    "schema = StructType(columnList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|Time|tempInCelsius|\n",
      "+----+-------------+\n",
      "| 6AM|         15.0|\n",
      "| 8AM|         16.0|\n",
      "|10AM|         17.0|\n",
      "+----+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- Time: string (nullable = true)\n",
      " |-- tempInCelsius: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDf = spark.read.json('tempData.json', schema=schema)\n",
    "tempDf.show(3)\n",
    "tempDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the udf() function has taken the name of the function in String format as its first argument and the return type of the UDF as the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def celsiustoFahrenheit(temp):\n",
    "    return ((temp*9.0/5.0)+32)\n",
    "\n",
    "celsiustoFahrenheitUdf = udf(celsiustoFahrenheit,DoubleType())\n",
    "\n",
    "celsiustoFahrenheit(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The required UDF has been created. So, we’ll now use this UDF to\n",
    "transform the temperature from Celsius to Fahrenheit and add the result as a new column.\n",
    "* We are going to use the withColumn() function with a second\n",
    "argument as a UDF and with tempInCelsius as the input to the UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+----------------+\n",
      "|Time|tempInCelsius|tempInFahrenheit|\n",
      "+----+-------------+----------------+\n",
      "| 6AM|         15.0|            59.0|\n",
      "| 8AM|         16.0|            60.8|\n",
      "|10AM|         17.0|            62.6|\n",
      "|12AM|         17.0|            62.6|\n",
      "| 2PM|         18.0|            64.4|\n",
      "| 4PM|         17.0|            62.6|\n",
      "| 6PM|         16.0|            60.8|\n",
      "| 8PM|         14.0|            57.2|\n",
      "+----+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDfFahrenheit = tempDf.withColumn('tempInFahrenheit',celsiustoFahrenheitUdf(tempDf.tempInCelsius))\n",
    "tempDfFahrenheit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def labelTemprature(temp) :\n",
    "    if temp > 15 :\n",
    "        return \"High\"\n",
    "    else :\n",
    "        return \"Low\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Low', 'High')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelTemprature(14), labelTemprature(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelTempratureUdf = udf(labelTemprature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-----+\n",
      "|Time|tempInCelsius|label|\n",
      "+----+-------------+-----+\n",
      "| 6AM|         15.0|  Low|\n",
      "| 8AM|         16.0| High|\n",
      "|10AM|         17.0| High|\n",
      "|12AM|         17.0| High|\n",
      "| 2PM|         18.0| High|\n",
      "| 4PM|         17.0| High|\n",
      "| 6PM|         16.0| High|\n",
      "| 8PM|         14.0|  Low|\n",
      "+----+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDf2 = tempDf.withColumn(\"label\",labelTempratureUdf(tempDf.tempInCelsius))\n",
    "tempDf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "| iv1| iv2|  iv3|\n",
      "+----+----+-----+\n",
      "| 5.5| 8.5|  9.5|\n",
      "|6.13|9.13|10.13|\n",
      "|5.92|8.92| 9.92|\n",
      "|6.89|9.89|10.89|\n",
      "|6.12|9.12|10.12|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corrData = spark.read.json(path='corrData.json')\n",
    "corrData.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- iv1: double (nullable = true)\n",
      " |-- iv2: double (nullable = true)\n",
      " |-- iv3: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corrData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|avg(iv2)|avg(iv1)|avg(iv3)|\n",
      "+--------+--------+--------+\n",
      "|   9.112|   6.112|  10.112|\n",
      "+--------+--------+--------+\n",
      "\n",
      "+------------------+-------------+------------------+\n",
      "|     var_samp(iv2)|var_samp(iv1)|     var_samp(iv3)|\n",
      "+------------------+-------------+------------------+\n",
      "|0.2542699999999997|      0.25427|0.2542699999999997|\n",
      "+------------------+-------------+------------------+\n",
      "\n",
      "+-------------------+-------------------+-------------------+\n",
      "|       var_pop(iv2)|       var_pop(iv1)|       var_pop(iv3)|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|0.20341599999999976|0.20341599999999999|0.20341599999999976|\n",
      "+-------------------+-------------------+-------------------+\n",
      "\n",
      "+----------+----------+----------+\n",
      "|count(iv2)|count(iv1)|count(iv3)|\n",
      "+----------+----------+----------+\n",
      "|         5|         5|         5|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now we calculate the population variance:\n",
    "meanVal = corrData.agg({\"iv1\":\"avg\",\"iv2\":\"avg\",\"iv3\":\"avg\"})\n",
    "meanVal.show()\n",
    "\n",
    "#Calculating the Variance of Each Column\n",
    "varSampleVal = corrData.agg({\"iv1\":\"var_samp\",\"iv2\":\"var_samp\",\"iv3\":\"var_samp\"})\n",
    "varSampleVal.show()\n",
    "\n",
    "#Now we calculate the population variance:\n",
    "varPopulation = corrData.agg({\"iv1\":\"var_pop\",\"iv2\":\"var_pop\",\"iv3\":\"var_pop\"})\n",
    "varPopulation.show()\n",
    "\n",
    "#Counting the Number of Data Points in Each Column\n",
    "countVal = corrData.agg({\"iv1\":\"count\",\"iv2\":\"count\",\"iv3\":\"count\"})\n",
    "countVal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+--------+\n",
      "|summary|avg(iv2)|avg(iv1)|avg(iv3)|\n",
      "+-------+--------+--------+--------+\n",
      "|  count|       1|       1|       1|\n",
      "|   mean|   9.112|   6.112|  10.112|\n",
      "| stddev|     NaN|     NaN|     NaN|\n",
      "|    min|   9.112|   6.112|  10.112|\n",
      "|    max|   9.112|   6.112|  10.112|\n",
      "+-------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meanVal.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|  stddev_samp(iv1)|\n",
      "+------------------+\n",
      "|0.5042519211663947|\n",
      "+------------------+\n",
      "\n",
      "+--------+--------+------------------+\n",
      "|sum(iv1)|avg(iv1)|  stddev_samp(iv1)|\n",
      "+--------+--------+------------------+\n",
      "|   30.56|   6.112|0.5042519211663947|\n",
      "+--------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculating Summation, Mean, and Standard Deviation on the First Column\n",
    "moreAggOnOneCol = corrData.agg({\"iv1\":\"sum\",\"iv1\":\"avg\", \"iv1\":\"stddev_samp\"})\n",
    "moreAggOnOneCol.show()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "moreAggOnOneCol = corrData.agg(sum(\"iv1\"), avg(\"iv1\"),stddev_samp(\"iv1\"))\n",
    "moreAggOnOneCol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+------------------+\n",
      "|avg(iv2)|var_samp(iv1)|  stddev_samp(iv3)|\n",
      "+--------+-------------+------------------+\n",
      "|   9.112|      0.25427|0.5042519211663945|\n",
      "+--------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Variance of the First Column, the Mean of the Second Column, and the Standard Deviation of the Third Column\n",
    "\n",
    "colWiseDiffAggregation = corrData.agg({\"iv1\":\"var_samp\",\"iv2\":\"avg\",\"iv3\":\"stddev_samp\"})\n",
    "colWiseDiffAggregation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2542699999999998\n",
      "0.2542699999999998\n"
     ]
    }
   ],
   "source": [
    "# Calculating Covariance Between Variables\n",
    "print(corrData.cov('iv1','iv2'))\n",
    "print(corrData.cov('iv1','iv3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999996\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#Calculating Correlation Between Variables\n",
    "print(corrData.corr('iv1','iv2'))\n",
    "print(corrData.corr('iv2','iv3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|               iv1|               iv2|               iv3|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|                 5|                 5|                 5|\n",
      "|   mean|             6.112|             9.112|            10.112|\n",
      "| stddev|0.5042519211663947|0.5042519211663945|0.5042519211663945|\n",
      "|    min|               5.5|               8.5|               9.5|\n",
      "|    max|              6.89|              9.89|             10.89|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying the describe( ) Function on Each Column\n",
    "dataDescription = corrData.describe()\n",
    "dataDescription.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|               iv1|               iv2|\n",
      "+-------+------------------+------------------+\n",
      "|  count|                 5|                 5|\n",
      "|   mean|             6.112|             9.112|\n",
      "| stddev|0.5042519211663947|0.5042519211663945|\n",
      "|    min|               5.5|               8.5|\n",
      "|    max|              6.89|              9.89|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying the describe( ) Function on Columns\n",
    "dataDescriptioniv1iv2 = corrData.describe(['iv1', 'iv2'])\n",
    "dataDescriptioniv1iv2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|               iv1|               iv2|               iv3|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|                 5|                 5|                 5|\n",
      "|   mean|             6.112|             9.112|            10.112|\n",
      "| stddev|0.5042519211663947|0.5042519211663945|0.5042519211663945|\n",
      "|    min|               5.5|               8.5|               9.5|\n",
      "|    25%|              5.92|              8.92|              9.92|\n",
      "|    50%|              6.12|              9.12|             10.12|\n",
      "|    75%|              6.13|              9.13|             10.13|\n",
      "|    max|              6.89|              9.89|             10.89|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying the summary( ) Function on Each Column\n",
    "summaryData = corrData.summary()\n",
    "summaryData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+------+\n",
      "|summary|  iv1|  iv2|   iv3|\n",
      "+-------+-----+-----+------+\n",
      "|   mean|6.112|9.112|10.112|\n",
      "|    max| 6.89| 9.89| 10.89|\n",
      "+-------+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using the summary() function, we can apply selective summary statistics\n",
    "summaryMeanMax = corrData.summary(['mean','max'])\n",
    "summaryMeanMax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+\n",
      "|summary| iv1| iv2|\n",
      "+-------+----+----+\n",
      "|    min| 5.5| 8.5|\n",
      "|    max|6.89|9.89|\n",
      "+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying the summary( ) Function on Columns\n",
    "summaryiv1iv2 = corrData.select('iv1','iv2').summary('min','max')\n",
    "summaryiv1iv2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a Column of Categorical Variables\n",
    "from pyspark.sql.functions import udf\n",
    "def labelIt(x):\n",
    "    if x > 10.0:\n",
    "        return 'High'\n",
    "    else:\n",
    "        return 'Low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIt = udf(labelIt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "| iv1| iv2|  iv3|\n",
      "+----+----+-----+\n",
      "| 5.5| 8.5|  9.5|\n",
      "|6.13|9.13|10.13|\n",
      "+----+----+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- iv1: double (nullable = true)\n",
      " |-- iv2: double (nullable = true)\n",
      " |-- iv3: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corrData.show(2)\n",
    "corrData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+----+\n",
      "| iv1| iv2|  iv3| iv4|\n",
      "+----+----+-----+----+\n",
      "| 5.5| 8.5|  9.5| Low|\n",
      "|6.13|9.13|10.13|High|\n",
      "|5.92|8.92| 9.92| Low|\n",
      "|6.89|9.89|10.89|High|\n",
      "|6.12|9.12|10.12|High|\n",
      "+----+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corrData1 = corrData.withColumn('iv4', labelIt('iv3'))\n",
    "corrData1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+------+----+\n",
      "|summary|  iv1|  iv2|   iv3| iv4|\n",
      "+-------+-----+-----+------+----+\n",
      "|   mean|6.112|9.112|10.112|null|\n",
      "|    max| 6.89| 9.89| 10.89| Low|\n",
      "+-------+-----+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meanMaxSummary = corrData1.summary('mean','max')\n",
    "meanMaxSummary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+-----+----+\n",
      "|summary| iv1| iv2|  iv3| iv4|\n",
      "+-------+----+----+-----+----+\n",
      "|  count|   5|   5|    5|   5|\n",
      "|    min| 5.5| 8.5|  9.5|High|\n",
      "|    max|6.89|9.89|10.89| Low|\n",
      "+-------+----+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countMinMaxSummary = corrData1.summary('count',\n",
    "'min','max')\n",
    "countMinMaxSummary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sort Data in a DataFrame Problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+----------------+-----+\n",
      "| id|Gender| Occupation|swimTimeInSecond|Speed|\n",
      "+---+------+-----------+----------------+-----+\n",
      "|id1|  Male| programmer|           16.73|1.195|\n",
      "|id2|Female|    Manager|           15.56|1.285|\n",
      "|id3|  Male|    Manager|           15.15| 1.32|\n",
      "|id4|  Male|riskanalyst|           15.27| 1.31|\n",
      "+---+------+-----------+----------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDf.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+----------------+-----+\n",
      "|  id|Gender| Occupation|swimTimeInSecond|Speed|\n",
      "+----+------+-----------+----------------+-----+\n",
      "| id3|  Male|    Manager|           15.15| 1.32|\n",
      "| id4|  Male|riskanalyst|           15.27| 1.31|\n",
      "| id2|Female|    Manager|           15.56|1.285|\n",
      "| id5|  Male| programmer|           15.65|1.278|\n",
      "| id6|  Male|riskanalyst|           15.74|1.271|\n",
      "|id12|Female|riskanalyst|            15.9|1.258|\n",
      "+----+------+-----------+----------------+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDfSorted1 = swimmerDf.orderBy(\"swimTimeInSecond\",ascending=True)\n",
    "swimmerDfSorted1.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+----------------+-----+\n",
      "|  id|Gender| Occupation|swimTimeInSecond|Speed|\n",
      "+----+------+-----------+----------------+-----+\n",
      "| id8|  Male|    Manager|           17.11|1.169|\n",
      "| id9|Female| programmer|           16.83|1.188|\n",
      "| id7|Female| programmer|            16.8| 1.19|\n",
      "| id1|  Male| programmer|           16.73|1.195|\n",
      "|id10|Female|riskanalyst|           16.34|1.224|\n",
      "|id11|  Male| programmer|           15.96|1.253|\n",
      "+----+------+-----------+----------------+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerDfSorted2 =swimmerDf.orderBy(\"swimTimeInSecond\",ascending=False)\n",
    "swimmerDfSorted2.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+----------------+-----+\n",
      "|  id|Gender| Occupation|swimTimeInSecond|Speed|\n",
      "+----+------+-----------+----------------+-----+\n",
      "| id4|  Male|riskanalyst|           15.27| 1.31|\n",
      "| id6|  Male|riskanalyst|           15.74|1.271|\n",
      "|id12|Female|riskanalyst|            15.9|1.258|\n",
      "|id10|Female|riskanalyst|           16.34|1.224|\n",
      "| id5|  Male| programmer|           15.65|1.278|\n",
      "|id11|  Male| programmer|           15.96|1.253|\n",
      "+----+------+-----------+----------------+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sorting on Two Columns in Different Order\n",
    "swimmerDfSorted3 = swimmerDf.orderBy(\"Occupation\",\"swimTimeInSecond\", ascending=[False,True])\n",
    "swimmerDfSorted3.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sort Data Partition-Wise Problem**\n",
    "\n",
    "DataFrames are partitioned over many nodes. Now we want to sort a DataFrame partition-wise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+----------------+-----+------------------+\n",
      "| id|Gender| Occupation|swimTimeInSecond|Speed|      swimmerSpeed|\n",
      "+---+------+-----------+----------------+-----+------------------+\n",
      "|id1|  Male| programmer|           16.73|1.195| 1.195457262402869|\n",
      "|id2|Female|    Manager|           15.56|1.285|1.2853470437017995|\n",
      "|id3|  Male|    Manager|           15.15| 1.32|1.3201320132013201|\n",
      "|id4|  Male|riskanalyst|           15.27| 1.31| 1.309757694826457|\n",
      "|id5|  Male| programmer|           15.65|1.278|1.2779552715654952|\n",
      "|id6|  Male|riskanalyst|           15.74|1.271|1.2706480304955527|\n",
      "+---+------+-----------+----------------+-----+------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sortedPartitons = swimmerDf.sortWithinPartitions(\"Occupation\",\"swimTimeInSecond\",\n",
    "                                                 ascending=[False,True])\n",
    "swimmerDf1.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swimmerDf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(id='id12', Gender='Female', Occupation='riskanalyst', swimTimeInSecond=15.9, Speed=1.258),\n",
       "  Row(id='id3', Gender='Male', Occupation='Manager', swimTimeInSecond=15.15, Speed=1.32),\n",
       "  Row(id='id6', Gender='Male', Occupation='riskanalyst', swimTimeInSecond=15.74, Speed=1.271),\n",
       "  Row(id='id1', Gender='Male', Occupation='programmer', swimTimeInSecond=16.73, Speed=1.195),\n",
       "  Row(id='id10', Gender='Female', Occupation='riskanalyst', swimTimeInSecond=16.34, Speed=1.224),\n",
       "  Row(id='id2', Gender='Female', Occupation='Manager', swimTimeInSecond=15.56, Speed=1.285)],\n",
       " [Row(id='id5', Gender='Male', Occupation='programmer', swimTimeInSecond=15.65, Speed=1.278),\n",
       "  Row(id='id4', Gender='Male', Occupation='riskanalyst', swimTimeInSecond=15.27, Speed=1.31),\n",
       "  Row(id='id8', Gender='Male', Occupation='Manager', swimTimeInSecond=17.11, Speed=1.169),\n",
       "  Row(id='id11', Gender='Male', Occupation='programmer', swimTimeInSecond=15.96, Speed=1.253),\n",
       "  Row(id='id7', Gender='Female', Occupation='programmer', swimTimeInSecond=16.8, Speed=1.19),\n",
       "  Row(id='id9', Gender='Female', Occupation='programmer', swimTimeInSecond=16.83, Speed=1.188)]]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swimmerDf1 = swimmerDf.repartition(2)\n",
    "swimmerDf1.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+----------------+-----+\n",
      "|  id|Gender| Occupation|swimTimeInSecond|Speed|\n",
      "+----+------+-----------+----------------+-----+\n",
      "| id6|  Male|riskanalyst|           15.74|1.271|\n",
      "|id12|Female|riskanalyst|            15.9|1.258|\n",
      "|id10|Female|riskanalyst|           16.34|1.224|\n",
      "| id1|  Male| programmer|           16.73|1.195|\n",
      "| id3|  Male|    Manager|           15.15| 1.32|\n",
      "+----+------+-----------+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sortedPartitons = swimmerDf1.sortWithinPartitions(\"Occupation\",\"swimTimeInSecond\", ascending=[False,True])\n",
    "sortedPartitons.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Duplicate Records from a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+-------+--------+--------------------+\n",
      "|Employee ID| Employee Name|Day Worked|Time In|Time Out|Report Download Date|\n",
      "+-----------+--------------+----------+-------+--------+--------------------+\n",
      "|       1816|Alyssa Reddall|10/21/2016| 7:21am|  5:00pm|          10/24/2016|\n",
      "|       1816|Alyssa Reddall|10/21/2016| 7:21am|  5:00pm|          10/25/2016|\n",
      "|       1816|Alyssa Reddall|10/22/2016| 8:03am|  5:12pm|          10/24/2016|\n",
      "|       1816|Alyssa Reddall|10/22/2016| 8:03am|  5:12pm|          10/25/2016|\n",
      "|       1719|  Carl Borunda|10/21/2016| 7:55am|  5:54pm|          10/24/2016|\n",
      "+-----------+--------------+----------+-------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duplicateDataDf = spark.read.csv(path='duplicateData.csv', inferSchema=True, header=True)\n",
    "duplicateDataDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicateDataDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noDuplicateDf1 = duplicateDataDf.drop_duplicates()\n",
    "noDuplicateDf1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "+-----------+-------------------+----------+-------+--------+--------------------+\n",
      "|Employee ID|      Employee Name|Day Worked|Time In|Time Out|Report Download Date|\n",
      "+-----------+-------------------+----------+-------+--------+--------------------+\n",
      "|       1816|     Alyssa Reddall|10/21/2016| 7:21am|  5:00pm|          10/24/2016|\n",
      "|       1719|       Carl Borunda|10/21/2016| 7:55am|  5:54pm|          10/24/2016|\n",
      "|       1651|     Carrie Richard|10/21/2016| 8:04am|  5:03pm|          10/24/2016|\n",
      "|       1740|         Eric Trent|10/21/2016| 8:10am|  5:03pm|          10/24/2016|\n",
      "|       1731|     Francis Turner|10/21/2016| 8:03am|  5:02pm|          10/24/2016|\n",
      "|       1618|    Jeanne Brunelle|10/21/2016| 8:05am|  5:54pm|          10/24/2016|\n",
      "|       1428|     Leslie Carlton|10/21/2016| 8:12am|  4:53pm|          10/24/2016|\n",
      "|       1041|     Linda Costigan|10/21/2016| 7:43am|  5:15pm|          10/24/2016|\n",
      "|       1088|       Marino Varga|10/21/2016| 7:55am|  5:00pm|          10/24/2016|\n",
      "|       1220|     Randell Harvey|10/21/2016| 8:03am|  5:15pm|          10/24/2016|\n",
      "|       1673|      Ryan Fontenot|10/21/2016| 8:12am|  5:12pm|          10/24/2016|\n",
      "|       1901|      Sabrina Hertz|10/21/2016| 7:44am|  5:12pm|          10/24/2016|\n",
      "|       1091|Stephanie Alexander|10/21/2016| 7:12am|  5:12pm|          10/24/2016|\n",
      "|       1209|Stephen Kirkpatrick|10/21/2016| 8:25am|  5:30pm|          10/24/2016|\n",
      "+-----------+-------------------+----------+-------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Removing the Duplicate Records Conditioned on column \n",
    "noDuplicateDf2 = duplicateDataDf.drop_duplicates(['Employee Name'])\n",
    "print(noDuplicateDf2.count())\n",
    "noDuplicateDf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "+-----------+-------------------+----------+-------+--------+--------------------+\n",
      "|Employee ID|      Employee Name|Day Worked|Time In|Time Out|Report Download Date|\n",
      "+-----------+-------------------+----------+-------+--------+--------------------+\n",
      "|       1041|     Linda Costigan|10/21/2016| 7:43am|  5:15pm|          10/24/2016|\n",
      "|       1088|       Marino Varga|10/21/2016| 7:55am|  5:00pm|          10/24/2016|\n",
      "|       1091|Stephanie Alexander|10/21/2016| 7:12am|  5:12pm|          10/24/2016|\n",
      "|       1209|Stephen Kirkpatrick|10/21/2016| 8:25am|  5:30pm|          10/24/2016|\n",
      "|       1220|     Randell Harvey|10/21/2016| 8:03am|  5:15pm|          10/24/2016|\n",
      "|       1428|     Leslie Carlton|10/21/2016| 8:12am|  4:53pm|          10/24/2016|\n",
      "|       1618|    Jeanne Brunelle|10/21/2016| 8:05am|  5:54pm|          10/24/2016|\n",
      "|       1651|     Carrie Richard|10/21/2016| 8:04am|  5:03pm|          10/24/2016|\n",
      "|       1673|      Ryan Fontenot|10/21/2016| 8:12am|  5:12pm|          10/24/2016|\n",
      "|       1719|       Carl Borunda|10/21/2016| 7:55am|  5:54pm|          10/24/2016|\n",
      "|       1731|     Francis Turner|10/21/2016| 8:03am|  5:02pm|          10/24/2016|\n",
      "|       1740|         Eric Trent|10/21/2016| 8:10am|  5:03pm|          10/24/2016|\n",
      "|       1816|     Alyssa Reddall|10/21/2016| 7:21am|  5:00pm|          10/24/2016|\n",
      "|       1901|      Sabrina Hertz|10/21/2016| 7:44am|  5:12pm|          10/24/2016|\n",
      "+-----------+-------------------+----------+-------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing the Duplicate Records Conditioned on two Columns \n",
    "noDuplicateDf3 = duplicateDataDf.drop_duplicates(['Employee ID','Employee Name'])\n",
    "print(noDuplicateDf3.count())\n",
    "noDuplicateDf3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling Data from the noDuplicateDf1 DataFrame Without Replacement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noDuplicateDf1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We are going to fetch 50% of the records as a sample without replacement.\n",
    "sampleWithoutKeyConsideration = noDuplicateDf1.sample(withReplacement=False, fraction=0.5, seed=200)\n",
    "sampleWithoutKeyConsideration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sampling Data from the noDuplicateDf1 DataFrame with Replacements\n",
    "sampleWithoutKeyConsideration1 = noDuplicateDf1.sample(withReplacement=True, fraction=0.5, seed=200)\n",
    "sampleWithoutKeyConsideration1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find Frequent Items**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|Employee Name_freqItems|\n",
      "+-----------------------+\n",
      "|   [Marino Varga, Ca...|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duplicateDataDf.freqItems(cols=['Employee Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------+\n",
      "|Employee ID_freqItems|Employee Name_freqItems|\n",
      "+---------------------+-----------------------+\n",
      "| [1088, 1091, 1719...|   [Marino Varga, Ca...|\n",
      "+---------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duplicateDataDf.freqItems(cols=['Employee ID','Employee Name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregate Data on a Single Key Problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+----+\n",
      "|   Admit|Gender|department|Freq|\n",
      "+--------+------+----------+----+\n",
      "|Admitted|  Male|         A| 512|\n",
      "|Rejected|  Male|         A| 313|\n",
      "|Admitted|Female|         A|  89|\n",
      "|Rejected|Female|         A|  19|\n",
      "|Admitted|  Male|         B| 353|\n",
      "+--------+------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ucbDataFrame = spark.read.csv(path='UCB_Admission.csv', inferSchema=True, header=True)\n",
    "ucbDataFrame.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|   admit|         avg(Freq)|\n",
      "+--------+------------------+\n",
      "|Admitted|            146.25|\n",
      "|Rejected|230.91666666666666|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculating the Required Means\n",
    "groupedOnAdmit = ucbDataFrame.groupby([\"admit\"]).mean()\n",
    "groupedOnAdmit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|gender|         avg(Freq)|\n",
      "+------+------------------+\n",
      "|  Male|            224.25|\n",
      "|Female|152.91666666666666|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping by Gender\n",
    "groupedOnGender = ucbDataFrame.groupby([\"gender\"]).mean()\n",
    "groupedOnGender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|department|avg(Freq)|\n",
      "+----------+---------+\n",
      "|         A|   233.25|\n",
      "|         B|   146.25|\n",
      "|         C|    229.5|\n",
      "|         D|    198.0|\n",
      "|         E|    146.0|\n",
      "|         F|    178.5|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding the Average Frequency of Application by Gender\n",
    "groupedOnDepartment = ucbDataFrame.groupby([\"department\"]).mean()\n",
    "groupedOnDepartment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------------------+\n",
      "|   admit|gender|         avg(Freq)|\n",
      "+--------+------+------------------+\n",
      "|Admitted|  Male|199.66666666666666|\n",
      "|Rejected|  Male|248.83333333333334|\n",
      "|Admitted|Female| 92.83333333333333|\n",
      "|Rejected|Female|             213.0|\n",
      "+--------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Aggregate Data on Multiple Keys Problem\n",
    "groupedOnAdmitGender =ucbDataFrame.groupby([\"admit\" ,\"gender\"]).mean()\n",
    "groupedOnAdmitGender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+\n",
      "|   admit|department|avg(Freq)|\n",
      "+--------+----------+---------+\n",
      "|Admitted|         A|    300.5|\n",
      "|Rejected|         A|    166.0|\n",
      "|Admitted|         B|    185.0|\n",
      "|Rejected|         B|    107.5|\n",
      "|Admitted|         C|    161.0|\n",
      "|Rejected|         C|    298.0|\n",
      "|Admitted|         D|    134.5|\n",
      "|Rejected|         D|    261.5|\n",
      "|Admitted|         E|     73.5|\n",
      "|Rejected|         E|    218.5|\n",
      "|Admitted|         F|     23.0|\n",
      "|Rejected|         F|    334.0|\n",
      "+--------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Grouping the Data on Department and Finding the Mean of Applications\n",
    "groupedOnAdmitDepartment = ucbDataFrame.groupby([\"admit\", \"department\"]).mean()\n",
    "groupedOnAdmitDepartment.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Contingency Table Problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+--------------------+-------------+\n",
      "|  borough|   cuisine|              grades|                name|restaurant_id|\n",
      "+---------+----------+--------------------+--------------------+-------------+\n",
      "|    Bronx|    Bakery|[[[1393804800000]...|Morris Park Bake ...|     30075445|\n",
      "| Brooklyn|Hamburgers|[[[1419897600000]...|             Wendy'S|     30112340|\n",
      "|Manhattan|     Irish|[[[1409961600000]...|Dj Reynolds Pub A...|     30191841|\n",
      "| Brooklyn| American |[[[1402358400000]...|     Riviera Caterer|     40356018|\n",
      "+---------+----------+--------------------+--------------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "surveyDf = spark.read.json(path='restaurants.json')\n",
    "surveyDf = surveyDf.drop(\"address\")\n",
    "surveyDf.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+-----------+----+-----------+-----+-------+-----+\n",
      "|   D|   H|      HDD|      Model| RAM| ScreenSize|    W| Weight| Year|\n",
      "+----+----+---------+-----------+----+-----------+-----+-------+-----+\n",
      "|9.48|0.61|512GB SSD|MacBook Pro|16GB|        15\"|13.75|   4.02| 2015|\n",
      "|7.74|0.52|256GB SSD|    MacBook| 8GB|        12\"|11.04|   2.03| 2016|\n",
      "|8.94|0.68|128GB SSD|MacBook Air| 8GB|      13.3\"| 12.8|   2.96| 2016|\n",
      "| 8.0|20.3|  1TB SSD|       iMac|64GB|        27\"| 25.6|   20.8| 2017|\n",
      "| 8.5|20.9|  1TB SSD|       iMac|64GB|        27\"| 25.2|   20.8| 2017|\n",
      "|9.48|0.61|512GB SSD|MacBook Pro|16GB|        15\"|13.75|   4.02| 2015|\n",
      "+----+----+---------+-----------+----+-----------+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "surveyDf = spark.read.json(path='DataFrames_sample.json')\n",
    "surveyDf = surveyDf.drop(\"Id\")\n",
    "surveyDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' D', ' H', ' HDD', ' Model', ' RAM', ' ScreenSize', ' W', ' Weight', ' Year']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surveyDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+-----+---+---+\n",
      "| Model_ ScreenSize|12\"|13.3\"|15\"|27\"|\n",
      "+------------------+---+-----+---+---+\n",
      "|       MacBook Air|  0|    1|  0|  0|\n",
      "|           MacBook|  1|    0|  0|  0|\n",
      "|       MacBook Pro|  0|    0|  2|  0|\n",
      "|              iMac|  0|    0|  0|  2|\n",
      "+------------------+---+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "surveyDf.crosstab(\" Model\",\" ScreenSize\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Joining Operations on Two DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "studentsDf = spark.read.csv('studentData.csv',header=True, inferSchema=True)\n",
    "subjectsDf = spark.read.csv('subjects.csv',header=True, inferSchema=True)\n",
    "subjectsDf = subjectsDf.drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+\n",
      "|studentid|   name|gender|\n",
      "+---------+-------+------+\n",
      "|      si1|  Robin|     M|\n",
      "|      si2|  Maria|     F|\n",
      "|      si3|  Julie|     F|\n",
      "|      si4|    Bob|     M|\n",
      "|      si6|William|     M|\n",
      "+---------+-------+------+\n",
      "\n",
      "+-----+---------+--------+\n",
      "|marks|studentid|subjects|\n",
      "+-----+---------+--------+\n",
      "|   78|      si4|     C++|\n",
      "|   83|      si2|    Java|\n",
      "|   72|      si3|    Ruby|\n",
      "|   85|      si2|  Python|\n",
      "|   77|      si5|     C++|\n",
      "|   75|      si1|  Python|\n",
      "|   84|      si4|  Python|\n",
      "|   76|      si3|    Java|\n",
      "|   81|      si1|    Java|\n",
      "+-----+---------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studentsDf.show(),subjectsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+-----+---------+--------+\n",
      "|studentid| name|gender|marks|studentid|subjects|\n",
      "+---------+-----+------+-----+---------+--------+\n",
      "|      si4|  Bob|     M|   78|      si4|     C++|\n",
      "|      si2|Maria|     F|   83|      si2|    Java|\n",
      "|      si3|Julie|     F|   72|      si3|    Ruby|\n",
      "|      si2|Maria|     F|   85|      si2|  Python|\n",
      "|      si1|Robin|     M|   75|      si1|  Python|\n",
      "|      si4|  Bob|     M|   84|      si4|  Python|\n",
      "|      si3|Julie|     F|   76|      si3|    Java|\n",
      "|      si1|Robin|     M|   81|      si1|    Java|\n",
      "+---------+-----+------+-----+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Performing an Inner Join on DataFrames\n",
    "innerDf = studentsDf.join(subjectsDf, studentsDf.studentid == subjectsDf.studentid, how= \"inner\")\n",
    "innerDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+-----+---------+--------+\n",
      "|studentid|   name|gender|marks|studentid|subjects|\n",
      "+---------+-------+------+-----+---------+--------+\n",
      "|      si1|  Robin|     M|   81|      si1|    Java|\n",
      "|      si1|  Robin|     M|   75|      si1|  Python|\n",
      "|      si2|  Maria|     F|   85|      si2|  Python|\n",
      "|      si2|  Maria|     F|   83|      si2|    Java|\n",
      "|      si3|  Julie|     F|   76|      si3|    Java|\n",
      "|      si3|  Julie|     F|   72|      si3|    Ruby|\n",
      "|      si4|    Bob|     M|   84|      si4|  Python|\n",
      "|      si4|    Bob|     M|   78|      si4|     C++|\n",
      "|      si6|William|     M| null|     null|    null|\n",
      "+---------+-------+------+-----+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing a Left Outer Join on DataFrames\n",
    "leftOuterDf = studentsDf.join(subjectsDf, studentsDf.studentid == subjectsDf.studentid, how= \"left\")\n",
    "leftOuterDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+-----+---------+--------+\n",
      "|studentid| name|gender|marks|studentid|subjects|\n",
      "+---------+-----+------+-----+---------+--------+\n",
      "|      si4|  Bob|     M|   78|      si4|     C++|\n",
      "|      si2|Maria|     F|   83|      si2|    Java|\n",
      "|      si3|Julie|     F|   72|      si3|    Ruby|\n",
      "|      si2|Maria|     F|   85|      si2|  Python|\n",
      "|     null| null|  null|   77|      si5|     C++|\n",
      "|      si1|Robin|     M|   75|      si1|  Python|\n",
      "|      si4|  Bob|     M|   84|      si4|  Python|\n",
      "|      si3|Julie|     F|   76|      si3|    Java|\n",
      "|      si1|Robin|     M|   81|      si1|    Java|\n",
      "+---------+-----+------+-----+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Performing a Right Outer Join on DataFrames\n",
    "rightOuterDf = studentsDf.join(subjectsDf, studentsDf.studentid == subjectsDf.studentid, how= \"right\")\n",
    "rightOuterDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+-----+---------+--------+\n",
      "|studentid|   name|gender|marks|studentid|subjects|\n",
      "+---------+-------+------+-----+---------+--------+\n",
      "|      si1|  Robin|     M|   75|      si1|  Python|\n",
      "|      si1|  Robin|     M|   81|      si1|    Java|\n",
      "|      si2|  Maria|     F|   83|      si2|    Java|\n",
      "|      si2|  Maria|     F|   85|      si2|  Python|\n",
      "|      si3|  Julie|     F|   72|      si3|    Ruby|\n",
      "|      si3|  Julie|     F|   76|      si3|    Java|\n",
      "|      si4|    Bob|     M|   78|      si4|     C++|\n",
      "|      si4|    Bob|     M|   84|      si4|  Python|\n",
      "|     null|   null|  null|   77|      si5|     C++|\n",
      "|      si6|William|     M| null|     null|    null|\n",
      "+---------+-------+------+-----+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Performing a Full Outer Join on DataFrames\n",
    "outerDf = studentsDf.join(subjectsDf, studentsDf.\n",
    "studentid == subjectsDf.studentid, how= \"outer\")\n",
    "outerDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vertically Stack Two DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "verticalDfOne = spark.read.csv('iv1.csv',header=True, inferSchema=True)\n",
    "verticalDfTwo = spark.read.csv('iv2.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|iv1|iv2|iv3|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  4|  5|  6|\n",
      "|  7|  8|  9|\n",
      "+---+---+---+\n",
      "\n",
      "+---+---+---+\n",
      "|iv1|iv2|iv3|\n",
      "+---+---+---+\n",
      "| 10| 11| 12|\n",
      "| 13| 14| 15|\n",
      "| 16| 17| 18|\n",
      "+---+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verticalDfOne.show(),verticalDfTwo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|iv1|iv2|iv3|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  4|  5|  6|\n",
      "|  7|  8|  9|\n",
      "| 10| 11| 12|\n",
      "| 13| 14| 15|\n",
      "| 16| 17| 18|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vstackedDf = verticalDfOne.union(verticalDfTwo)\n",
    "vstackedDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Missing Value Imputation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|  iv1|  iv2|  iv3|\n",
      "+-----+-----+-----+\n",
      "|  9.0|11.43|10.25|\n",
      "|10.23| null| 8.17|\n",
      "|10.26| 8.35| 9.94|\n",
      "| 9.84| null| null|\n",
      "|10.77|10.18|11.02|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missingDf  = spark.read.csv('missing.csv',header=True, inferSchema=True)\n",
    "missingDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- iv1: double (nullable = true)\n",
      " |-- iv2: double (nullable = true)\n",
      " |-- iv3: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missingDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingDf = missingDf.withColumn(\"iv2\", missingDf.\n",
    "iv2.cast(DoubleType())).withColumn(\"iv3\", missingDf.iv3.\n",
    "cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|  iv1|  iv2|  iv3|\n",
      "+-----+-----+-----+\n",
      "|  9.0|11.43|10.25|\n",
      "|10.26| 8.35| 9.94|\n",
      "|10.77|10.18|11.02|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dropping the Rows that Have Null Values\n",
    "missingDf.dropna(how ='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|  iv1|  iv2|  iv3|\n",
      "+-----+-----+-----+\n",
      "|  9.0|11.43|10.25|\n",
      "|10.23| null| 8.17|\n",
      "|10.26| 8.35| 9.94|\n",
      "| 9.84| null| null|\n",
      "|10.77|10.18|11.02|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Since all the values are not null, the all value of how won’t affect the DataFrame\n",
    "missingDf.dropna(how ='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dropping Rows that Have Null Values Using the thresh Argument \n",
    "2. If the thresh value is set to 2, any row containing less than two non-null\n",
    "values will be dropped. Only the fourth column has fewer than two nonnull values (it has only one), so it is the only row that will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|  iv1|  iv2|  iv3|\n",
      "+-----+-----+-----+\n",
      "|  9.0|11.43|10.25|\n",
      "|10.23| null| 8.17|\n",
      "|10.26| 8.35| 9.94|\n",
      "| 9.84| null| null|\n",
      "|10.77|10.18|11.02|\n",
      "+-----+-----+-----+\n",
      "\n",
      "+-----+-----+-----+\n",
      "|  iv1|  iv2|  iv3|\n",
      "+-----+-----+-----+\n",
      "|  9.0|11.43|10.25|\n",
      "|10.23| null| 8.17|\n",
      "|10.26| 8.35| 9.94|\n",
      "|10.77|10.18|11.02|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missingDf.show()\n",
    "missingDf.dropna(how ='all',thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|  iv1|  iv2|  iv3|\n",
      "+-----+-----+-----+\n",
      "|  9.0|11.43|10.25|\n",
      "|10.26| 8.35| 9.94|\n",
      "|10.77|10.18|11.02|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missingDf.dropna(how ='all',thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|  iv1|  iv2|  iv3|\n",
      "+-----+-----+-----+\n",
      "|  9.0|11.43|10.25|\n",
      "|10.23|  0.0| 8.17|\n",
      "|10.26| 8.35| 9.94|\n",
      "| 9.84|  0.0|  0.0|\n",
      "|10.77|10.18|11.02|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filling in the Missing Value with Some Number\n",
    "missingDf.fillna(value=0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Temp View from a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+\n",
      "|studentid|   name|gender|\n",
      "+---------+-------+------+\n",
      "|      si1|  Robin|     M|\n",
      "|      si2|  Maria|     F|\n",
      "|      si3|  Julie|     F|\n",
      "|      si4|    Bob|     M|\n",
      "|      si6|William|     M|\n",
      "+---------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "studentsDf.createOrReplaceTempView(\"Students\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.tableNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+\n",
      "|studentid|   name|gender|\n",
      "+---------+-------+------+\n",
      "|      si1|  Robin|     M|\n",
      "|      si2|  Maria|     F|\n",
      "|      si3|  Julie|     F|\n",
      "|      si4|    Bob|     M|\n",
      "|      si6|William|     M|\n",
      "+---------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputDf = spark.sql(\"select * from students\")\n",
    "outputDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+\n",
      "| col_name|data_type|comment|\n",
      "+---------+---------+-------+\n",
      "|studentid|   string|   null|\n",
      "|     name|   string|   null|\n",
      "|   gender|   string|   null|\n",
      "+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using Column Names in Spark SQL\n",
    "spark.sql(\"Describe students\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Sex|\n",
      "+-------+---+\n",
      "|  Robin|  M|\n",
      "|  Maria|  F|\n",
      "|  Julie|  F|\n",
      "|    Bob|  M|\n",
      "|William|  M|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating an Alias for Column Names\n",
    "spark.sql(\"select name as Name,gender as Sex from students\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|gender|\n",
      "+-----+------+\n",
      "|Maria|     F|\n",
      "|Julie|     F|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filtering Data Using a Where Clause\n",
    "spark.sql(\"select name ,gender from students\\\n",
    "          where gender ='F'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name|gender|\n",
      "+----+------+\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name ,gender from Students where Gender =\\\n",
    "'f'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|gender|\n",
      "+-----+------+\n",
      "|Maria|     F|\n",
      "|Julie|     F|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name ,gender from Students\\\n",
    "          where lower(gender) = 'f'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+-----------+-----+\n",
      "|studentid|   name|gender|dateofbirth|score|\n",
      "+---------+-------+------+-----------+-----+\n",
      "|      si1|  Robin|     M| 1981-09-06|   20|\n",
      "|      si2|  Maria|     F| 1986-06-06|   30|\n",
      "|      si3|  Julie|     F| 1988-09-05|   10|\n",
      "|      si4|    Bob|     M| 1987-05-04|   15|\n",
      "|      si6|William|     M| 1980-11-12|   25|\n",
      "+---------+-------+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Apply Spark UDF Methods on Spark SQL\n",
    "studentsDf = spark.read.csv('studentData.csv',header=True, inferSchema=True)\n",
    "studentsDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representing a String Value as a Date Value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|  studentid|   string|   null|\n",
      "|       name|   string|   null|\n",
      "|     gender|   string|   null|\n",
      "|dateofbirth|   string|   null|\n",
      "|      score|      int|   null|\n",
      "+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentsDf.createOrReplaceTempView(\"Students\")\n",
    "spark.sql(\"Describe students\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+----------+\n",
      "|studentid|   name|gender|       DOB|\n",
      "+---------+-------+------+----------+\n",
      "|      si1|  Robin|     M|1981-09-06|\n",
      "|      si2|  Maria|     F|1986-06-06|\n",
      "|      si3|  Julie|     F|1988-09-05|\n",
      "|      si4|    Bob|     M|1987-05-04|\n",
      "|      si6|William|     M|1980-11-12|\n",
      "+---------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select studentid, name, gender,\\\n",
    "          to_date(dateofbirth) as DOB from students\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|  studentid|   string|   null|\n",
      "|       name|   string|   null|\n",
      "|     gender|   string|   null|\n",
      "|dateofbirth|     date|   null|\n",
      "+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentDf_dob=spark.sql(\"select studentid, name, gender,\\\n",
    "          to_date(dateofbirth) as dateofbirth from students\")\n",
    "studentDf_dob.createOrReplaceTempView(\"StudentsDob\")\n",
    "spark.sql(\"Describe studentsDob\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-----+----+\n",
      "|dateofbirth|day|month|year|\n",
      "+-----------+---+-----+----+\n",
      "| 1981-09-06|  6|    9|1981|\n",
      "| 1986-06-06|  6|    6|1986|\n",
      "| 1988-09-05|  5|    9|1988|\n",
      "| 1987-05-04|  4|    5|1987|\n",
      "| 1980-11-12| 12|   11|1980|\n",
      "+-----------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using Date Functions for Various Date Manipulations\n",
    "spark.sql(\"select dateofbirth,dayofmonth(dateofbirth) day,\\\n",
    "          month(dateofbirth) month, year(dateofbirth) year\\\n",
    "          from studentsdob\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MALE\n",
      "FEMALE\n",
      "NA\n"
     ]
    }
   ],
   "source": [
    "#Create a New PySpark UDF Problem\n",
    "def genderCodeToValue(code):\n",
    "    return('FEMALE' if code == 'F' else 'MALE' if code =='M' else 'NA')\n",
    "print(genderCodeToValue('M'))\n",
    "print(genderCodeToValue('F'))\n",
    "print(genderCodeToValue('S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Registering the Python Function in a Spark SQL Session\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.genderCodeToValue(code)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"genderCodeToValue\", genderCodeToValue,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   Name|Gender|\n",
      "+-------+------+\n",
      "|  Robin|  MALE|\n",
      "|  Maria|FEMALE|\n",
      "|  Julie|FEMALE|\n",
      "|    Bob|  MALE|\n",
      "|William|  MALE|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calling the Registered UDF from PySpark SQL\n",
    "spark.sql(\"select name as Name, genderCodeToValue(gender) \\\n",
    "          as Gender from studentsdob\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join Two DataFrames Using SQL Problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+--------+\n",
      "| id|marks|studentid|subjects|\n",
      "+---+-----+---------+--------+\n",
      "|  6|   78|      si4|     C++|\n",
      "|  9|   83|      si2|    Java|\n",
      "|  5|   72|      si3|    Ruby|\n",
      "|  4|   85|      si2|  Python|\n",
      "|  7|   77|      si5|     C++|\n",
      "|  1|   75|      si1|  Python|\n",
      "|  8|   84|      si4|  Python|\n",
      "|  2|   76|      si3|    Java|\n",
      "|  3|   81|      si1|    Java|\n",
      "+---+-----+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subjectsDf = spark.read.csv('subjects.csv',header=True, inferSchema=True)\n",
    "subjectsDf.createOrReplaceTempView(\"subjects\")\n",
    "spark.sql(\"select * from subjects\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+\n",
      "|studentId|   name|gender|\n",
      "+---------+-------+------+\n",
      "|      si1|  Robin|     M|\n",
      "|      si2|  Maria|     F|\n",
      "|      si3|  Julie|     F|\n",
      "|      si4|    Bob|     M|\n",
      "|      si6|William|     M|\n",
      "+---------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select studentId,name,gender from studentsdob\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+-----------+---+-----+---------+--------+\n",
      "|studentid| name|gender|dateofbirth| id|marks|studentid|subjects|\n",
      "+---------+-----+------+-----------+---+-----+---------+--------+\n",
      "|      si4|  Bob|     M| 1987-05-04|  6|   78|      si4|     C++|\n",
      "|      si2|Maria|     F| 1986-06-06|  9|   83|      si2|    Java|\n",
      "|      si3|Julie|     F| 1988-09-05|  5|   72|      si3|    Ruby|\n",
      "|      si2|Maria|     F| 1986-06-06|  4|   85|      si2|  Python|\n",
      "|      si1|Robin|     M| 1981-09-06|  1|   75|      si1|  Python|\n",
      "|      si4|  Bob|     M| 1987-05-04|  8|   84|      si4|  Python|\n",
      "|      si3|Julie|     F| 1988-09-05|  2|   76|      si3|    Java|\n",
      "|      si1|Robin|     M| 1981-09-06|  3|   81|      si1|    Java|\n",
      "+---------+-----+------+-----------+---+-----+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''By observing both these datasets, note that studentId is the column\n",
    "that can be used to join these two datasets. Let’s write the query to join\n",
    "these datasets'''\n",
    "\n",
    "spark.sql(\"select * from studentsdob st join subjects sb on\\\n",
    "          (st.studentId = sb.studentId)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join Multiple DataFrames Using SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StudentDOB st\n",
      "+---------+-------+------+-----------+\n",
      "|studentid|   name|gender|dateofbirth|\n",
      "+---------+-------+------+-----------+\n",
      "|      si1|  Robin|     M| 1981-09-06|\n",
      "|      si2|  Maria|     F| 1986-06-06|\n",
      "|      si3|  Julie|     F| 1988-09-05|\n",
      "|      si4|    Bob|     M| 1987-05-04|\n",
      "|      si6|William|     M| 1980-11-12|\n",
      "+---------+-------+------+-----------+\n",
      "\n",
      "subjects sb\n",
      "+---+-----+---------+-------+\n",
      "| id|marks|studentid|subject|\n",
      "+---+-----+---------+-------+\n",
      "|  6|   78|      si4|    C++|\n",
      "|  9|   83|      si2|   Java|\n",
      "|  5|   72|      si3|   Ruby|\n",
      "|  4|   85|      si2| Python|\n",
      "|  7|   77|      si5|    C++|\n",
      "|  1|   75|      si1| Python|\n",
      "|  8|   84|      si4| Python|\n",
      "|  2|   76|      si3|   Java|\n",
      "|  3|   81|      si1|   Java|\n",
      "+---+-----+---------+-------+\n",
      "\n",
      "attendance at\n",
      "+---------+-------+----------+\n",
      "|studentid|subject|attendance|\n",
      "+---------+-------+----------+\n",
      "|      si1| Python|        30|\n",
      "|      si3|   Java|        22|\n",
      "|      si1|   Java|        34|\n",
      "|      si2| Python|        39|\n",
      "|      si3|   Ruby|        25|\n",
      "|      si4|    C++|        38|\n",
      "|      si5|      C|        35|\n",
      "|      si4| Python|        39|\n",
      "|      si2|   Java|        39|\n",
      "|      si6|   Java|        35|\n",
      "+---------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentDf_dob=spark.sql(\"select studentid, name, gender,to_date(dateofbirth) as dateofbirth from students\")\n",
    "studentDf_dob.createOrReplaceTempView(\"StudentsDob\")\n",
    "print('StudentDOB st')\n",
    "spark.sql(\"select * from studentsDob\").show()\n",
    "\n",
    "subjectsDf = spark.read.csv('subjects.csv',header=True, inferSchema=True)\n",
    "subjectsDf.createOrReplaceTempView(\"subjects\")\n",
    "print('subjects sb')\n",
    "spark.sql(\"select * from subjects\").show()\n",
    "\n",
    "attendanceDf = spark.read.csv('attendance.csv',header=True, inferSchema=True)\n",
    "attendanceDf.createOrReplaceTempView(\"attendance\")\n",
    "print('attendance at')\n",
    "spark.sql(\"select * from attendance\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query is joining the third dataset as a continuation of the existing\n",
    "join. Similarly, you can join any number of tables in the same manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+-----------+---+-----+---------+-------+---------+-------+----------+\n",
      "|studentid| name|gender|dateofbirth| id|marks|studentid|subject|studentid|subject|attendance|\n",
      "+---------+-----+------+-----------+---+-----+---------+-------+---------+-------+----------+\n",
      "|      si4|  Bob|     M| 1987-05-04|  6|   78|      si4|    C++|      si4| Python|        39|\n",
      "|      si4|  Bob|     M| 1987-05-04|  6|   78|      si4|    C++|      si4|    C++|        38|\n",
      "|      si2|Maria|     F| 1986-06-06|  9|   83|      si2|   Java|      si2|   Java|        39|\n",
      "|      si2|Maria|     F| 1986-06-06|  9|   83|      si2|   Java|      si2| Python|        39|\n",
      "|      si3|Julie|     F| 1988-09-05|  5|   72|      si3|   Ruby|      si3|   Ruby|        25|\n",
      "|      si3|Julie|     F| 1988-09-05|  5|   72|      si3|   Ruby|      si3|   Java|        22|\n",
      "|      si2|Maria|     F| 1986-06-06|  4|   85|      si2| Python|      si2|   Java|        39|\n",
      "|      si2|Maria|     F| 1986-06-06|  4|   85|      si2| Python|      si2| Python|        39|\n",
      "|      si1|Robin|     M| 1981-09-06|  1|   75|      si1| Python|      si1|   Java|        34|\n",
      "|      si1|Robin|     M| 1981-09-06|  1|   75|      si1| Python|      si1| Python|        30|\n",
      "|      si4|  Bob|     M| 1987-05-04|  8|   84|      si4| Python|      si4| Python|        39|\n",
      "|      si4|  Bob|     M| 1987-05-04|  8|   84|      si4| Python|      si4|    C++|        38|\n",
      "|      si3|Julie|     F| 1988-09-05|  2|   76|      si3|   Java|      si3|   Ruby|        25|\n",
      "|      si3|Julie|     F| 1988-09-05|  2|   76|      si3|   Java|      si3|   Java|        22|\n",
      "|      si1|Robin|     M| 1981-09-06|  3|   81|      si1|   Java|      si1|   Java|        34|\n",
      "|      si1|Robin|     M| 1981-09-06|  3|   81|      si1|   Java|      si1| Python|        30|\n",
      "+---------+-----+------+-----------+---+-----+---------+-------+---------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from studentsdob st inner Join subjects sb on\\\n",
    "          (st.studentid=sb.studentid) Join attendance at on\\\n",
    "          (at.studentid=st.studentid)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the task is to get a cleaner report with only the required columns\n",
    "by combining all the datasets—student, subject, attendance—that will\n",
    "provide a report:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and register StudentsDob\n",
    "studentDf = spark.read.csv('studentData.csv',header=True, inferSchema=True)\n",
    "studentDf.createOrReplaceTempView(\"StudentsDob\")\n",
    "#Load and register subjects\n",
    "subjectsDf = spark.read.csv('subjects.csv',header=True, inferSchema=True)\n",
    "subjectsDf.createOrReplaceTempView(\"subjects\")\n",
    "#Load and register attendance\n",
    "attendanceDf = spark.read.csv('attendance.csv',header=True, inferSchema=True)\n",
    "attendanceDf.createOrReplaceTempView(\"attendance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.genderCodeToValue(code)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the gender User Defined Function\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "def genderCodeToValue(code):\n",
    "    return('FEMALE' if code == 'F' else 'MALE' if code == 'M' else 'NA')\n",
    "spark.udf.register(\"genderCodeToValue\", genderCodeToValue,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+----------+\n",
      "| Name|Gender|Marks|Attendance|\n",
      "+-----+------+-----+----------+\n",
      "|Robin|  MALE|   81|        34|\n",
      "|Robin|  MALE|   81|        30|\n",
      "|Robin|  MALE|   75|        34|\n",
      "|Robin|  MALE|   75|        30|\n",
      "|Maria|FEMALE|   85|        39|\n",
      "|Maria|FEMALE|   85|        39|\n",
      "|Maria|FEMALE|   83|        39|\n",
      "|Maria|FEMALE|   83|        39|\n",
      "|Julie|FEMALE|   76|        25|\n",
      "|Julie|FEMALE|   76|        22|\n",
      "|Julie|FEMALE|   72|        25|\n",
      "|Julie|FEMALE|   72|        22|\n",
      "|  Bob|  MALE|   84|        39|\n",
      "|  Bob|  MALE|   84|        38|\n",
      "|  Bob|  MALE|   78|        39|\n",
      "|  Bob|  MALE|   78|        38|\n",
      "+-----+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply a query to get the final report\n",
    "spark.sql(\"select name as Name, genderCodeToValue(gender) as Gender,\\\n",
    "          marks as Marks, attendance as Attendance from studentsdob st\\\n",
    "          Join subjects sb on (st.studentId = sb.studentId)\\\n",
    "          Join attendance at on (at.studentId = st.studentId)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizing PySpark SQL**\n",
    "\n",
    "**Apply Aggregation Using PySpark SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+-----------+-----+---+-----+---------+-------+\n",
      "|studentid| name|gender|dateofbirth|score| id|marks|studentid|subject|\n",
      "+---------+-----+------+-----------+-----+---+-----+---------+-------+\n",
      "|      si1|Robin|     M| 1981-09-06|   20|  3|   81|      si1|   Java|\n",
      "|      si1|Robin|     M| 1981-09-06|   20|  1|   75|      si1| Python|\n",
      "|      si2|Maria|     F| 1986-06-06|   30|  4|   85|      si2| Python|\n",
      "|      si2|Maria|     F| 1986-06-06|   30|  9|   83|      si2|   Java|\n",
      "|      si3|Julie|     F| 1988-09-05|   10|  2|   76|      si3|   Java|\n",
      "|      si3|Julie|     F| 1988-09-05|   10|  5|   72|      si3|   Ruby|\n",
      "|      si4|  Bob|     M| 1987-05-04|   15|  8|   84|      si4| Python|\n",
      "|      si4|  Bob|     M| 1987-05-04|   15|  6|   78|      si4|    C++|\n",
      "+---------+-----+------+-----------+-----+---+-----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load and register Student data\n",
    "studentDf = spark.read.csv('studentData.csv',header=True, inferSchema=True)\n",
    "studentDf.createOrReplaceTempView(\"StudentsDob\")\n",
    "#Load and register subjects\n",
    "subjectsDf = spark.read.csv('subjects.csv',header=True, inferSchema=True)\n",
    "subjectsDf.createOrReplaceTempView(\"subjects\")\n",
    "spark.sql(\"select * from studentsdob st join subjects\\\n",
    "          sb on(st.studentId = sb.studentId)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   name|avg(marks)|\n",
      "+-------+----------+\n",
      "|  Robin|      78.0|\n",
      "|  Maria|      84.0|\n",
      "|  Julie|      74.0|\n",
      "|    Bob|      81.0|\n",
      "|William|      null|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Group By on this query to identify the average marks per student\n",
    "spark.sql(\"select name,avg(marks) from studentsdob st left\\\n",
    "          join subjects sb on (st.studentId = sb.studentId) group by\\\n",
    "          name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+-----------+-----+----+-----+---------+-------+\n",
      "|studentid|   name|gender|dateofbirth|score|  id|marks|studentid|subject|\n",
      "+---------+-------+------+-----------+-----+----+-----+---------+-------+\n",
      "|      si1|  Robin|     M| 1981-09-06|   20|   3|   81|      si1|   Java|\n",
      "|      si1|  Robin|     M| 1981-09-06|   20|   1|   75|      si1| Python|\n",
      "|      si2|  Maria|     F| 1986-06-06|   30|   4|   85|      si2| Python|\n",
      "|      si2|  Maria|     F| 1986-06-06|   30|   9|   83|      si2|   Java|\n",
      "|      si3|  Julie|     F| 1988-09-05|   10|   2|   76|      si3|   Java|\n",
      "|      si3|  Julie|     F| 1988-09-05|   10|   5|   72|      si3|   Ruby|\n",
      "|      si4|    Bob|     M| 1987-05-04|   15|   8|   84|      si4| Python|\n",
      "|      si4|    Bob|     M| 1987-05-04|   15|   6|   78|      si4|    C++|\n",
      "|      si6|William|     M| 1980-11-12|   25|null| null|     null|   null|\n",
      "+---------+-------+------+-----------+-----+----+-----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple Steps\n",
    "resultDF = spark.sql(\"select * from studentsdob st left join\\\n",
    "                     subjects sb on(st.studentId = sb.studentId)\")\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   name|avg(marks)|\n",
      "+-------+----------+\n",
      "|  Robin|      78.0|\n",
      "|  Maria|      84.0|\n",
      "|  Julie|      74.0|\n",
      "|    Bob|      81.0|\n",
      "|William|      null|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDF.groupBy(\"name\").agg({\"marks\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|   name|subject|avg(marks)|\n",
      "+-------+-------+----------+\n",
      "|  Robin|   Java|      81.0|\n",
      "|  Robin| Python|      75.0|\n",
      "|  Maria| Python|      85.0|\n",
      "|  Maria|   Java|      83.0|\n",
      "|  Julie|   Java|      76.0|\n",
      "|  Julie|   Ruby|      72.0|\n",
      "|    Bob| Python|      84.0|\n",
      "|    Bob|    C++|      78.0|\n",
      "|William|   null|      null|\n",
      "+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name,subject,avg(marks) from studentsdob\\\n",
    "          st left join subjects sb on (st.studentId = sb.studentId)\\\n",
    "          group by name,subject\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|   name|subject|avg(marks)|\n",
      "+-------+-------+----------+\n",
      "|  Robin|   Java|      81.0|\n",
      "|  Robin| Python|      75.0|\n",
      "|  Maria| Python|      85.0|\n",
      "|  Maria|   Java|      83.0|\n",
      "|  Julie|   Java|      76.0|\n",
      "|  Julie|   Ruby|      72.0|\n",
      "|    Bob| Python|      84.0|\n",
      "|    Bob|    C++|      78.0|\n",
      "|William|   null|      null|\n",
      "+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternate\n",
    "resultDF.groupBy('name','subject').agg({\"marks\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the Number of Students per Subject**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|subject|Students_count|\n",
      "+-------+--------------+\n",
      "|   Java|             3|\n",
      "| Python|             3|\n",
      "|   Ruby|             1|\n",
      "|    C++|             1|\n",
      "|   null|             1|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select subject,count(name) Students_count\\\n",
    "          from students st left join subjects sb on\\\n",
    "          (st.studentId =sb.studentId) group by subject\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|   name|Students_count|\n",
      "+-------+--------------+\n",
      "|  Robin|             2|\n",
      "|  Maria|             2|\n",
      "|  Julie|             2|\n",
      "|    Bob|             2|\n",
      "|William|             0|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding the Number of Subjects per Student\n",
    "spark.sql(\"select name,count(subject) Students_count\\\n",
    "          from studentsdob st left join subjects sb on\\\n",
    "          (st.studentId =sb.studentId) group by name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Windows Functions Using PySpark SQL**\n",
    "\n",
    "The rank function provides a sequential number for each row within a selected set of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+----+\n",
      "| name|subject|marks|RANK|\n",
      "+-----+-------+-----+----+\n",
      "|  Bob| Python|   84|   1|\n",
      "|  Bob|    C++|   78|   1|\n",
      "|Julie|   Java|   76|   3|\n",
      "|Julie|   Ruby|   72|   3|\n",
      "|Maria| Python|   85|   5|\n",
      "|Maria|   Java|   83|   5|\n",
      "|Robin|   Java|   81|   7|\n",
      "|Robin| Python|   75|   7|\n",
      "+-----+-------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  apply the rank function on all the rows without any window selection\n",
    "\n",
    "spark.sql(\"select name,subject,marks,rank() over\\\n",
    "          (order by name) as RANK from studentsdob st join\\\n",
    "          subjects sb ON (st.studentId = sb.studentId)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Partition By in a Rank Function**\n",
    "\n",
    "In this query we added a new PARTITION clause on the subject. This is\n",
    "to say that we are windowing based on the subject column. This will create\n",
    "a separate window for each distinct subject column value and will apply\n",
    "the window function rank only within the rows of the window set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+----+\n",
      "| name|subject|marks|RANK|\n",
      "+-----+-------+-----+----+\n",
      "|  Bob|    C++|   78|   1|\n",
      "|Maria|   Java|   83|   1|\n",
      "|Robin|   Java|   81|   2|\n",
      "|Julie|   Java|   76|   3|\n",
      "|Maria| Python|   85|   1|\n",
      "|  Bob| Python|   84|   2|\n",
      "|Robin| Python|   75|   3|\n",
      "|Julie|   Ruby|   72|   1|\n",
      "+-----+-------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name,subject,marks,rank() over\\\n",
    "          (PARTITION BY subject order by marks DESC) as RANK\\\n",
    "          from studentsdob st join subjects sb ON\\\n",
    "          (st.studentId = sb.studentId)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtaining the Top Two Students for Each Subject**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+----+\n",
      "| name|subject|marks|RANK|\n",
      "+-----+-------+-----+----+\n",
      "|  Bob|    C++|   78|   1|\n",
      "|Maria|   Java|   83|   1|\n",
      "|Robin|   Java|   81|   2|\n",
      "|Maria| Python|   85|   1|\n",
      "|  Bob| Python|   84|   2|\n",
      "|Julie|   Ruby|   72|   1|\n",
      "+-----+-------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name,subject,marks,RANK from (select\\\n",
    "          name,subject,marks,rank() over\\\n",
    "          (PARTITION BY subject order by marks desc) as RANK\\\n",
    "          from studentsdob st join subjects sb ON\\\n",
    "          (st.studentId = sb.studentId)) where RANK <= 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cache Data Using PySpark SQL**\n",
    "\n",
    "With caching Spark will keep the data in memory so\n",
    "it needs to read it again. Reading the data from disk is a costly operation.\n",
    "\n",
    "We don’t want to cache huge volumes of data. It will lead to out of memory\n",
    "exceptions. Handling huge volumes of data in memory will leave Spark\n",
    "with less memory for other operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"cache table students_cached AS select * from studentsdob\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-----+\n",
      "|studentid|   name|subject|marks|\n",
      "+---------+-------+-------+-----+\n",
      "|      si1|  Robin|   Java|   81|\n",
      "|      si1|  Robin| Python|   75|\n",
      "|      si2|  Maria| Python|   85|\n",
      "|      si2|  Maria|   Java|   83|\n",
      "|      si3|  Julie|   Java|   76|\n",
      "|      si3|  Julie|   Ruby|   72|\n",
      "|      si4|    Bob| Python|   84|\n",
      "|      si4|    Bob|    C++|   78|\n",
      "|      si6|William|   null| null|\n",
      "+---------+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select st.studentid,name,subject,marks\\\n",
    "          from students_cached st left join subjects sb\\\n",
    "          on (st.studentId =sb.studentId)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the Distribute By, Sort By, and Cluster By Clauses in PySpark SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
